



{\href{http://rfs.oxfordjournals.org/content/1/3/195.short}{Campbell and Shiller's (1988)} Approximation}\label{slide:CSlinearproof}

\hyperlink{slide:CSlinear}{\beamergotobutton{back}}



The results rsults from:
\begin{eqnarray*}
&& \log(P_{t+1}+D_{t+1})\\
&=& \log(P_{t+1}) + \log \left(1+\frac{D_{t+1}}{P_{t+1}}\right) = p_{t+1} + \log(1+\exp(d_{t+1}-p_{t+1}))\\
&=& p_{t+1} + \log\left(1+\exp(\overline{d-p}) + \exp(d_{t+1}-p_{t+1}) - \exp(\overline{d-p})\right)\\
&=& p_{t+1} + \log(1+\exp(\overline{d-p})) + \log\left(1+\frac{\exp(d_{t+1}-p_{t+1}) -\exp(\overline{d-p})}{1+\exp(\overline{d-p})}\right)\\
&=& p_{t+1} + \log(1+\exp(\overline{d-p})) + \log\left(1+\exp(\overline{d-p})\frac{\exp(\overbrace{d_{t+1}-p_{t+1}- \{\overline{d-p}\}}^{\mbox{assumed to be small}}) -1}{1+\exp(\overline{d-p})}\right)\\
&=& p_{t+1} + \log(1+\exp(\overline{d-p})) + \frac{\exp(\overline{d-p})}{{1+\exp(\overline{d-p})}} \left(d_{t+1}-p_{t+1}- \{\overline{d-p}\}\right)\\
&=& p_{t+1} \frac{1}{1+\exp(\overline{d-p})} + d_{t+1} \frac{\exp(\overline{d-p})}{{1+\exp(\overline{d-p})}} +\\
&& \log(1+\exp(\overline{d-p})) - \left(\overline{d-p}\right) \frac{\exp(\overline{d-p})}{{1+\exp(\overline{d-p})}}.
\end{eqnarray*}



{S.D.F. in the CES Epstein-Zin Context}\label{slide:sdf_appendix}

We denote by $\pi_t(x_{t+1})$ the price of an asset that provides the payoff $x_{t+1}$ at date $t+1$ (as of date $t$, this payoff may be random). If one purchases $\varepsilon$ units of this asset and consumes them at date $t+1$, the intertemporal utility becomes $F(C_t{\color{blue} - \varepsilon \pi_t(x_{t+1})},R_t(F(C_{t+1}+{\color{blue}\varepsilon x_{t+1}},R_{t+1}(U_{t+2}))))$.

\vspace{.2cm}
If $\pi_t(x_{t+1})$ is the equilibrium price of the asset, then agents should be indifferent between buying a small amount of this asset and not. That is, we should have:
$$
F(C_t,R_t(F(C_{t+1},R_{t+1}(U_{t+2})))) =
F(C_t {\color{blue} - \varepsilon \pi_t(x_{t+1})},R_t(F(C_{t+1}+{\color{blue}\varepsilon x_{t+1}},R_{t+1}(U_{t+2})))).
$$
Let's compute the first-order Taylor expansion of right-hand side term w.r.t. $\varepsilon$. To begin with, we have:
$$
F(C_{t+1}+{\color{blue}\varepsilon x_{t+1}},R_{t+1}(U_{t+2})) = U_{t+1} + \varepsilon x_{t+1} (1-\beta) C_{t+1}^{-\rho}U_{t+1}^\rho + o(\varepsilon).
$$
Now,
$$
F(C_{t+1}+{\color{blue}\varepsilon x_{t+1}},R_{t+1}(U_{t+2}))^{1-\gamma} = U_{t+1}^{1-\gamma} + \varepsilon x_{t+1}  (1-\beta) C_{t+1}^{-\rho}U_{t+1}^{\rho-\gamma} + o(\varepsilon).
$$
Then,
$$
\mathbb{E}_t \left( F(C_{t+1}+{\color{blue}\varepsilon x_{t+1}},R_{t+1}(U_{t+2}))^{1-\gamma} \right)^{\frac{1}{1-\gamma}}=R_t(U_{t+1}) + \varepsilon R_t(U_{t+1})^{\gamma} \mathbb{E}_t \left(  x_{t+1} (1-\beta) C_{t+1}^{-\rho} U_{t+1}^{\rho-\gamma} \right).
$$
Therefore, $F(C_t{\color{blue} - \varepsilon \pi_t(x_{t+1})},R_t(F(C_{t+1}+{\color{blue}\varepsilon x_{t+1}},R_{t+1}(U_{t+2}))))$ is equal to $F(C_t,R_t(U_{t+1}))+$
$$
\varepsilon R_t(U_{t+1})^{\gamma - \rho} \mathbb{E}_t \left(  x_{t+1} (1-\beta) C_{t+1}^{-\rho} U_{t+1}^{\rho-\gamma} \right) U_t^{\rho}
- \varepsilon \pi_t(x_{t+1}) (1-\beta) C_t^{-\rho} U_t^{\rho} + o(\varepsilon),
$$
which leads to Equation (\@ref(eq:SS}).\qed



\begin{exampleblock}{Intertemporal Utility}
In that box, we consider a deterministic context where the intertemporal utility is given by:
$$
U = \sum_{t=1}^{\infty} \beta u(C_{t}).
$$

	* Assume that one can substitute consumption in date $t$ for consumption in date $t+1$ by investing on some asset whose gross real return is $R$.
	* A marginal substitution of date-$t$ consumption for  date-$t+1$ consumption will have a neutral effect on intertemporal utility if:
	$$
	\underbrace{u'(C_{t})}_{\mbox{marginal decrease in utility}} = \underbrace{R \beta u'(C_{t+1})}_{\mbox{marginal increase in utility}}
	$$
	* Therefore, if $R = \exp{r}$, the interest rate $r$ is given by:
	$$
	r = - \log\left(\beta\frac{u'(C_{t+1})}{u'(C_{t})}\right).
	$$

\end{exampleblock}





{}

\begin{defn}[Intertemporal Elasticity of Substitution]\label{def:IES}
The {\color{blue}Intertemporal Elasticity of Substitution (IES)} is defined as the change in consumption growth per change in the interest rate, that is:
$$
IES = \frac{d \log\left( \dfrac{C_{t+1}}{C_{t}}\right)}{d r} = - \frac{d \log\left( \dfrac{C_{t+1}}{C_{t}}\right)}{d\log\left(\dfrac{u'(C_{t+1})}{u'(C_{t})}\right)}.
$$
\end{defn}

\begin{exampleblock}{Case of the isoelastic utility function}
For an isoelastic utility function ($u:$ $c \rightarrow \dfrac{c^{1 - \rho}-1}{1-\rho}$), we have:
$$
d\log\left(\dfrac{u'(C_{t+1})}{u'(C_{t})}\right) = - \rho d\log\left(\dfrac{C_{t+1}}{C_{t}}\right),
$$
hence $IES = 1/\rho$, where $\rho$ is the relative risk aversion parameter (see Definition \@ref(def:RAmeasures}).
\end{exampleblock}





{}

\begin{defn}[Certainty Equivalent]\label{def:CE}
Consider a gamble that provides a random payoff $X_t$. The (non-random) amount $CE(X_t)$ is the {\color{blue}certainty equivalent} of that gamble if an agent is indifferent between the gamble and receiving $CE(X_t)$.
\end{defn}



{}


\begin{defn}[Stochastic discount factor]\label{def:sdf}

We denote by $\pi_t(x_{t+1})$ the date-$t$ price of an asset whose payoff at date $t+1$ is $x_{t+1}$ (this payoff may be random conditional on the information available at date $t$).

\vspace{.2cm}

A {\color{blue} stochastic discount factor (s.d.f.)} $S_{t,t+1}$ is a stochastic process that is such that:
$$
\pi_t(x_{t+1}) = \mathbb{E}_t(S_{t,t+1}x_{t+1}),
$$
where $\mathbb{E}_t$ denotes the expectation conditional on the information available at date $t$.
\end{defn}

\begin{prop}[s.d.f. fundamental properties]
%see Gilchrist lecture 2
* There are no arbitrage opportunities (AO) if and only if there exists (at least) one strictly positive s.d.f..
* The stochastic discount factor is unique if and only if markets are complete

\end{prop}

\begin{exampleblock}{Seminal contributions on s.d.f.}
\href{http://faculty.chicagobooth.edu/john.cochrane/teaching/35904_asset_pricing/hansen\%20richard\%20econometrica.pdf}{Hansen and Richard (1987)} and \href{http://www.sciencedirect.com/science/article/pii/0022053179900437}{Harrison and Kreps (1979)}.
\end{exampleblock}





{Affine term-structure models in discrete-time}{Recursive bond-pricing formula - Proof}\label{slide:proofrecursive}
\hyperlink{slide:ATSM}{\beamergotobutton{back}}
\begin{small}

	* The formula is valid for $h=0$.
	* Assume that it is valid for a given $h>0$, then:
	\begin{eqnarray*}
	&& \mathbb{E}_t(\exp(-\gamma_{h+1}'X_{t+1}+\gamma_h'X_{t+2} + \dots + \gamma_1'X_{t+h+1}))\\
	&=& \mathbb{E}_t(\mathbb{E}_{t+1}(\exp(\gamma_{h+1}'X_{t+1} + \dots + \gamma_{1}'X_{t+h+1})) \\
	&=& \mathbb{E}_t(\exp(-\gamma_{h+1}'X_{t+1})\mathbb{E}_{t+1}(\exp(\gamma_{h}'X_{t+2} + \dots + \gamma_{1}'X_{t+h+1}))) \\
	&=& \mathbb{E}_t(\exp(\gamma_{h+1}'X_{t+1} + a_h + b_h'X_{t+1})) \\
	&=& \exp(a_h + a(b_h+\gamma_{h+1}) +  b(b_h+\gamma_{h+1})'X_{t}).
	\end{eqnarray*}

\end{small}




\label{slide:proofaffineBY}


	* Conditional Laplace transform of $X_t$:\hyperlink{slide:BYaffine}{\beamergotobutton{back}}
	\begin{eqnarray*}
	&& \mathbb{E}_t(\exp(u x_{t+1} + v \sigma^2_{t+1} + w g_{t+1}))\\
	&=& \mathbb{E}_t(\exp(u (\rho_x x_t + \phi_e \sigma_t e_{t+1}) + v [\sigma^2 + \nu_1(\sigma^2_t -\sigma^2) + \sigma_w w_{t+1}] + w (\mu + x_t + \sigma_t \eta_{t+1})))\\
	&=& \exp\left(u \rho_x x_t + v [\sigma^2 + \nu_1(\sigma^2_t -\sigma^2)] + w (\mu + x_t) + \frac{1}{2}(u^2 \phi_e^2 \sigma_t^2+v^2 \sigma_w^2 + w^2 \sigma_t^2) \right)\\
	&=& \exp\left(
	v \sigma^2 (1 - \nu_1) + w \mu + \frac{1}{2}v^2 \sigma_w^2 + x_t(u \rho_x +  w) + \sigma_t^2\left(v \nu_1+ \frac{1}{2}(u^2 \phi_e^2 + w^2)\right)
	\right).
	\end{eqnarray*}
	* Conditional Laplace transform of $X_t^*$:\hyperlink{slide:BYaugment}{\beamergotobutton{back}}
	\begin{eqnarray*}
	&& \mathbb{E}_t(\exp(u x_{t+1} + v \sigma^2_{t+1} + w g_{t+1} + s \pi_{t+1}))\\
	&=& \mathbb{E}_t(\exp\{u (\rho_x x_t + \phi_e \sigma_t e_{t+1}) + v [\sigma^2 + \nu_1(\sigma^2_t -\sigma^2) + \sigma_w w_{t+1}] + \\
	&& w (\mu + x_t + \sigma_t \eta_{t+1}) +
	s [ \mu_\pi + \rho_{\pi} (\pi_{t} - \mu_\pi) + \phi_{\pi,x} x_{t} + \phi_{\pi,\eta} \sigma_t \eta_{t+1} + \sigma_\varepsilon \varepsilon_{t+1}]\})\\
	&=& \exp\left(u \rho_x x_t + v [\sigma^2 + \nu_1(\sigma^2_t -\sigma^2)] + w (\mu + x_t) + s ( \mu_\pi + \rho_{\pi} (\pi_{t} - \mu_\pi) + \phi_{\pi,x} x_{t}) +  \right. \\
	&& \left. \frac{1}{2}\left(u^2 \phi_e^2 \sigma_t^2+v^2 \sigma_w^2 + (w+s \phi_{\pi,\eta})^2 \sigma_t^2 + s^2 \sigma_\varepsilon^2\right) \right)\\
	&=& \exp\left(
	v \sigma^2 (1 - \nu_1) + w \mu + \frac{1}{2}\left(v^2 \sigma_w^2+s^2 \sigma_\varepsilon^2\right) +
	s \mu_\pi (1 - \rho_\pi) + \right. \\
	&& \left. x_t(u \rho_x +  w + s\phi_{\pi,x}) + \sigma_t^2\left(v \nu_1+ \frac{1}{2}(u^2 \phi_e^2 + (w+s \phi_{\pi,\eta})^2)\right) + s \rho_\pi \pi_t
	\right).
	\end{eqnarray*}





\label{slide:unc_moments_X}


	* The unconditional mean of $X_t$ is $[0,\sigma^2,\mu,\mu_\pi]'$.
	* The system of equations (\@ref(eq:BY_heterosckedInfla}) writes, in matrix form:
	\begin{eqnarray}(\#eq:VARX}
	\left[
	\begin{array}{c}
	x_{t+1}\\
	\sigma^2_{t+1}\\
	g_{t+1}\\
	\pi_{t+1}
	\end{array}
	\right] &=&
	\left[
	\begin{array}{c}
	0\\
	\sigma^2(1-\nu_1)\\
	\mu\\
	\mu_\pi(1 - \rho_\pi)
	\end{array}
	\right]+
	\left[
	\begin{array}{cccc}
	\rho_x & 0 & 0 & 0\\
	0 & \nu_1 &0&0\\
	1 & 0& 0& 0\\
	\phi_{\pi,x} & 0 & 0 & \rho_\pi
	\end{array}
	\right]
	\left[
	\begin{array}{c}
	x_{t}\\
	\sigma^2_{t}\\
	g_{t}\\
	\pi_{t}
	\end{array}
	\right]
	+ \nonumber\\
	&&	\left[
	\begin{array}{cccc}
	\phi_e \sigma_t & 0 & 0 & 0\\
	0 & \sigma_w &0&0\\
	0 & 0& \sigma_t& 0\\
	0 & 0 & \phi_{\pi,\eta} \sigma_t & \sigma_\varepsilon
	\end{array}
	\right]
	\left[
	\begin{array}{c}
	e_{t+1}\\
	w_{t+1}\\
	\eta_{t+1}\\
	\varepsilon_{t+1}
	\end{array}
	\right].
	\end{eqnarray}
	* Let's denote the conditional matrix of covariance of $X_{t+1}$ by $\Sigma(\sigma_t^2)$, we have:
	$$
	\Sigma_t(\sigma_t^2) = \left[
	\begin{array}{cccc}
	\phi_e^2 \sigma_t^2 & 0 & 0 & 0\\
	0 & \sigma_w^2 &0&0\\
	0 & 0& \sigma_t^2& \phi_{\pi,\eta} \sigma_t^2\\
	0 & 0 & \phi_{\pi,\eta} \sigma_t^2 & \phi_{\pi,\eta}^2 \sigma_t^2 + \sigma_\varepsilon^2
	\end{array}
	\right].
	$$
	* Using the total variance formula, we get the unconditional variance of $X_t$:
	\begin{equation}
	Vec(\Omega) = (I - \Phi \otimes \Phi)^{-1} Vec(\Sigma(\sigma^2)),
	\end{equation}
	where $\Phi$ denotes the matrix of autoregressive parameters appearing on the right-hand side of Eq. (\@ref(eq:VARX}).







\begin{assum}\label{assum:XVAR}

	* Consider a process $X_t$ that follows a VAR(1) of the form:
	\begin{equation}(\#eq:generalVAR}
	X_{t+1} = \mu + \Phi X_{t} + \Omega(X_t) \varepsilon_{t+1},\quad \varepsilon_t \sim \mathcal{N}(0,Id),
	\end{equation}
	where the conditional variance of $X_{t+1}$, that is $\Omega{\Omega}'(X_t)$ is an affine function of $X_t$, i.e.
	$$
	Vec\left(\mathbb{V}ar_t(X_{t+1})\right)=\Sigma_0 + \Sigma_1 X_t.
	$$
	* We consider the variable $Z_{t+1,t+h} = \gamma_{h}'X_{t+1}+\dots+\gamma_1'X_{t+h}$.

\end{assum}





\begin{prop}\label{prop:qwertz}

	* Under Assumption (\@ref(assum:XVAR}) we have:
	\begin{eqnarray}
	\mathbb{E}_t(Z_{t+1,t+h}) &=& a_h(\gamma_1,\dots,\gamma_h) + b_h(\gamma_1,\dots,\gamma_h)'X_t\\
	\mathbb{V}ar_t(Z_{t+1,t+h}) &=& \alpha_h(\gamma_1,\dots,\gamma_h) + \beta_h(\gamma_1,\dots,\gamma_h)'X_t
	\end{eqnarray}
	where
	\begin{eqnarray*}
	a_{h+1}(\gamma_{h+1},\dots,\gamma_{1}) &=& a_h(\gamma_{h},\dots,\gamma_{1}) + a_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1})) \\
	b_{h+1}(\gamma_{h+1},\dots,\gamma_{1}) &=& b_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1}))\\
	\alpha_{h+1}(\gamma_{h+1},\dots,\gamma_{1}) &=& \alpha_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1}))\\
	&& + \alpha_{h}(\gamma_{h},\dots,\gamma_{1}) + a_1(\beta(\gamma_{h},\dots,\gamma_{1})) \\
	\beta_{h+1}(\gamma_{h+1},\dots,\gamma_{1}) &=& \beta_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1}))\\
	&& + b_1(\beta_h(\gamma_{h},\dots,\gamma_{1})),
	\end{eqnarray*}
	with
	\begin{eqnarray*}
	a_1(\gamma_1) &=& \mu \\
	b_1(\gamma_1) &=& \Phi' \gamma_1\\
	\alpha_1(\gamma_1) &=& (\gamma_1 \otimes \gamma_1)' \Sigma_0 \\
	\beta_1(\gamma_1) &=& \Sigma_1' (\gamma_1 \otimes \gamma_1).
	\end{eqnarray*}

\end{prop}





\begin{proof} 
We have:
\begin{eqnarray*}
\mathbb{E}_t(Z_{t+1,t+h+1}) &=&\mathbb{E}_t(\mathbb{E}_{t+1}[Z_{t+1,t+h+1}]) \\
&=& \mathbb{E}_t(\gamma_{h+1}'X_{t+1}+\mathbb{E}_{t+1}[Z_{t+2,t+h+1}])\\
&=& \mathbb{E}_t(\gamma_{h+1}'X_{t+1}+ a_h(\gamma_{h},\dots,\gamma_{1}) + b_h(\gamma_{h},\dots,\gamma_{1})'X_{t+1})\\
&=& a_h(\gamma_{h},\dots,\gamma_{1}) + a_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1}))\\
&&+ b_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1}))'X_t.
\end{eqnarray*}
\begin{eqnarray*}
\mathbb{V}ar_t(Z_{t+1,t+h+1}) &=& \mathbb{V}ar_t(\mathbb{E}_{t+1}(Z_{t+1,t+h+1})) + \mathbb{E}_t(\mathbb{V}ar_{t+1}(Z_{t+1,t+h+1}))\\
&=& \mathbb{V}ar_t(\gamma_{h+1}'X_{t+1} + b_h(\gamma_{h},\dots,\gamma_{1})'X_{t+1})\\
&& + \mathbb{E}_t(\alpha_{h}(\gamma_{h},\dots,\gamma_{1}) + \beta_h(\gamma_{h},\dots,\gamma_{1})'X_{t+1})\\
&=& \alpha_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1})) + \beta_1(\gamma_{h+1}+b_h(\gamma_{h},\dots,\gamma_{1}))'X_t\\
&&+\alpha_{h}(\gamma_{h},\dots,\gamma_{1}) + a_1(\beta_h(\gamma_{h},\dots,\gamma_{1})) + b_1(\beta_h(\gamma_{h},\dots,\gamma_{1}))'X_t.
\end{eqnarray*}

\begin{eqnarray*}
\mathbb{V}ar_t(\gamma_1'X_{t+1}) &=& \gamma_1' \Sigma(X_t) \gamma_1\\
&=& Vec(\mathbb{V}ar_t(\gamma_1'X_{t+1})) = (\gamma_1 \otimes \gamma_1)' (\Sigma_0 + \Sigma_1 X_t).
\end{eqnarray*}

\qed
\end{proof}




\label{app:fama70}

\begin{exmpl}[The different forms of market efficiency]

	* 	\href{http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1970.tb00518.x/full}{Fama (1970)}: in order to test whether prices correctly incorporate all relevant available information, so that deviation from predicted returns are unpredictable, the researchers need to know what these expected returns are in the first place.
	* {\color{blue} weak-form informational efficiency}: it is impossible to systematically beat the market using historical asset prices.
	* {\color{blue} semi-strong-form informational efficiency}: it is impossible to systematically beat the market using publicly available information.
	* {\color{blue} strong-form informational efficiency}: it is impossible to systematically beat the market using any information.
	* Joint hypothesis problem: impossible to test these hypotheses without simultaneously testing the appropriateness of the model.
	* In the short run, the joint-hypothesis problem is negligible: differences in expected returns is low across models; hence testing these hypotheses amount to testing the martingale properties of asset prices.

\end{exmpl}





%{Problems caused by stochastic trends}
%
%
%	* If a time series features a stochastic trend --i.e. has a unit root--, OLS estimators and OLS-based $t$-statistics have nonstandard distributions.
%	* {\color{blue}Problem 1}: Bias of autoregressive coefficient towards 0.
%	* {\color{blue}Problem 2}: Even in large samples, the $t$-statistics cannot be approximated by the normal distribution.
%	* {\color{blue}Problem 3}: Spurious regressions -- OLS-based regression analysis tends to indicate relationships between \textit{independent} unit-root series.
%
%
%
%
%
%
%\begin{exampleblock}{Problem 1: Bias towards zero}
%
%	* If $y_t$ has a unit root, the estimate of $\phi$ in the (OLS) regression $y_t = c + \phi y_{t-1} + \varepsilon_t$ is biased toward zero.
%	* In particular, we have the approximation $\mathbb{E}(\phi)=1-5.3/T$ where $T$ is the sample size.
%	* The 5th percentile of the distribution of $\phi_1$ is aproximately $1 - 14.1/T$, e.g. 0.824 for $T=80$.
%	* This notably poses problems for forecasts.
%
%\end{exampleblock}
%
%
%
%
%
%		\begin{figure}
%			\caption{Bias toward zero in the presence of unit root}
%			\includegraphics[width=1.05\linewidth]{../../cours/UNIL/figures/Figure_NonStat_Probl1.pdf}
%			
%			Note: 2000 random walk samples of size $T$ ($T=50$ for the left plot and $T=400$ for the right plot) have been simulated. For each sample, we run the OLS regression $y_t = c + \phi y_{t-1} + \varepsilon_t$. The plots show the distributions (kernel-based estimation) of the estimated $\phi$. The vertical bars indicate the means of the distributions (the red shows the mean; the blue bar gives the approximated mean, given by $1-5.3/T$).
%			
%		\end{figure}
%
%
%
%
%\label{slide:spurious}
%
%
%\begin{exampleblock}{Problems 2 and 3: Non-normal $t$ statistics and spurious regressions}
%
%	* Consider two independent non-stationary (with unit roots) variables.
%	* If we regress one on the other and if we use the standard OLS formulas to compute the standard deviation of the regression parameter, we will underestimate this standard deviation.
%	* As a result, if we use standard $t$-statistics to test for the existence of a relationship between the two variables, we will often reject the null hypothesis of no relationship ($H_0$: regression coefficient = 0).
%	* This phenomenon is called "spurious regressions" (\href{http://www.eco.uc3m.es/~jgonzalo/teaching/timeseriesMA/examplesspuriousregression.pdf}{examples of spurious regressions}).
%
%\end{exampleblock}
%
%
%
%
%
%		\begin{figure}
%			\caption{Non-normal distribution of the $t$-statistics}
%			\includegraphics[width=1.05\linewidth]{../../cours/UNIL/figures/Figure_NonStat_Probl2.pdf}
%			
%			Note: 2000 {\color{blue}independent} random walk samples $\{x_t\}$ and $\{y_t\}$, of size $T$ ($T=50$ for the left plot and $T=400$ for the right plot) have been simulated. For each sample, we run the OLS regression $y_t = c + \phi x_{t} + \varepsilon_t$. The plots show the distributions of the estimated $\phi$. The vertical bars indicate the means of the distributions. The titles of the plots report the average of the estimates of the asymptotic standard deviations of $\phi$ based on the OLS formula. It can be seen that these estimates of the standard deviations underestimate the "true" standard deviations (which are those associated to the reported distributions).
%			
%		\end{figure}
%
%
%
%
%
%
%
%
%	* Let us illustrate the influence of autocorrelation using simulations.
%	* We consider the following model:
%	$$
%	y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2).
%	$$
%	where the $x_i$s and the $\varepsilon_i$s are such that:
%	$$
%	x_i = 0.8 x_{i-1} + u_i \quad and \quad \varepsilon_i = 0.8 \varepsilon_{i-1} + v_i
%	$$
%	where the $u_i$s and the $v_i$s are i.i.d. $\mathcal{N}(0,1)$.
%	* Here is a simulated sample ($n=200$) of this model:
%
%		\begin{figure}
%			\includegraphics[width=\linewidth]{../../cours/UNIL/figures/Figure_OLS_GRM_autocorr1.pdf}
%		\end{figure}
%
%
%
%
%
%
%	* We simulate 1000 samples of the same model with $n=200$.
%	* For each sample, we compute the OLS estimate of $\beta$ (=1).
%	* Using these 1000 estimates of $b$, we construct an approximated (kernel-based) distribution of this OLS estimator (in red on the figure).
%	* For each of the 1000 OLS estimations, we employ the standard OLS variance formula ($s^2 (\bv{X}'\bv{X})^{-1}$) to estimate the variance of $b$. The blue curve is a normal distribution centred on 1 and whose variance is the average of the 1000 previous variance estimates.
%
%		\begin{figure}
%			\includegraphics[width=\linewidth]{../../cours/UNIL/figures/Figure_OLS_GRM_autocorr2.pdf}
%		\end{figure}
%
%
%
%
%
%
%	* The variance of the simulated $b$ is of 0.020 (that is the "true" one); the average of the estimated variances based on the standard OLS formula is of 0.005 ("bad" estimate); the average of the estimated variances based on the White robust covariance matrix is of 0.015 ("better" estimate).
%	* The standard OLS formula for the variance of $b$ overestimates the precision of this estimator.
%	* For about 35\% of the simulations, 1 is not included in the 95\% confidence interval of $\beta$ when the computation of the interval is based on the standard OLS formula for the variance of $b$.
%	* When the Newey-West robust covariance matrix is used, 1 is not in the 95\% confidence interval of $\beta$ for about 13\% of the simulations.
%
%
%
%
%
%
%
%	* For the sake of comparison, let us consider a model with no auto-correlation ($x_i \sim i.i.d. \mathcal{N}(0,2.8)$ and $\varepsilon_i \sim i.i.d. \mathcal{N}(0,2.8)$).
%
%		\begin{figure}
%			\includegraphics[width=\linewidth]{../../cours/UNIL/figures/Figure_OLS_GRM_autocorr3.pdf}
%		\end{figure}
%		\begin{figure}
%			\includegraphics[width=\linewidth]{../../cours/UNIL/figures/Figure_OLS_GRM_autocorr4.pdf}
%		\end{figure}
%
%
%


{Forecasts based on linear projections}\label{slide:linearproj}

\hyperlink{slide:gmm}{\beamergotobutton{back}}

\begin{thm}[Smallest MSE for linear forecasts]\label{thm:smallestMSElinear}
Among the class of linear forecasts, the smallest MSE is obtained with the linear projection of $Y_{t+1}$ on $X_t$.
This projection, denoted by $\hat{P}(Y_{t+1}|X_t):=\boldsymbol\alpha'X_t$, satisfies:
\begin{equation}(\#eq:proj}
\mathbb{E}\left( [Y_{t+1} - \boldsymbol\alpha'X_t]X_t' \right)=\bv{0}.
\end{equation}
\end{thm}

\begin{proof}
% Same idea as for Theorem \@ref(thm:smallestMSE}.
Consider the function $f:$ $\boldsymbol\alpha \rightarrow \mathbb{E}\left( [Y_{t+1} - \boldsymbol\alpha'X_t]^2 \right)$. We have:
$$
f(\boldsymbol\alpha) = \mathbb{E}\left( Y_{t+1}^2 - 2 Y_t X_t'\boldsymbol\alpha + \boldsymbol\alpha'X_t X_t'\boldsymbol\alpha] \right).
$$
We have $\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha = \mathbb{E}(-2 Y_{t+1} X_t' + 2 \boldsymbol\alpha' X_t X_t')$ (see Proposition \@ref(prop:partial}). The function is minimised for $\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha =0$.\qed
\end{proof}


	* Equation (\@ref(eq:proj}) implies: $\mathbb{E}\left( Y_{t+1}X_t' \right)=\boldsymbol\alpha'\mathbb{E}\left(X_t'X_t \right)$.
	* Hence, if $X_t'X_t$ is nonsingular,
	\begin{equation}(\#eq:linproj}
	\boldsymbol\alpha'=\mathbb{E}\left( Y_{t+1}X_t' \right)[\mathbb{E}\left(X_t'X_t \right)]^{-1}.
	\end{equation}
	* The MSE then is:
	$$
	\mathbb{E}([Y_{t+1} - \boldsymbol\alpha'X_t]^2) = \mathbb{E}{(Y_{t+1}^2)} - \mathbb{E}\left( Y_{t+1}X_t' \right)[\mathbb{E}\left(X_t'X_t \right)]^{-1}\mathbb{E}\left(X_tY_{t+1} \right).
	$$






{}\label{slide:rfBYappendix}

\hyperlink{slide:rfBY}{\beamergotobutton{back}}

\begin{exampleblock}{The risk-free short term rate in \href{http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2004.00670.x/abstract}{Bansal and Yaron (2004)}}

	* We have (Eq. A26 in BY):
	
	\begin{eqnarray*}
	r_{f,t} &=& - \log(\delta) + \frac{1}{\psi} \mathbb{E}_t(g_{t+1}) + \frac{1-\theta}{\theta} \mathbb{E}_t(r_{a,t+1} - r_{f,t}) - \frac{1}{2\theta}\mathbb{V}ar_t(m_{t,t+1})\\
	&=& - \log(\delta) + \frac{\mu + x_t}{\psi} \\
	&& +\frac{1-\theta}{\theta} \left(\sigma_t^2(-\lambda_\eta+B \lambda_{e})+ \sigma_w^2\kappa_1 A_2 \lambda_w- \frac{1}{2}\left((1+B^2)\sigma_t^2 + (A_2 \kappa_1)^2\sigma_w^2\right)\right)\\
	&& - \frac{1}{2\theta}\left[(\lambda_\eta^2 + \lambda_e^2)\sigma^2_t + \lambda_w^2 \sigma_w^2\right]\\
	&=& - \log(\delta) +  \frac{\mu}{\psi} + \frac{1-\theta}{\theta}\left(\sigma_w^2\kappa_1 A_2 \lambda_w \right.\\
	&& \left.- \frac{1}{2}(A_2 \kappa_1)^2\sigma_w^2\right)- \frac{1}{2\theta} \lambda_w^2 \sigma_w^2\\
	&& + \frac{x_t}{\psi} + \sigma^2_t \left[
	\frac{1-\theta}{\theta} \left(-\lambda_\eta+B \lambda_{e} - \frac{1}{2}(1+B^2)\right)
	- \frac{1}{2\theta}(\lambda_\eta^2 + \lambda_e^2)
	\right].
	\end{eqnarray*}
	
	* Therefore $r_{f,t}$ is an affine function of $[x_t,\sigma^2_t]'$.

\end{exampleblock}



{Proof of Proposition \@ref(prop:affineVAR}}\label{slide:proof_affineVAR}

\hyperlink{slide:affineVAR}{\beamergotobutton{back}}


	* Because $X_t$ is affine (as defined in Def. \@ref(def:affine}), we have:
	$$
	\frac{\partial }{\partial u}  \mathbb{E}_t(\exp(u'X_{t+1})) =\frac{\partial }{\partial u} \psi_t(u) = \left(\frac{\partial }{\partial u}a(u) + \frac{\partial }{\partial u}b(u)'X_t \right)\exp(a(u)+b(u)'X_t).
	$$
	* We also have:
	$$
	\frac{\partial }{\partial u} \mathbb{E}_t(\exp(u'X_{t+1})) = \mathbb{E}_t(X_{t+1}\exp(u'X_{t+1}))
	$$
	* Using that $\psi_t(0)  = 1$, we obtain:
	$$
	\mathbb{E}_t(X_{t+1}) = \left.\frac{\partial }{\partial u}a(u)\right|_{u=0} + \left.\frac{\partial }{\partial u}b(u)\right|_{u=0}'X_t.
	$$
	* In order to derive $\mathbb{V}ar_t\left(X_{t+1}\right)$ ($=\Sigma(X_t)\Sigma(X_t)'$), we compute the 2nd-order derivative of $\psi_t(u)$:
	$$
	\frac{\partial }{\partial u\partial u'} \psi_t(u) = \psi_t(u)\left[ \left(\frac{\partial }{\partial u\partial u'}a(u) + \frac{\partial }{\partial u\partial u'}b(u)'X_t \right)  + \left(\frac{\partial }{\partial u}a(u) + \frac{\partial }{\partial u}b(u)'X_t \right)
	\left(\frac{\partial }{\partial u}a(u) + \frac{\partial }{\partial u}b(u)'X_t \right)'\right].
	$$
	* $\left.\frac{\partial }{\partial u\partial u'} \psi_t(u)\right|_{u=0}=\mathbb{E}_t\left(X_{t+1}^2\right)$ $\Rightarrow$ $\mathbb{V}ar_t\left(X_{t+1}\right) = \left.\frac{\partial }{\partial u\partial u'}a(u)\right|_{u=0} + \left.\frac{\partial }{\partial u\partial u'}b(u)'X_t\right|_{u=0}$.





{Surveys (Inflation)}\label{slide:surveyinflation}
\hyperlink{slide:ATSM}{\beamergotobutton{back}}
\begin{figure}
	\includegraphics[width=1\linewidth]{figures/tab_GMR_survey1.pdf}
\end{figure}
\begin{center}

Source: [\href{XXX}{Grishchenko, Mouabbi, Renne (2016)}].

\end{center}



{Surveys (Inflation)}
\begin{figure}
	\includegraphics[width=1\linewidth]{figures/tab_GMR_survey2.pdf}
\end{figure}
\begin{center}

Source: [\href{XXX}{Grishchenko, Mouabbi, Renne (2016)}].

\end{center}



{Surveys (Inflation)}
\begin{figure}
	\includegraphics[width=1\linewidth]{figures/Figure_distri_prez1.pdf}
\end{figure}
\begin{center}

Source: [\href{XXX}{Grishchenko, Mouabbi, Renne (2016)}].

\end{center}



{}\label{slide:spectral}

\hyperlink{slide:MFA}{\beamergotobutton{back}}


	* Assumption: the fluctuations of
the underlying process are produced by a large number of elementary
cycles of different frequencies, and that the contribution of each
cycle is constant throughout the sample.
* Accordingly, the {\color{blue}spectral
density}, being a function of frequency, measures the importance of
the cosine function of that frequency as a component of a time series.
* The spectral density function can be obtained by way of the auto-covariance
function, which are readily available as soon as the model can be
written as a VAR model with stable roots.
* Specifically, for a $n_{V}$-dimensional covariance-stationary process $V_{t}$,
whose mean is given by $\overline{V}$, the spectral density function
--or population spectrum--, which associates an $n_{V}\times n_{V}$
matrix of complex numbers with the real scalar $\omega$, is given
by 

\[
s_{V}(\omega)=\frac{1}{2\pi}\sum_{k=-\infty}^{\infty}\Gamma_{k}e^{-i\omega k},
\]
 

where $\Gamma_{k}^{V}=\mathbb{E}\left[\left(V_{t}-\overline{V}\right)\left(V_{t-k}-\overline{V}\right)^{\prime}\right]$.






{}\label{slide:spectral}

\hyperlink{slide:MFA}{\beamergotobutton{back}}



* If $V_{t}$ follows a VAR(1) process 
\[
V_{t}=\Phi V_{t-1}+\Omega\varepsilon_{t},
\]
then, given that $\Gamma_{-k}^{V}=\Gamma_{k}^{V^{\prime}}$ and that
gamma $\Gamma_{k}^{V}=\Phi^{k}\Gamma_{0}^{V}$, it comes 
\[
s_{V}(\omega)=\frac{1}{2\pi}\Gamma_{0}^{V}+\frac{1}{2\pi}\sum\limits _{k=1}^{\infty}(\Phi^{k}\Gamma_{0}^{V}e^{-i\omega k}+\Gamma_{0}^{V}\Phi^{k\prime}e^{i\omega k})
\]
where $\Gamma_{0}^{V}=\Omega\Omega^{\prime}+\Phi\Omega\Omega^{\prime}\Phi^{\prime}+\ldots+\Phi^{k}\Omega\Omega^{\prime}\Phi^{k\prime}+\ldots$

* On the diagonal of $s_{V}(\omega)$: spectral density functions of the different variables of $V_{t}$ = contribution of frequency-$\omega$ cycle
to the variance of the variables in $V_{t}$.
* The off-diagonal elements
of $s_{V}(\omega)$ are complex conjugate of each other. Real  part: {\color{blue}cospectrum}; imaginary
part: {\color{blue}quadrature spectrum}. The cospectrum is proportional to the portion of the covariance
between two variables that is attributable to cycles with a given frequency. 
* A dual representation is provided by
the gain $R(\omega)$ (modulus of the complex elements in the population
spectrum) and phase measures $\varphi(\omega)$.





