[["index.html", "Introduction to Term Structure Models Introduction to Term Structure Models", " Introduction to Term Structure Models Jean-Paul Renne and Alain Monfort 2024-01-03 Introduction to Term Structure Models Modeling dynamic term structures serves as a practical and indispensable tool in the realm of finance. It enables investors, institutions, and policymakers to make informed decisions, manage risk effectively, and allocate resources wisely. By understanding how interest rates and yields evolve over time, these models offer a clear lens through which to assess market trends and price financial instruments accurately. This course has been developed by Jean-Paul Renne and Alain Monfort. It is illustrated by R codes using various packages that can be obtained from CRAN. This TSModels package is available on GitHub. To install it, one need to employ the devtools library: install.packages(&quot;devtools&quot;) # in case this library has not been loaded yet library(devtools) install_github(&quot;jrenne/TSModels&quot;) library(AEC) Useful (R) links: Download R: R software: https://cran.r-project.org (the basic R software) RStudio: https://www.rstudio.com (a convenient R editor) Tutorials: Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (by Oscar Torres-Reyna) R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (by Emmanuel Paradis) My own tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/ "],["ChapterAffine.html", "Chapter 1 Affine processes 1.1 Information in the Economy: The “factors” 1.2 Dynamic models and Laplace transform (L.T.) 1.3 Laplace Transform and moments/cumulants 1.4 Additional properties of the Laplace transform 1.5 Affine (or Car) processes 1.6 Markov chains 1.7 Wishart autoregressive (WAR) processes 1.8 Building affine processes 1.9 Multi-horizon Laplace transform 1.10 VAR representation and conditional moments 1.11 Truncated Laplace transforms of affine processes 1.12 Appendices", " Chapter 1 Affine processes 1.1 Information in the Economy: The “factors” On each date \\(t=1,2,\\dots,T\\), agents receive new information by observing factors, also called states. We denote the (\\(K\\)-dimensional) vector of factors by \\(w_t\\). Vector \\(w_t\\) is usually random. On date \\(t\\), vector \\(w_t\\) is supposed to be perfectly observed by the agents (investors), but can be only partially observed, or unobserved by the econometrician. Naturally, \\(w_t\\) can be decomposed into different sub-vectors of different natures. For instance, we can have \\(w_t = (y_t&#39;, z_t&#39;)&#39;\\) with * \\(y_t\\): observable vector of (geometric) returns, * \\(z_t\\): regime, unobserved by the econometrician. Some of the components of \\(w_t\\) can be prices. For instance, one component could be a short-term rate, a stock return, or an exchange rate. It can also include macroeconomic variables (inflation, GDP growth), or agent-specific variables. 1.2 Dynamic models and Laplace transform (L.T.) The objective of a dynamic model is to describe the random changes in \\(w_t\\). The dynamics can be historical or risk-neutral (see Chapter ??). The dynamics we consider are parametric, in the sense that the conditional distribution \\(w_{t+1}|\\underline{w_t}\\) (with \\(\\underline{w_t}=\\{w_t,w_{t-1},\\dots\\}\\)) depends on a vector of parameters \\(\\theta\\). In practice, it may be the case that \\(\\theta\\) is unknown by the econometrician (see Chapter ??). The choice (or estimation) of a conditional distribution is equivalent to the choice (or estimation) of a conditional Laplace transforms: \\[\\begin{equation} \\varphi(u|\\underline{w_t},\\theta) = \\mathbb{E}_{\\theta}[\\exp(u&#39;w_{t+1})|\\underline{w_t}], \\quad u \\in \\mathbb{R}^K,\\tag{1.1} \\end{equation}\\] or a conditional log Laplace transforms: \\[ \\psi(u|\\underline{w_t},\\theta) = \\log\\{\\mathbb{E}_{\\theta}[\\exp(u&#39;w_{t+1})|\\underline{w_t}]\\}, \\quad u \\in \\mathbb{R}^K. \\] Example 1.1 (Conditionally Bernoulli process) If \\(w_{t+1}|\\underline{w_t} \\sim {\\mathcal{I}} [p(\\underline{w_t},\\theta)]\\), then: \\[ \\varphi(u|w_t)= \\mathbb{E}[\\exp(u w_{t+1}) \\mid \\underline{w_t}] = p_t \\exp(u) + 1-p_t \\] with \\(p_t = p(\\underline{w_t}, \\theta)\\). Example 1.2 (Conditionally Binomial process) If \\(w_{t+1}|\\underline{w_t} \\in {\\mathcal{B}}(n, p_t)\\), then: \\[ \\varphi(u|w_t)=[p_t \\exp(u) + 1-p_t]^n. \\] Example 1.3 (Conditionally Poisson process) If \\(w_{t+1}|\\underline{w_t} \\sim {\\mathcal{P}}(\\lambda_t)\\), then: \\[\\begin{eqnarray*} \\varphi(u|w_t) &amp; =&amp; \\sum^\\infty_{j=0} \\dfrac{1}{j!} \\exp(-\\lambda_t) \\lambda^j_t \\exp(uj) = \\exp(-\\lambda_t) exp[\\lambda_t \\exp(u)] \\\\ &amp; =&amp; \\exp\\{\\lambda_t[\\exp(u)-1]\\}. \\end{eqnarray*}\\] Example 1.4 (Conditionally normal (or Gaussian) process) If \\(w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}\\left(m(\\underline{w_t},\\theta), \\Sigma(\\underline{w_t},\\theta)\\right)\\), then: \\[ u&#39;w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}\\left(u&#39;m(\\underline{w_t},\\theta), u&#39;\\Sigma(\\underline{w_t},\\theta)u\\right). \\] \\[ \\Rightarrow \\left\\{ \\begin{array}{ccc} \\varphi(u|\\underline{w_t},\\theta) &amp;=&amp; \\exp\\left[ \\begin{array}{l} u&#39;m(\\underline{w_t},\\theta)+ \\frac{1}{2} u&#39;\\Sigma(\\underline{w_t},\\theta)u\\end{array} \\right]\\\\ \\psi(u|\\underline{w_t},\\theta) &amp;=&amp; u&#39;m(\\underline{w_t},\\theta) + \\frac{1}{2} u&#39;\\Sigma(\\underline{w_t},\\theta)u. \\end{array} \\right. \\] 1.3 Laplace Transform and moments/cumulants Here are some properties of the Laplace transform (Eq. (1.1)): \\(\\varphi(0|\\underline{w_t},\\theta) = 1\\) and \\(\\psi(0|\\underline{w_t},\\theta)=0\\). It is defined in a convex set \\(E\\) (containing \\(0\\)). If the interior of \\(E\\) is non empty, all the (conditional) moments exist. As mentioned above, knowing the (conditional) Laplace transform is equivalent to knowing the (conditional) moments—if they exist. In the scalar case, we have that: the moment of order \\(n\\) closely relates to the \\(n^{th}\\) derivatives of \\(\\varphi\\): \\[ \\left[ \\begin{array}{l} \\dfrac{\\partial^n \\varphi(u|\\underline{w_t},\\theta)}{\\partial u^n} \\end{array} \\right]_{u=0} = \\mathbb{E}_{\\theta}[w^n_{t+1}|\\underline{w_t}], \\] the cumulant of order \\(n\\) closely relates to the \\(n^{th}\\) derivatives of \\(\\psi\\): \\[ \\left[ \\begin{array}{l} \\dfrac{\\partial^n \\psi(u|\\underline{w_t},\\theta)}{\\partial u^n} \\end{array} \\right]_{u=0} = K_n(\\underline{w_t},\\theta). \\] In particular, what precedes implies that: \\[ \\left\\{ \\begin{array}{ccc} K_1(\\underline{w_t},\\theta) &amp;=&amp; \\mathbb{E}_{\\theta}[w_{t+1}|\\underline{w_t}]\\\\ K_2(\\underline{w_t}, \\theta) &amp;=&amp; \\mathbb{V}ar_{\\theta}[w_{t+1}|\\underline{w_t}]. \\end{array} \\right. \\] Accordingly, \\(\\varphi\\) and \\(\\psi\\) are respectively called conditional moment and cumulant generating function. In the multivariate case, we have: \\[\\begin{eqnarray*} \\left[\\begin{array}{l} \\dfrac{\\partial \\psi}{\\partial u} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} &amp;=&amp; \\mathbb{E}_{\\theta}[w_{t+1}|\\underline{w_t}] \\\\ \\left[\\begin{array}{l} \\dfrac{\\partial^2 \\psi}{\\partial u\\partial u&#39;} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} &amp;=&amp; \\mathbb{V}ar_{\\theta}[w_{t+1}|\\underline{w_t}]. \\end{eqnarray*}\\] Example 1.4 (Conditionally normal (or Gaussian) process) Consider Example 1.4. Applying the previous formula, we have, in the scalar case: \\(\\psi(u|\\underline{w_t},\\theta)=u m(\\underline{w_t},\\theta) + \\frac{1}{2}u^2\\sigma^2(\\underline{w_t},\\theta)\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial \\psi}{\\partial u} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = m(\\underline{w_t},\\theta)\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial^2 \\psi}{\\partial u^2} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = \\sigma^2(\\underline{w_t},\\theta)\\). and, in the multidimensional normal case: \\(\\psi(u|\\underline{w_t},\\theta)=u&#39; m(\\underline{w_t},\\theta) + \\frac{1}{2}u&#39;\\Sigma(\\underline{w_t},\\theta)u\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial \\psi}{\\partial u} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = m(\\underline{w_t},\\theta)\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial^2 \\psi}{\\partial u\\partial u&#39;} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = \\Sigma(\\underline{w_t},\\theta)\\). In both cases, cumulants of order \\(&gt;2\\) equal to \\(0\\). 1.4 Additional properties of the Laplace transform Here are additional properties of multivariate Laplace transform: If \\(w_t=(w&#39;_{1t},w&#39;_{2t})&#39;\\) \\(, u=(u&#39;_1, u&#39;_2)&#39;\\): \\[\\begin{eqnarray*} \\mathbb{E}_{\\theta}[\\exp(u&#39;_1 w_{1,t+1}|\\underline{w_t})&amp;=&amp;\\varphi(u_1,0|\\underline{w_t},\\theta)] \\\\ \\mathbb{E}_{\\theta}[\\exp(u&#39;_2 w _{2,t+1}|\\underline{w_t})&amp;=&amp;\\varphi(0,u_2|\\underline{w_t},\\theta)]. \\end{eqnarray*}\\] If \\(w_t=(w&#39;_{1t},w&#39;_{2t})&#39;\\), and if \\(w_{1t}\\) and \\(w_{2t}\\) are conditionally independent: \\[\\begin{eqnarray*} \\varphi(u|\\underline{w_t},\\theta) &amp;=&amp; \\varphi(u_1,0|\\underline{w_t},\\theta)\\times\\varphi(0,u_2|\\underline{w_t},\\theta) \\\\ \\psi(u|\\underline{w_t},\\theta) &amp;=&amp; \\psi(u_1,0|\\underline{w_t},\\theta)+\\psi(0,u_2|\\underline{w_t},\\theta). \\end{eqnarray*}\\] If \\(w_{1t}\\) and \\(w_{2t}\\) have the same size and if \\[ \\varphi(u_1, u_2|\\underline{w_t},\\theta) = \\mathbb{E}_\\theta[\\exp(u&#39;_1 w_{1, t+1} + u&#39;_2 w_{2,t+1}|\\underline{w_t}], \\] then the conditional Laplace transform of \\(w_{1, t+1} + w_{2, t+1}\\) given \\(\\underline{w_t}\\) is \\(\\varphi(u, u|\\underline{w_t},\\theta)\\). In particular, if \\(w_{1t}\\) and \\(w_{2t}\\) are conditionally independent and have the same size, the conditional Laplace transform and Log-Laplace transform of \\(w_{1,t+1}+w_{2,t+1}\\) are respectively: \\[ \\varphi(u,0|\\underline{w_t},\\theta)\\times \\varphi(0, u|\\underline{w_t},\\theta), \\quad \\mbox{and}\\quad \\psi(u,0|\\underline{w_t},\\theta)+ \\psi(0, u|\\underline{w_t},\\theta). \\] Lemma 1.1 (Conditional zero probability for non-negative processes) If \\(w_t\\) is univariate and nonnegative its (conditional) Laplace transform \\(\\varphi_t(u) = \\mathbb{E}_t[\\exp(u w_{t+1})]\\) is defined for \\(u \\leq 0\\) and \\[ \\mathbb{P}_t(w_{t+1} = 0) = \\lim_{u\\rightarrow - \\infty} \\varphi_t(u). \\] Proof. We have \\(\\varphi_t(u) = \\mathbb{P}_t(w_{t+1} = 0) + \\int_{w_{t+1}&gt; 0} \\exp(u w_{t+1}) d\\mathbb{P}_t(w_{t+1})\\). The Lebesgue theorem ensures that the last integral converges to zero when \\(u\\) goes to \\(-\\infty\\). Lemma 1.2 (Conditional zero probability for non-negative multivariate processes) Assume that: \\(w_{1,t}\\) is valued in \\(\\mathbb{R}^{d}\\) (\\(d \\geq 1\\)), \\(w_{2,t}\\) is valued in \\(\\mathbb{R}^+ = [0, + \\infty )\\), \\(\\mathbb{E}_t \\left[ \\exp \\left( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} \\right) \\right]\\) exists for a given \\(u_1\\) and \\(u_2 \\leq 0\\). Then, we have: \\[\\begin{equation} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1}) \\textbf{1}_{\\{w_{2,t+1} = 0 \\}} \\right] = \\underset{u_2 \\rightarrow -\\infty}{\\lim} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} ) \\right].\\tag{1.2} \\end{equation}\\] Proof. We have that \\[\\begin{eqnarray*} &amp;&amp;\\underset{u_2 \\rightarrow -\\infty}{\\lim} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} ) \\right] \\\\ &amp;=&amp; \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1}) \\textbf{1}_{\\{w_{2,t+1} = 0 \\}} \\right] +\\\\ &amp;&amp; \\underset{u_2 \\rightarrow -\\infty}{\\lim} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} ) \\textbf{1}_{\\{w_{2,t+1} &gt; 0 \\}} \\right] , \\end{eqnarray*}\\] and since in the second term on the right-hand side \\(\\exp(u_2 w_{2,t+1}) \\textbf{1}_{\\{w_{2,t+1} &gt; 0 \\}} \\rightarrow 0\\) when \\(u_2 \\rightarrow -\\infty\\), Eq. (1.2) is a consequence of the Lebesgue theorem. 1.5 Affine (or Car) processes In term structure applications, we will often consider affine processes (Definitions 1.1 and 1.2). These processes are indeed such that their multi-horizon Laplace transform are simple to compute (Lemma 1.5 and Proposition 1.5), which is key to compute bond prices. 1.5.1 Car processes of order one Here is the definition of a compound auto-regressive (Car) process of order one: Definition 1.1 (Affine process of order 1) A multivariate process \\(w_{t+1}\\) is Affine of order 1 [or \\(Car(1)\\)] if \\[ \\varphi_t(u)=\\mathbb{E}_t[\\exp(u&#39;w_{t+1})]=\\exp[a(u)&#39;w_t+b(u)] \\] for some functions \\(a(.)\\) and \\(b(.)\\). These functions are univariate if \\(w_{t+1}\\) (and therefore \\(u\\)) is scalar. Note that \\(a(.)\\) and \\(b(.)\\) may be deterministic functions of time (e.g., Chikhani and Renne (2023)). A first key example is that of the Gaussian auto-regressive processes: Example 1.5 (Univariate AR(1) Gaussian process) If \\(w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\nu+\\rho w_t, \\sigma^2)\\), then: \\[ \\varphi_t(u) = \\exp\\left( \\begin{array}{l} u \\rho w_t + u \\nu + u^2 \\frac{\\sigma^2}{2} \\end{array} \\right) = \\exp[a(u)&#39;w_t+b(u)], \\] \\[ \\mbox{with }\\left\\{ \\begin{array}{ccc} a(u) &amp;=&amp; u \\rho\\\\ b(u) &amp;=&amp; u \\nu + u^2 \\dfrac{\\sigma^2}{2}. \\end{array} \\right. \\] Example 1.6 (Gaussian VAR) If \\(w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\mu+\\Phi w_t, \\Sigma)\\), then: \\[ \\varphi_t(u) = \\exp\\left( \\begin{array}{l} u&#39; (\\mu + \\Phi w_t) + \\frac{1}{2} u&#39; \\Sigma u \\end{array} \\right) = \\exp[a(u)&#39;w_t+b(u)], \\] \\[ \\mbox{with }\\left\\{ \\begin{array}{ccl} a(u) &amp;=&amp; \\Phi&#39;u\\\\ b(u) &amp;=&amp; u&#39; \\mu + \\frac{1}{2} u&#39; \\Sigma u = u&#39; \\mu + \\frac{1}{2}(u \\otimes u)&#39; vec(\\Sigma). \\end{array} \\right. \\] Example 1.7 (Quadratic Gaussian process) Consider vector \\(w_t = (x&#39;_t,vec(x_t x_t&#39;)&#39;)&#39;\\), where \\(x_t\\) is a \\(n\\)-dimensional vector following a Gaussian VAR(1), i.e. \\[ x_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\mu+\\Phi x_t, \\Sigma). \\] Proposition 1.2 shows that if \\(u = (v,V)\\) where \\(v \\in \\mathbb{R}^n\\) and \\(V\\) a square symmetric matrix of size \\(n\\), we have: \\[\\begin{eqnarray*} \\varphi_t(u) &amp;=&amp; \\mathbb{E}_t\\big\\{\\exp\\big[(v&#39;,vec(V)&#39;)\\times w_{t+1}\\big]\\big\\} \\\\ &amp; =&amp; \\exp \\left\\{a_1(v,V)&#39;x_t +vec(a_2(v,V))&#39; vec(x_t&#39;x_t) + b(v,V) \\right\\}, \\end{eqnarray*}\\] where: \\[\\begin{eqnarray*} a_2(u) &amp; = &amp; \\Phi&#39;V (I_n - 2\\Sigma V)^{-1} \\Phi \\nonumber \\\\ a_1(u) &amp; = &amp; \\Phi&#39;\\left[(I_n-2V\\Sigma)^{-1}(v+2V\\mu)\\right] \\nonumber \\\\ b(u) &amp; = &amp; u&#39;(I_n - 2 \\Sigma V)^{-1}\\left(\\mu + \\frac{1}{2} \\Sigma v\\right) +\\\\ &amp;&amp; \\mu&#39;V(I_n - 2 \\Sigma V)^{-1}\\mu - \\frac{1}{2}\\log\\big|I_n - 2\\Sigma V\\big|.\\tag{1.3} \\end{eqnarray*}\\] Quadratic processes can be used to construct positive process. Indeed, one can determine linear combinations of the components of \\(w_t\\) (\\(\\alpha&#39;w_t\\), say) that are such that \\(\\alpha&#39;w_t \\ge 0\\). For instance, if \\(x_t\\) is scalar, \\(\\alpha&#39;w_t = x_t^2\\) if \\(\\alpha = (0,1)&#39;\\). This is illustrated by Figure 1.1. T &lt;- 200 phi &lt;- .9;sigma &lt;- 1 x.t &lt;- 0; x &lt;- NULL for(t in 1:T){ x.t &lt;- phi*x.t + sigma*rnorm(1) x &lt;- c(x,x.t)} par(mfrow=c(1,2),plt=c(.1,.95,.15,.85)) plot(x,type=&quot;l&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;x_t&quot;) plot(x^2,type=&quot;l&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;x_t^2&quot;) Figure 1.1: Simulation of a quadratic processes \\(x_t\\). Another example of nonnegative process is that of the auto-regressive Gamma process (Gourieroux and Jasiak 2006) and its extension (Monfort et al. 2017). Example 1.8 (Autoregressive gamma process, ARG(1)) An ARG process is defined as follows: \\[ \\frac{w_{t+1}}{\\mu} \\sim \\gamma(\\nu+z_t) \\quad \\mbox{where} \\quad z_t \\sim \\mathcal{P} \\left( \\frac{\\rho w_t}{\\mu} \\right), \\] with \\(\\nu\\), \\(\\mu\\), \\(\\rho &gt; 0\\). (Alternatively \\(z_t \\sim {\\mathcal{P}}(\\beta w_t)\\), with \\(\\rho = \\beta \\mu\\).) Proposition 1.3 shows that we have \\(\\varphi_t(u) = \\exp[a(u)&#39;w_t+b(u)]\\) with \\[ \\left\\{ \\begin{array}{ccc} a(u) &amp;=&amp; \\dfrac{\\rho u}{1-u \\mu}\\\\ b(u) &amp;=&amp; -\\nu \\log(1-u \\mu). \\end{array} \\right. \\] One can simulate ARG processes by using this web-interface (select the “ARG” panel). It can be shown that: \\[ \\left\\{ \\begin{array}{ccc} \\mathbb{E}(w_{t+1}|\\underline{w_t}) &amp;=&amp; \\nu \\mu + \\rho w_t \\\\ \\mathbb{V}ar(w_{t+1}|\\underline{w_t}) &amp;=&amp; \\nu \\mu^2 + 2 \\mu \\rho w_t. \\end{array} \\right. \\] and that: \\[ w_{t+1}=\\nu\\mu+\\rho w_t+\\varepsilon_{t+1}, \\] where \\(\\varepsilon_{t+1}\\) is a martingale difference \\(\\Rightarrow\\) \\(w_{t+1}\\) is a weak \\(AR(1)\\). Monfort et al. (2017) porpose the extended ARG process and the ARG\\(_0\\) process. The latter is such that \\(\\nu = 0\\) and \\(\\beta w_t\\) is replaced with \\(\\alpha + \\beta w_t\\), i.e.: \\[\\begin{equation} \\frac{w_{t+1}}{\\mu} \\sim \\gamma(z_t),\\quad z_t \\sim {\\mathcal{P}}(\\alpha + \\beta w_t).\\tag{1.4} \\end{equation}\\] It is easily seen that we then have: \\[ \\varphi_t(u) = exp \\left[\\frac{\\beta \\mu u}{1-u \\mu} w_t + \\frac{\\alpha \\mu u}{1-u \\mu} \\right]. \\] The ARG\\(_0\\) process features a point mass at zero, with conditional probability \\(exp(-\\alpha - \\beta w_t)\\). Note that 0 is absorbing if \\(\\alpha = 0\\). Figure 1.2 displays the simulated path of an ARG\\(_0\\) process (since we set \\(\\nu\\) to zero). Note that function simul.ARG is included in the TSModels package. library(TSModels) W &lt;- simul.ARG(300,mu=.5,nu=0,rho=.9,alpha=.1) plot(W,type=&quot;l&quot;) Figure 1.2: Simulation of an ARG0 processes. Certain affine processes are valued in specific sets (e.g., integers). It is the case of compound Poisson proceses: Example 1.9 (Compound Poisson process) A compound Poisson process is defined as follows (with \\(\\gamma &gt; 0\\), \\(0 &lt; \\pi&lt; 1\\), and \\(\\lambda &gt; 0\\)): \\[\\frac{w_{t+1}}{\\gamma} = z_{t+1} + \\varepsilon_{t+1}, \\] where \\(z_{t+1}\\) and \\(\\varepsilon_{t+1}\\) conditionally independent, and where \\(z_{t+1} \\sim {\\mathcal B} \\left(\\frac{w_t}{\\gamma},\\pi\\right)\\), with \\(\\varepsilon_{t+1} \\sim {\\mathcal P}(\\lambda)\\). This process is valued in \\(\\{j \\gamma, j \\in \\mathbb{N}, \\gamma \\in \\mathbb{R}^+\\}\\) and we have: \\[ \\varphi_t(u) = \\exp\\left\\{ \\begin{array}{l} \\dfrac{w_t}{\\gamma} \\log[\\pi \\exp(u\\gamma)+1-\\pi]-\\lambda[1-\\exp(u \\gamma)] \\end{array} \\right\\}, \\] i.e., \\(\\varphi_t(u) = \\exp\\left(a(u)w_t+b(u)\\right)\\) with \\[ \\left\\{ \\begin{array}{ccl} a(u)&amp;=&amp; \\frac{1}{\\gamma} \\log[\\pi \\exp(u \\gamma)+1-\\pi],\\\\ b(u) &amp;=&amp; -\\lambda[1-\\exp(u \\gamma)]. \\end{array} \\right. \\] We also have: \\(w_{t+1} = \\pi w_t + \\lambda \\gamma + \\eta_{t+1}\\), where \\(\\eta_{t+1}\\) is a martingale difference. One can simulate such processes by using this web-interface (select the “Compound Poisson” panel). Figure 1.3 makes use of function simul.compound.poisson (in package TSModels) to simulate a compound Poisson process. library(TSModels) W &lt;- simul.compound.poisson(100,Gamma=.5,Pi=0.5,lambda=.9) plot(W,type=&quot;l&quot;) Figure 1.3: Simulation of a Compound Poisson process. 1.5.2 Car processes of order \\(p\\) Let us now define Car processes of order \\(p\\): Definition 1.2 (Affine process of order p) A multivariate process \\(w_{t+1}\\) is affine of order \\(p\\) [or \\(Car(p)\\)] if there exist functions \\(a_1(.),\\dots,a_p(.)\\), and \\(b(.)\\) such that: \\[ \\varphi_t(u)=\\mathbb{E}_t[\\exp(u&#39; w_{t+1})]=\\exp[a_1(u)&#39;w_t+\\dots+a_p(u)&#39;w_{t+1-p}+b(u)]. \\] It can be seen that if \\(w_t\\) is \\(Car(p)\\), then \\(W_t = [w_t&#39;, w_{t-1}&#39;,\\dots,w_{t-p+1}&#39;]&#39;\\) is \\(Car(1)\\).1 Therefore, without loss of generality we can assume \\(p = 1\\). The standard Car(\\(p\\)) processes are auto-regressive processes of order \\(p\\). These processes satisfy the definition of index affine processes: Definition 1.3 (Univariate index affine process of order p) Let \\(\\exp[a(u)w_t+b(u)]\\) be the conditional Laplace transform of a univariate affine process of order 1, the process \\(w_{t+1}\\) is an index-affine process of order \\(p\\) if: \\[ \\varphi_t(u)=\\mathbb{E}_t[\\exp(u w_{t+1})]=\\exp[a(u)(\\beta_1 w_t+\\dots+\\beta_p w_{t+1-p})+b(u)]. \\] Examples 1.10 and 1.11 are two examples of index affine processes. Example 1.10 (Gaussian AR(p) process) This example extends Example 1.5. Consider a Gaussian AR(p) process \\(w_t\\); that is: \\[ w_{t+1} = \\nu + \\varphi_1 w_t +\\dots+ \\varphi_p w_{t+1-p}+\\sigma \\varepsilon_{t+1},\\quad \\varepsilon_{t+1} \\sim i.i.d. \\mathcal{N}(0,1). \\] We have: \\[ \\varphi_t(u) = \\exp \\left[ u \\rho (\\beta_1 w_t+\\dots+\\beta_p w_{t+1-p})+u\\nu + u^2 \\frac{\\sigma^2}{2}\\right] \\] with \\(\\varphi_i = \\rho \\beta_i\\). Example 1.11 (ARG(p) process (positive)) This example extends Example 1.8. An ARG process of order \\(p\\) is defined as follows: \\[ \\frac{w_{t+1}}{\\mu} \\sim \\gamma(\\nu+z_t) \\quad \\mbox{where} \\quad z_t \\sim \\mathcal{P} \\left( \\beta_1 w_t+\\dots+\\beta_p w_{t+1-p} \\right), \\] with \\(\\nu\\), \\(\\mu\\), \\(\\beta_i &gt; 0\\). We have: \\[ \\varphi_t(u) = \\exp\\left[\\frac{\\rho u}{1-u \\mu} (\\beta_1 w_t+\\dots+\\beta_p w_{t+1-p})-\\nu \\log(1-u\\mu)\\right], \\] Process \\(w_t\\) admits the following AR(\\(p\\)) representation: \\[ w_{t+1} = \\nu\\mu + \\varphi_1 w_t +\\dots+ \\varphi_p w_{t+1-p}+\\varepsilon_{t+1}, \\] with \\(\\varphi_i = \\beta_i\\mu\\) and where \\(\\varepsilon_{t+1}\\) is a martingale difference. 1.6 Markov chains In this subsection, we show that the family of affine processes includes (some) regime-switching models. We consider a time-homogeneous Markov chain \\(z_t\\), valued in the set of columns of \\(Id_J\\), the identity matrix of dimension \\(J \\times J\\). The transition probabilities are denoted by \\(\\pi(e_i, e_j)\\), with \\(\\pi(e_i, e_j) = \\mathbb{P}(z_{t+1}=e_j | z_t=e_i)\\). With these notations: \\[ \\mathbb{E}[\\exp(v&#39;z_{t+1})|z_t=e_i,\\underline{z_{t-1}}] = \\sum^J_{j=1} \\exp(v&#39;e_j)\\pi (e_i, e_j). \\] Hence, we have: \\[ \\varphi_t(v) = \\exp[a_z(v)&#39;z_t], \\] with \\[ a_z(v)= \\left[ \\begin{array}{c} \\log \\left(\\sum^J_{j=1} \\exp(v&#39;e_j) \\pi(e_1,e_j)\\right)\\\\ \\vdots\\\\ \\log \\left(\\sum^J_{j=1} \\exp(v&#39;e_j) \\pi(e_J,e_j)\\right) \\end{array}\\right]. \\] This proves that \\(z_t\\) is an affine process. One can simulate a two-regime Markov chain by using this web-interface (select the “Markov-Switching” panel). 1.7 Wishart autoregressive (WAR) processes WAR are matrix processes, valued in the space of \\((L \\times L)\\) symmetric positive definite matrices. Definition 1.4 (Wishart autoregressive (WAR) processes) Let \\(W_{t+1}\\) be a \\(WAR_L(K, M, \\Omega)\\) process. It is defined by: \\[\\begin{eqnarray} &amp;&amp;\\mathbb{E}[\\exp Tr(\\Gamma W_{t+1})|\\underline{W_t}] \\tag{1.5}\\\\ &amp;=&amp; \\exp\\left\\{Tr[M&#39;\\Gamma(Id-2\\Omega \\Gamma)^{-1}M W_t] - \\frac{K}{2} \\log [det(Id-2\\Omega \\Gamma)]\\right\\}, \\nonumber \\end{eqnarray}\\] where \\(\\Gamma\\) is a symmetric matrix,2 \\(K\\) is a positive scalar, \\(M\\) is a \\((L \\times L)\\) matrix, and \\(\\Omega\\) is a \\((L \\times L)\\) symmetric positive definite matrix. If \\(K\\) is an integer, Proposition 1.4 (in the appendix) shows that \\(W_{t+1}\\) can be obtained from: \\[\\begin{eqnarray*} \\left\\{ \\begin{array}{ccl} W_{t+1} &amp; =&amp; \\sum^K_{k=1} x_{k,t+1} x&#39;_{k,t+1}\\\\ &amp;&amp;\\\\ x_{k,t+1} &amp; =&amp; M x_{k,t} + \\varepsilon_{k,t+1},\\quad k \\in \\{1,\\dots,K\\}, \\end{array} \\right. \\end{eqnarray*}\\] where \\(\\varepsilon_{k,t+1} \\sim i.i.d. \\mathcal{N}(0, \\Omega)\\) (independent across \\(k\\)’s). The proposition also shows that we have: \\[ \\mathbb{E}(W_{t+1}|\\underline{W_t}) = MW_tM&#39;+K \\Omega, \\] i.e. \\(W_t\\) follows a matrix weak AR(1) process. In particular case, where \\(L=1\\) (univariate case), we have that: \\[\\begin{eqnarray*} \\mathbb{E}[\\exp(u W_{t+1})|\\underline{W_t}] = \\exp\\left[ \\frac{u m^2}{1-2\\omega u}W_t - \\frac{K}{2} \\log(1-2\\omega u)\\right]. \\end{eqnarray*}\\] Hence, when \\(L=1\\), the Wishart process boils down to an \\(ARG(1)\\) process (Example 1.8) with \\(\\rho = m^2\\), \\(\\mu = 2\\omega\\), \\(\\nu = \\frac{K}{2}\\). 1.8 Building affine processes 1.8.1 Univariate affine processes with stochastic parameters Some univariate affine processes can be extended if they satisfy certain conditions. Specifically, consider a univariate affine process whose conditional L.T. is of the form: \\[\\begin{equation} \\mathbb{E}_t \\exp(u y_{t+1}) = \\exp[a_0(u)y_t+b_0(u)\\delta],\\tag{1.6} \\end{equation}\\] where \\(\\delta = (\\delta_1,\\dots,\\delta_m)&#39; \\in \\mathcal{D}\\). This process can be generalized by making \\(\\delta\\) stochastic (while staying in the affine family). More precisely assume that: \\[ \\mathbb{E}[\\exp(u y_{t+1})|\\underline{y_t}, \\underline{z_{t+1}}] = \\exp[a_0(u)y_t+b_0(u)&#39;\\Lambda z_{t+1}], \\] where \\(\\Lambda\\) is a \\((m\\times k)\\) matrix, with \\(\\Lambda z_{t+1} \\in \\mathcal{D}\\). In this case, if: \\[ \\mathbb{E}[\\exp(v&#39; z_{t+1})|\\underline{y_t}, \\underline{z_{t}}] = \\exp[a_1(v)&#39;z_t+b_1(v)], \\] then \\(w_{t+1} = (y_{t+1}, z&#39;_{t+1})&#39;\\) is affine.3 Example 1.12 (Gaussian AR(p)) Using the notation of Example 1.10, it comes that an AR(p) processes satisfies Eq. (1.6) with \\(b_0(u) = \\left(u, \\; \\frac{u^2}{2}\\right)&#39;\\) and \\(\\delta = (\\nu,\\sigma^2)&#39; \\in \\mathcal{D}=\\mathbb{R} \\times \\mathbb{R}^+\\). In that case, \\(\\delta\\) (the vector of conditional mean and variance) can be replaced by … \\(\\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right)\\), where \\(z_{1,t+1}\\) and \\(z_{2,t+1}\\) are independent AR(1) (see Example 1.5) and ARG(1) (see Example 1.8) processes, respectively. \\(\\left( \\begin{array}{ll} \\lambda&#39;_1 &amp; 0 \\\\ 0 &amp; \\lambda&#39;_2 \\end{array} \\right)\\)\\(\\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right)\\), where \\(z_{1,t+1}\\) and \\(z_{2,t+1}\\) are independent Markov chains. \\(\\left( \\begin{array}{l} \\lambda&#39;_1 \\\\ \\lambda&#39;_2 \\end{array}\\right)z_{t+1}\\), where \\(z_{t+1}\\) is a Markov chain. Example 1.11 (ARG(p) model) \\(b_0(u)= - \\nu \\log(1-u\\mu)\\), \\(\\delta=\\nu\\). \\(\\nu\\) (\\(\\ge 0\\)) can be specified for instance as a Markov chain or an ARG. 1.8.2 Multivariate affine processes One can construct multivariate affine processes by employing the so-called recursive approach. Let us illustrate this by considering the bivariate case. (The multivariate generalization is straightforward.) Consider \\(w_t = \\left(\\begin{array}{c} w_{1,t}\\\\ w_{2,t} \\end{array} \\right)\\),and assume that we have: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}[\\exp(u_1 w_{1,t+1}|\\underline{w_{1,t}}, \\underline{w_{2,t}})]\\\\ &amp;=&amp; \\exp[a_{11}(u_1)w_{1,{\\color{red}{t}}}+a_{12}(u_1)w_{2,{\\color{red}{t}}}+b_{1}(u_1)], \\end{eqnarray*}\\] and: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(u_2 w_{2,t+1}|\\underline{w_{1,t+1}}, \\underline{w_{2,t}})]\\\\ &amp;= &amp; \\exp[a_0(u_2)w_{1,{\\color{red}{t+1}}}+a_{21}(u_2)w_{1,{\\color{red}{t}}}+a_{22}(u_2)w_{2,{\\color{red}{t}}}+b_2(u_2)]. \\end{eqnarray*}\\] Then \\(w_t\\) is an affine process.4 The dynamics of the two components of \\(w_t\\) are of the form: \\[\\begin{eqnarray*} w_{1,t+1} &amp;=&amp; \\alpha_1 \\hspace{1.55cm} + \\alpha_{11}w_{1,t} + \\alpha_{12}w_{2,t} + \\varepsilon_{1,t+1} \\\\ w_{2,t+1} &amp;=&amp; \\alpha_2 + \\alpha_{0}w_{1,t+1} + \\alpha_{21}w_{1,t} + \\alpha _{22} w_{2,t} + \\varepsilon_{2,t+1} \\end{eqnarray*}\\] Note that \\(\\varepsilon_{1,t+1}\\) and \\(\\varepsilon_{2,t+1}\\) are non-correlated martingale differences. In the general case, they are conditionally heteroskedastic. What precedes is at play in \\(VAR\\) model; Monfort et al. (2017) employ this approach to build vector auto-regressive gamma (VARG) processes. 1.8.3 Extending multivariate stochastic processes Consider the same framework as in Section 1.8.1 when \\(y_t\\) is a \\(n\\)-dimensional vector. That is, replace Eq. (1.6) with: \\[\\begin{equation} \\mathbb{E}_t \\exp(u&#39; y_{t+1}) = \\exp[a_0(u)&#39;y_t+b_0(u)\\delta],\\tag{1.7} \\end{equation}\\] and further assume that \\(\\delta\\) is stochastic and depends on \\(z_t\\), such that: \\[ \\mathbb{E}[\\exp(u y_{t+1})|\\underline{y_t}, \\underline{z_{t+1}}] = \\exp[a_0(u)y_t+b_0(u)&#39;\\Lambda z_{t+1}], \\] where \\(\\Lambda\\) is a \\((m\\times k)\\) matrix, with \\(\\Lambda z_{t+1} \\in \\mathcal{D}\\). In this case, if: \\[ \\mathbb{E}[\\exp(v&#39; z_{t+1})|\\underline{y_t}, \\underline{z_{t}}] = \\exp[a_1(v)&#39;z_t+b_1(v)], \\] then \\(w_{t+1} = (y_{t+1}, z&#39;_{t+1})&#39;\\) is affine. Example 1.13 (Stochastic parameters Gaussian VAR(1)) This example extends Example 1.6. Using the same notations as in the latter example 1.6, we have \\[ b_0(u) = \\left(u&#39;, \\frac{1}{2} (u \\otimes u)&#39;\\right)&#39; \\quad \\mbox{and} \\quad\\delta = (\\mu&#39;, vec(\\Sigma)&#39;)&#39; \\in \\mathbb{R}^n \\times vec(\\mathcal{S}), \\] where \\(\\mathcal{S}\\) is the set of symmetric positive semi-definite matrices. Vector \\(\\delta\\) can be replaced by: \\[ \\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right), \\] where \\(z_{1,t+1}\\) is, for instance, a Gaussian VAR process. \\(z_{2,t+1}\\) is obtained by applying the \\(vec\\) operator to a Wishart process, replaced by \\(\\Lambda_2 z_{2,t+1}\\), where \\(\\Lambda_2\\) is a \\((n^2 \\times J)\\) matrix whose columns are \\(vec(\\Sigma_j)\\), \\(j \\in \\{1,\\dots,J\\}\\), the \\(\\Sigma_j\\) being \\((n \\times n)\\) positive semi-definite, a standardized \\(J\\)-dimensional VARG process (multivariate extension of Example 1.8). Example 1.14 (Regime-switching VAR(1)) One can also use this approach to construct (affine) regime-switching VAR processes (which is another extension of Example 1.6. For that, replace \\(\\delta\\) with \\(\\left( \\begin{array}{ll} \\Lambda_1 &amp; 0 \\\\ 0 &amp; \\Lambda_2 \\end{array} \\right)\\)\\(\\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right)\\), where \\(\\Lambda_1\\) is a \\((n \\times J_1)\\) matrix and \\(z_{1,t+1}\\) is a Markov chain valued in the set of selection vectors of size \\(J_1\\) (see Subsection 1.6), \\(\\Lambda_2\\) is the same matrix as in Example 1.13 and \\(z_{2,t+1}\\) is a Markov chain valued in the set of selection vectors of size \\(J_2\\). or \\(\\left( \\begin{array}{l} \\Lambda_1 \\\\ \\Lambda_2 \\end{array}\\right)z_{t+1}\\), where \\(\\Lambda_1\\) and\\(\\Lambda_2\\) are the same matrices as above with \\(J_1=J_2=J\\), and \\(z_{t+1}\\) is a Markov chain valued in the set of selection vectors of size \\(J\\). 1.8.4 Extended affine processes Some processes are not affine, but may be sub-components of an affine process. This can be useful to compute their conditional moments and multi-horizon Laplace transform (as one can use the formulas presented above for that, using the enlarged—affine—vector). Let us formally define an extended affine process: Definition 1.5 (Extended Affine Processes) A process \\(w_{1,t}\\) is extended affine if there exists a process \\(w_{2,t} = g(\\underline{w_{1,t}})\\) such that \\((w&#39;_{1,t}, w&#39;_{2,t})&#39;\\) is affine (of order 1). For an extended affine processes, \\(\\varphi_{1,t}(u) = \\mathbb{E}[\\exp(u&#39;w_{1,t+1})|\\underline{w_{1,t}}]\\) can be obtained from: \\[\\begin{eqnarray*} \\varphi_t(u_1, u_2) &amp;=&amp; \\mathbb{E}[\\exp(u&#39;_1w_{1,t+1}+u&#39;_2 w_{2,t+1)}|\\underline{w_{1,t}}, \\underline{w_{2,t}}] \\\\ &amp;=&amp; \\exp[a&#39;_1(u_1,u_2)w_{1,t} + a&#39;_2(u_1,u_2)w_{2,t}+b(u_1,u_2)] \\end{eqnarray*}\\] by: \\[ \\varphi_{1,t}(u) = \\varphi_t(u, 0) = \\exp[a&#39;_1(u,0)w_{1,t}+a&#39;_2(u,0)g(\\underline{w_{1,t}}) + b(u, 0)]. \\] In particular \\(w_{1,t}\\) may be non-Markovian. Similarly the multi-horizon Laplace transform (see Section 1.9) \\[ \\mathbb{E}[\\exp(\\gamma&#39;_{1}w_{1,t+1}+\\dots+\\gamma&#39;_{h}w_{1,t+h})|\\underline{w_{1,t}}] \\] can be obtained from the knowledge of the extended multi-horizon Laplace transform: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}_t[\\exp(\\{\\gamma&#39;_{1,1}w_{1,t+1}+\\gamma&#39;_{2,1}w_{2,t+1}\\}+\\dots+ \\{\\gamma&#39;_{1,h}w_{1,t+h}+\\gamma&#39;_{2,h}w_{2,t+h}\\}] \\\\ &amp;=&amp; \\exp[A&#39;_{1,t,h}(\\gamma^h_1, \\gamma^h_2)w_{1,t}+A&#39;_{2,t,h}(\\gamma^h_1, \\gamma^h_2)w_{2,t}+B_{t,h}(\\gamma^h_1, \\gamma^h_2)], \\end{eqnarray*}\\] (with \\(\\gamma^h_1 = (\\gamma&#39;_{1,1},\\dots, \\gamma&#39;_{1,h})&#39;\\), and \\(\\gamma^h_2 = (\\gamma&#39;_{2,1},\\dots, \\gamma&#39;_{2,h})&#39;\\)). We indeed have: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(\\gamma&#39;_{1}w_{1,t+1}+\\dots+\\gamma&#39;_{h}w_{1,t+h})|\\underline{w_{1,t}}]\\\\ &amp;=&amp; \\exp[A&#39;_{1,t,h}(\\gamma^h,0) w_{1,t} + A&#39;_{2,t,h}(\\gamma^h,0)g (\\underline{w_{1,t}}) + B_{t,h}(\\gamma^h,0)], \\end{eqnarray*}\\] with \\(\\gamma^h = (\\gamma_1&#39;,\\dots,\\gamma_h&#39;)&#39;\\). Example 1.15 (Affine process of order p) If \\(\\{w_{1,t}\\}\\) is affine or order \\(p&gt;1\\), then \\((w_{1,t},\\dots,w_{1,t-p+1})\\) is affine of order 1, but \\(\\{w_{1,t}\\}\\) is not affine. That is, in that case, \\(w_{2,t} = (w&#39;_{1,t-1}, \\dots.w&#39;_{1,t-p+1})&#39;\\). This a kind of extreme case since \\(w_{2,t}\\) belongs to the information at \\(t-1\\), which implies \\(a_2(u_1, u_2) = u_2\\). Example 1.16 (Gaussian ARMA process) Consider an \\(ARMA(1,1)\\) process \\[ w_{1,t} - \\varphi w_{1,t-1} = \\varepsilon_t-\\theta \\varepsilon_{t-1}, \\] with \\(|\\varphi | &lt; 1\\), \\(|\\theta| &lt; 1\\), and \\(\\varepsilon_t \\sim i.i.d. \\mathcal{N}(0, \\sigma^2)\\). \\(w_{1,t}\\) is not Markovian. Now, take \\(w_{2,t} = \\varepsilon_t = (1-\\theta L)^{-1}(1-\\varphi L)w_{1,t}\\). We have: \\[ \\left( \\begin{array}{l} w_{1,t+1} \\\\ w_{2,t+1} \\end{array} \\right) = \\left( \\begin{array}{ll} \\varphi &amp; -\\theta \\\\ 0 &amp; 0 \\end{array} \\right) \\left( \\begin{array}{l} w_{1,t} \\\\ w_{2,t} \\end{array} \\right) + \\left( \\begin{array}{l} 1 \\\\ 1 \\end{array} \\right) \\varepsilon_{t+1}. \\] Hence \\((w_{1,t}, w_{2,t})&#39;\\) is Gaussian \\(VAR(1)\\), and, therefore, it is affine of order 1. This is easily extended to \\(ARMA(p,q)\\) and \\(VARMA(p,q)\\) processes. Example 1.17 (GARCH type process) Consider process \\(w_{1,t}\\), defined by: \\[ w_{1,t+1} = \\mu + \\varphi w_{1,t} + \\sigma_{t+1} \\varepsilon_{t+1}, \\] where \\(|\\varphi| &lt; 1\\) and \\(\\varepsilon_t \\sim i.i.d. \\mathcal{N}(0,1)\\), and \\[ \\sigma^2_{t+1} = \\omega + \\alpha \\varepsilon^2_t + \\beta \\sigma^2_t, \\] where \\(0 &lt; \\beta &lt; 1\\). Consider \\(w_{2,t} = \\sigma^2_{t+1}\\) (which is a non-linear function of \\(\\underline{w_{1,t}}\\)). Proposition 1.7 shows that: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}\\left[\\exp(u_1 w_{1,t+1} + u_2 w_{2,t+1})|\\underline{w_{1,t}}\\right] \\\\ &amp;=&amp; \\exp\\left[u_1 \\mu + u_2 \\omega - \\frac{1}{2} \\log(1-2 u_2 \\alpha) \\right. \\\\ &amp;&amp;\\left. + u_1 \\varphi w_{1,t} + (u_2\\beta + \\frac{u^2_1}{2(1-2u_2\\alpha)}) w_{2,t}\\right], \\end{eqnarray*}\\] which is exponential affine in \\((w_{1,t}, w_{2,t})\\). 1.9 Multi-horizon Laplace transform 1.9.1 Recursive computation and direct pricing implications In this subsection, we show that multi-horizon Laplace transforms of affine processes can be calculated recursively. Various examples will show how this can be exploited to price long-dated financial instruments. Let us consider a multivariate process \\(w_{t}\\), affine of order one. (As explained in Subsection 1.5.2, this includes the case of the order \\(p\\) case.) For the sake of generality, we consider the case where functions \\(a(.)\\), \\(b(.)\\) are possibly deterministic functions of time, denoted in this case \\(a_{t+1}(.)\\) and \\(b_{t+1}(.)\\): \\[ \\mathbb{E}_t \\exp[(u&#39;w_{t+1})] = \\exp[a&#39;_{t+1}(u)w_t+b_{t+1}(u)]. \\] The multi-horizon Laplace transform of associated with date \\(t\\) and horizon \\(h\\) is defined by: \\[\\begin{equation} \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\mathbb{E}_t[\\exp(\\gamma&#39;_1w_{t+1}+\\dots+\\gamma&#39;_h w_{t+h})].\\tag{1.8} \\end{equation}\\] Lemma 1.5 (in the appendix) shows that we have: \\[ \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\exp(A&#39;_{t,h} w_t + B_{t,h}), \\] where \\(A_{t,h} = A^h_{t,h}\\) and \\(B_{t,h} = B^h_{t,h}\\), the \\(A^h_{t,i}, B^h_{t,i}\\) \\(i = 1,\\dots,h\\), being given recursively by: \\[ \\left\\{ \\begin{array}{ccl} A^h_{t,i} &amp;=&amp; a_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}), \\\\ B^h_{t,i} &amp;=&amp; b_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}) + B^h_{t,i-1}, \\\\ A^h_{t,0} &amp;=&amp; 0, B^h_{t,0} = 0. \\end{array} \\right. \\] If the functions \\(a_{t}\\) and \\(b_{t}\\) do not depend on \\(t\\), these recursive formulas do not depend on \\(t\\), and we get \\(\\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h)\\), for any \\(t\\), with only one recursion for each \\(h\\). Moreover, if the functions \\(a_{t}\\) and \\(b_{t}\\) do not depend on \\(t\\), and if different sequences \\((\\gamma^h_1,\\dots,\\gamma^h_h), h=1,\\dots,H\\) (say) satisfy \\(\\gamma^h_{h+1-i} = u_i\\), for \\(i=1,\\dots,h\\), and for any \\(h \\leq H\\), that is if we want to compute (reverse-order case): \\[\\begin{equation} \\varphi_{t,h}(u_h,\\dots,u_1)=\\mathbb{E}_t[\\exp(u&#39;_{{\\color{red}h}} w_{{\\color{red}t+1}}+\\dots+u&#39;_{{\\color{red}1}} w_{{\\color{red}t+h}})], \\quad h=1,\\dots,H,\\tag{1.9} \\end{equation}\\] then Proposition 1.5 (in the appendix) shows that we can compute the \\(\\varphi_{t,h}(u_h,\\dots,u_1)\\) for any \\(t\\) and any \\(h \\leq H\\) with only one recursion. That is \\(\\varphi_{t,h}(u_h,\\dots,u_1)=\\exp(A&#39;_hw_t+B_h)\\) with: \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} A_{h} &amp;=&amp; a(u_{h} + A_{h-1}), \\\\ B_{h} &amp;=&amp; b(u_{h} + A_{h-1}) + B_{h-1}, \\\\ A_{0} &amp;=&amp; 0,\\quad B_{0} = 0. \\end{array} \\right. \\end{equation*}\\] As mentioned above, what precedes has useful implications to price long-dated financial instruments such as nominal and real bonds (Examples 1.18 and 1.20, respectively), or futures (Example 1.21). Example 1.18 (Nominal interest rates) Let \\(B(t,h)\\) denote the date-\\(t\\) price of a nominal zero-coupon bond of maturity \\(h\\). We Have: \\[\\begin{equation} B(t,h) = \\mathbb{E}^{\\mathbb{Q}}_t exp (-r_{t}-\\dots-r_{t+h-1}),\\tag{1.10} \\end{equation}\\] where \\(r_{t}\\) is the nominal short rate between \\(t\\) and \\(t+1\\) (observed at \\(t\\)), and the associated (continuously-compounded) yield-to-maturity is given by: \\[\\begin{equation} R(t,h) = - \\frac{1}{h} \\log B(t,h), \\quad h=1,\\dots,H. \\end{equation}\\] If \\(r_t = \\omega&#39;w_t\\) (say), then: \\[ B(t,h) = \\exp(-r_{t}) \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-\\omega&#39; w_{t+1} - \\dots - \\omega&#39; w_{t+h-1}). \\] One can then price this bond by directly employing Eq. (1.9), with \\(u_1 = 0\\) and \\(u_i = - \\omega\\), \\(i = 2,\\dots, H\\). The price \\(B(t,h)\\) is exponential affine in \\(w_t\\), the associated yield-to-maturity \\(R(t,h)=-1/h\\log B(t,h)\\) is affine in \\(w_t\\). Example 1.19 (No-arbitrage Nelson-Siegel model) In this example, we employ the results of Example 1.18 in the context described by Christensen, Diebold, and Rudebusch (2009). Specifically, we consider a three factor model following a Gaussian VAR (see Example 1.6): \\[ w_t = \\left[\\begin{array}{c}X_{1,t}\\\\X_{2,t}\\\\X_{3,t}\\end{array}\\right] = \\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0\\\\ 0&amp;1-\\lambda&amp;\\lambda\\\\ 0&amp;0&amp;1-\\lambda\\end{array}\\right] \\left[\\begin{array}{c}X_{1,t-1}\\\\X_{2,t-1}\\\\X_{2,t-1}\\end{array}\\right] + \\left[\\begin{array}{ccc} \\sigma_{11} &amp; 0 &amp; 0\\\\ \\sigma_{21}&amp;\\sigma_{22}&amp;0\\\\ \\sigma_{31}&amp;\\sigma_{32}&amp;\\sigma_{33}\\end{array}\\right] \\left[\\begin{array}{c}\\varepsilon_{1,t}\\\\\\varepsilon_{2,t}\\\\\\varepsilon_{3,t}\\end{array}\\right], \\] where \\(\\left[\\begin{array}{c}\\varepsilon_{1,t}\\\\\\varepsilon_{2,t}\\\\\\varepsilon_{3,t}\\end{array}\\right] \\sim \\,i.i.d.\\, \\mathcal{N}(0,Id)\\). The nominal short-term rate is given by \\(r_t = X_{1,t}+X_{2,t}\\). In that case, we can use the results of Example 1.18 with \\(\\omega = (-1,-1,0)&#39;\\). The following lines of code do that: library(TSModels) lambda &lt;- .05 Phi &lt;- diag(c(1,1-lambda,1-lambda));Phi[2,3] &lt;- lambda Sigma &lt;- .0005 * diag(3) psi.parameterization=list(mu=matrix(0,3,1),Phi=Phi,Sigma=Sigma) u1 &lt;- matrix(0,3,1) u2 &lt;- matrix(c(-1,-1,0),ncol=1) H &lt;- 20 AB &lt;- reverse.MHLT(psi.GaussianVAR,u1 = u1,u2 = u2,H = H, psi.parameterization = psi.parameterization) AB$A[1:2,,] &lt;- AB$A[1:2,,] - 1 # add terms corresponding to exp(-r_t) a.yield &lt;- - AB$A / array((1:H) %x% rep(1,3),c(3,1,H)) b.yield &lt;- - AB$B / array((1:H) %x% rep(1,3),c(1,1,H)) plot(a.yield[1,,],type=&quot;l&quot;,lwd=2,ylim=c(0,1), xlab=&quot;Maturity&quot;,ylab=&quot;Factor loadings&quot;) lines(a.yield[2,,],col=&quot;red&quot;,lwd=2,lty=2) lines(a.yield[3,,],col=&quot;blue&quot;,lwd=2,lty=3) Figure 1.4: Factor loadings in the context of a no-arbitrage nelson-Siegel model (Christensen, Diebold and Rudebusch, 2009). The first factor (black solid line) is a level factor. The second and third factors (red dashed line and blue dotted line, respectively) are slope factors. In the previous example, note the use of function reverse.MHLT (in package TSModels), that notably takes a L.T. as an argument (psi). In the previous example, we consider a Gaussian VAR, and we therefore assign psi.GaussianVAR to psi. We then need to provide function reverse.MHLT with the arguments of the psi function. These arguments are provided in the form of a list (input psi.parameterization). Example 1.20 (Real interest rates) Denote by \\(q_t\\) the price index on date \\(t\\) and by \\(\\pi_{t+1} = \\log \\dfrac{q_{t+1}}{q_t}\\) the inflation rate on date \\(t+1\\). We have: \\[\\begin{eqnarray*} \\bar{R}(t,h) &amp; =&amp; - \\frac{1}{h} \\log \\bar{B}(t,h), \\quad h=1,\\dots,H \\\\ \\\\ \\bar{B}(t,h) &amp; =&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t}-\\dots-r_{t+h-1} + \\pi_{t+1}+\\dots+\\pi_{t+h}), \\\\ \\\\ &amp; =&amp; \\exp(-r_{t}) \\times \\\\ &amp;&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t+1}-\\dots-r_{t+h-1}+\\pi_{t+1}+\\dots+\\pi_{t+h}) \\end{eqnarray*}\\] If \\(r_t = \\omega&#39;w_t\\) and \\(\\pi_t = \\bar\\omega&#39;w_t\\), then \\(\\bar{B}(t,h)\\) is given by: \\[\\begin{eqnarray*} \\exp(-r_{t}) \\mathbb{E}^{\\mathbb{Q}}_t exp[(\\bar\\omega-\\omega)&#39;w_{t+1}+\\dots+(\\bar\\omega-\\omega)&#39;w_{t+h-1}+\\bar\\omega&#39; w_{t+h}] \\end{eqnarray*}\\] One can then price this bond by directly employing Eq. (1.9), with \\(u_1 = \\bar\\omega\\) and \\(u_i = \\bar\\omega-\\omega\\), \\(i = 2,\\dots, H\\). Example 1.21 (Futures) Denote by \\(F(t,h)\\) the date-\\(t\\) price of a future of maturity \\(h\\) (see Section XXX). That is \\(F(t,h) = \\mathbb{E}^{\\mathbb{Q}}_t (S_{t+h})\\), \\(h=1,\\dots,H\\), where \\(S_t\\) is the date-\\(t\\) price of the underlying asset. If \\(w_t = (\\log S_t, x&#39;_t)&#39;\\) then \\(F(t,h) = \\mathbb{E}^{\\mathbb{Q}}_t \\exp(e&#39;_1 w_{t+h})\\). This can be calculated by using Eq. (1.9) with \\(u_1 = e_1\\), and \\(u_i = 0\\), for \\(i=2,\\dots,H\\). If \\(w_t = (y_t, x&#39;_t)&#39;\\) with \\(y_t = \\log\\frac{S_t}{S_{t-1}}\\), then \\(F(t,h) = S_t \\mathbb{E}^{\\mathbb{Q}}_t \\exp(e&#39;_1 w_{t+1}+\\dots+e&#39;_1 w_{t+h})\\). This can be calculated by using Eq. (1.9) with \\(u_i = e&#39;_1\\), \\(i=1,\\dots,H\\). 1.9.2 Exponential payoff Consider an asset providing the payoff \\(\\exp(\\nu&#39; w_{t+h})\\) on date \\(t+h\\). Its price is given by: \\[ P(t,h;\\nu) = \\mathbb{E}^{\\mathbb{Q}}_t[\\exp(-r_{t}-\\dots-r_{t+h-1}) \\exp(\\nu&#39; w_{t+h})]. \\] If \\(r_t = \\omega&#39;w_t\\), we have: \\[ P(t,h;\\nu) = \\exp(-r_{t})\\mathbb{E}^{\\mathbb{Q}}_t \\left(exp[-\\omega&#39; w_{t+1}-\\dots-\\omega&#39; w_{t+h-1}+ \\nu&#39; w_{t+h}]\\right), \\] which can be calculated by Eq. (1.9), with \\(u_1 = \\nu\\) and \\(u_i = -\\omega\\) for \\(i = 2,\\dots,H\\). What precedes can be extended to the case where the payoff (settled on date \\(t+h\\)) is of the form: \\[ (\\nu_1&#39;w_{t+h}) \\exp(\\nu_2&#39; w_{t+h}). \\] Indeed, we have \\[ \\left[\\frac{\\partial \\exp[(s \\nu_1+ \\nu_2)&#39;w_{t+h}]}{\\partial s}\\right]_{s=0} = (\\nu_1&#39;w_{t+h}) \\exp(\\nu_2&#39; w_{t+h}). \\] Therefore: \\[\\begin{eqnarray} &amp;&amp;\\mathbb{E}_t^{\\mathbb{Q}}[\\exp(-r_t - \\dots - r_{t+h-1})(\\nu_1&#39;w_{t+h}) \\exp(\\nu_2&#39; w_{t+h})] \\nonumber\\\\ &amp;=&amp; \\left[ \\frac{\\partial P(t,h;s \\nu_1 + \\nu_2)}{\\partial s} \\right]_{s=0}.\\tag{1.11} \\end{eqnarray}\\] This method is easily extended to price payoffs of the form \\((\\nu_1&#39;w_{t+h})^k \\exp(\\nu_2&#39; w_{t+h})\\), with \\(k \\in \\mathbb{N}\\). 1.10 VAR representation and conditional moments An important property of affine processes is that their dynamics can be written as a vector-autoregressive process. This is useful to compute conditional moments of the process. Proposition 1.1 (VAR representation of an affine process' dynamics) If \\(w_t\\) is the affine process whose Laplace transform is defined in Def. 1.1, then its dynamics admits the following vectorial autoregressive representation: \\[\\begin{equation} w_{t+1} = \\mu + \\Phi w_{t} + \\Sigma^{\\frac{1}{2}}(w_t) \\varepsilon_{t+1},\\tag{1.12} \\end{equation}\\] where \\(\\varepsilon_{t+1}\\) is a difference of martingale sequence whose conditional covariance matrix is the identity matrix and where \\(\\mu\\), \\(\\Phi\\) and \\(\\Sigma(w_t) = \\Sigma^{\\frac{1}{2}}(w_t){\\Sigma^{\\frac{1}{2}}(w_t)}&#39;\\) satisfy: \\[\\begin{equation} \\mu = \\left[\\frac{\\partial }{\\partial u}b(u)\\right]_{u=0}, \\quad \\Phi= \\left[\\frac{\\partial }{\\partial u}a(u)&#39;\\right]_{u=0}\\tag{1.13} \\end{equation}\\] \\[\\begin{equation} \\Sigma(w_t) = \\left[\\frac{\\partial }{\\partial u\\partial u&#39;}b(u)\\right]_{u=0} + \\left[\\frac{\\partial }{\\partial u\\partial u&#39;}a(u)&#39;w_t\\right]_{u=0}.\\tag{1.14} \\end{equation}\\] Proof. When \\(w_t\\) is affine, its (conditional) cumulant generating function is of the form \\(\\psi(u)=a(u)&#39;w_t+b(u)\\). The result directly follows from the formulas given in Section 1.3. Proposition 1.6 (in the appendix) shows that the conditional means and variances of \\(w_t\\) are given by: \\[\\begin{eqnarray} \\mathbb{E}_t(w_{t+h}) &amp;=&amp; (I - \\Phi)^{-1}(I - \\Phi^h)\\mu + \\Phi^h w_t \\tag{1.15}\\\\ \\mathbb{V}ar_t(w_{t+h}) &amp;=&amp; \\Sigma(\\mathbb{E}_t(w_{t+h-1}))+\\Phi \\Sigma(\\mathbb{E}_t(w_{t+h-2}))\\Phi&#39; + \\nonumber \\\\ &amp;&amp; \\dots + \\Phi^{h-1} \\Sigma(w_{t}){\\Phi^{h-1}}&#39;. \\tag{1.16} \\end{eqnarray}\\] Eq. (1.16) notably implies that \\(\\mathbb{V}ar_t(w_{t+h})\\) is an affine function of \\(w_t\\). Indeed \\(\\Sigma(.)\\) is an affine function, and the conditional expectations \\(\\mathbb{E}_t(w_{t+h})\\) are affine in \\(w_t\\), as shown by Eq. (1.15). The unconditional means and variances are given by: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} \\mathbb{E}(w_t) &amp;=&amp; (I - \\Phi)^{-1}\\mu\\\\ vec[\\mathbb{V}ar(w_t)] &amp;=&amp; (I_{n^2} - \\Phi \\otimes \\Phi)^{-1} vec\\left(\\Sigma[(I - \\Phi)^{-1}\\mu]\\right). \\end{array} \\right.\\tag{1.17} \\end{equation}\\] 1.11 Truncated Laplace transforms of affine processes In this section, we show how one can employ Fourier transforms to compute truncated conditional moments of affine processes. For that, let us introduce the following notation: \\[ w_{t+1,T} = (w&#39;_{t+1}, w&#39;_{t+2},\\dots, w&#39;_T)&#39; \\] with \\(w_t\\) affine \\(n\\)-dimensional process. We want to compute: \\[ \\tilde{\\varphi}_t(u ; v, \\gamma) = \\mathbb{E}_t[\\exp(u&#39;w_{t+1,T})\\textbf{1}_{\\{v&#39;w_{t+1,T}&lt;\\gamma\\}}]. \\] Consider the complex untruncated conditional Laplace transform: \\[ \\varphi_t(z) = \\mathbb{E}_t[\\exp(z&#39;w_{t+1,T})],\\quad z \\in \\mathbb{C}^{nT}, \\] computed using the same recursive algorithm as in the real case (see Section 1.9). Duffie, Pan, and Singleton (2000) have shown that we have (see also Proposition 1.8 in the appendix): \\[\\begin{equation} \\tilde{\\varphi}_t(u ; v, \\gamma) = \\frac{\\varphi_t(u)}{2} - \\frac{1}{\\pi} \\int^\\infty_0 \\frac{Im[\\varphi_t(u+ivx) \\exp(-i\\gamma x)]}{x} dx.\\tag{1.18} \\end{equation}\\] where \\(Im\\) means imaginary part. Note that the integral in Eq. (1.18) is one dimensional (whatever the dimension of \\(w_t\\)). As shown in the following example, this can be exploited to price options. Example 1.22 (Option pricing) Pricing calls and puts amounts to conditional expectations of the type (with \\(k &gt; 0\\)): \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}_t\\left([\\exp(u&#39;_1 w_{t+1,T})-k \\exp(u&#39;_2 w_{t+1,T})]^+\\right) \\\\ &amp;= &amp; \\mathbb{E}_t\\left([\\exp(u&#39;_1 w_{t+1,T})-k \\exp(u&#39;_2 w_{t+1,T})]\\textbf{1}_{\\{[\\exp(u_1-u_2)&#39;w_{t+1,T}] &gt; k \\}}\\right) \\\\ &amp;= &amp; \\tilde{\\varphi}_t(u_1 ; u_2-u_1, - \\log k) - k \\tilde{\\varphi}_t(u_2 ; u_2-u_1, - \\log k). \\end{eqnarray*}\\] Example 1.23 (Exogenous short rate) Consider an asset whose date-\\(t\\) price is \\(p_t\\). Denote its geometric asset return by \\(y_t\\), i.e., \\(y_t = \\log(p_t/p_{t-1})\\). Consider an option written on this asset, with a strike equal \\(k p_t\\). If interest rates are deterministic, the option price, for a maturity \\(h\\), is given by: \\[ p_t \\exp(-r_{t}-\\dots-r_{t+h-1}) \\mathbb{E}^{\\mathbb{Q}}_t[\\exp u&#39;_1 w_{t+1, t+h} - k]^+ \\] with \\(u_1 = e \\otimes e_1\\), where \\(e\\) is the \\(h\\)-dimensional vector with components equal to 1, and \\(e_1\\) is the \\(n\\)-vector selecting the 1st component (\\(y_t\\) being the 1st component of \\(w_t\\), say). Example 1.24 (Endogenous short rate) Consider the same context as in Example 1.23, but with a stochastic (endogenous) short-term rate. For instance, assume that \\(r_{t+1} = \\omega_0 + \\omega&#39;_1 w_t\\). The option price then is: \\[\\begin{eqnarray*} &amp;&amp; p_t \\mathbb{E}^{\\mathbb{Q}}_t \\left[ \\exp(-\\omega_0 - \\omega&#39;_1 w_t-\\dots- \\omega_0 - \\omega&#39;_1 w_{t+h-1}) [\\exp(u&#39;_1 w_{t+1,t+h})-k]^+ \\right]\\\\ &amp;= &amp; p_t \\exp(-h \\omega_0 - \\omega&#39;_1 w_t)\\mathbb{E}^{\\mathbb{Q}}_t\\left(\\left[\\exp(\\tilde{u}&#39;_1w_{t+1,t+h})-k \\exp(u_2 w_{t+1, t+h})\\right]^+\\right), \\end{eqnarray*}\\] with \\(\\tilde{u}&#39;_1 = u_1 + u_2\\), [\\(u_1 = e \\otimes e_1\\) as before], and \\(u_2 = (-\\omega&#39;_1,\\dots, -\\omega&#39;_1, 0)&#39;\\). Example 1.25 (Numerical example: Conditional cumulated distribution function (c.d.f.)) Let us use the model used in Example 1.19. Suppose we want to compute the conditional distribution of the average interest rate over the next \\(H\\) periods, i.e., \\(\\frac{1}{H}(r_{t+1}+\\dots+r_{t+H})\\). Hence, we want to compute \\(\\mathbb{E}_t[\\textbf{1}_{\\{v&#39;w_{t+1,T}&lt;\\gamma\\}}]\\) with \\(v&#39;w_{t+1,T}=\\frac{1}{H}(r_{t+1}+\\dots+r_{t+H})\\). H &lt;- 10 X &lt;- matrix(c(0.01,.02,0),3,1) x &lt;- exp(seq(-10,10,length.out=1000)) u1 &lt;- matrix(c(1/H,1/H,0),3,1) %*% matrix(1i*x,nrow=1);u2 &lt;- u1 AB &lt;- reverse.MHLT(psi.GaussianVAR,u1 = u1,u2 = u2,H = H, psi.parameterization = psi.parameterization) s1 &lt;- matrix(exp(t(X) %*% AB$A[,,H] + AB$B[,,H]),ncol=1) dx &lt;- matrix(x-c(0,x[1:length(x)-1]),length(x),1) gamma &lt;- seq(-.2,.3,length.out=1000) fx &lt;- outer(x,gamma,function(r,c){Im(s1[,1]*exp(-1i*r*c))/r})*dx[,1] f &lt;- 1/2 - 1/pi * apply(fx,2,sum) plot(gamma,f,type=&quot;l&quot;,xlab=&quot;&quot;,lwd=2) Figure 1.5: Conditional cumulated distribution function (c.d.f.) of \\(\\frac{1}{H}(r_{t+1}+\\dots+r_{t+H})\\). 1.12 Appendices Lemma 1.3 If \\(\\mu \\in \\mathbb{R}^L\\) and \\(Q\\) is a \\((L \\times L)\\) matrix symmetric positive definite, then: \\[ \\int_{\\mathbb{R}^{L}} \\exp(-u&#39;Q u + \\mu&#39;u)du = \\frac{\\pi^{L/2}}{(det Q)^{1/2}} exp \\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right). \\] Proof. The integral is: \\[\\begin{eqnarray*} &amp;&amp; \\int_{\\mathbb{R}^{L}} exp \\left[ \\begin{array}{l} - (u - \\frac{1}{2} Q^{-1} \\mu)&#39; Q (u - \\frac{1}{2} Q^{-1} \\mu)&#39; \\end{array} \\right] exp\\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right)du \\\\ &amp;=&amp; \\frac{\\pi^{L/2}}{(det Q)^{1/2}} exp\\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right) \\end{eqnarray*}\\] [using the formula for the unit mass of \\(\\mathcal{N}( 0.5Q^{-1}\\mu,(2Q)^{-1})\\)]. Lemma 1.4 If \\(\\varepsilon_{t+1} \\sim \\mathcal{N}(0,Id)\\), we have \\[ \\mathbb{E}_t \\left(\\exp[\\lambda&#39;\\varepsilon_{t+1}+\\varepsilon&#39;_{t+1} V \\varepsilon_{t+1}]\\right) = \\frac{1}{[\\det(I-2V)]^{1/2}} \\exp\\left[ \\frac{1}{2} \\lambda&#39;(I-2V)^{-1}\\lambda \\right]. \\] Proof. We have \\[ \\mathbb{E}_t \\exp(\\lambda&#39;\\varepsilon_{t+1}+\\varepsilon&#39;_{t+1}V\\varepsilon_{t+1}) = \\frac{1}{(2\\pi)^{n/2}} \\int_{\\mathbb{R}^{n}} \\exp\\left[ \\begin{array}{l} -u&#39;\\left( \\begin{array}{l} \\frac{1}{2} I-V \\end{array} \\right)u+\\lambda&#39;u \\end{array} \\right]du \\] From Lemma 1.3, if \\(u\\in\\mathbb{R}^n\\), then \\[ \\int_{\\mathbb{R}^{n}} \\exp(-u&#39; Q u+\\mu&#39;u) du = \\frac{\\pi^{n/2}}{(\\det Q)^{1/2}} \\exp\\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right). \\] Therefore: \\[ \\begin{array}{l} \\mathbb{E}_t \\exp(\\lambda&#39;\\varepsilon_{t+1}+\\varepsilon&#39;_{t+1}V\\varepsilon_{t+1}) \\\\ = \\frac{1}{2^{n/2} \\left[ \\begin{array}{l} \\det \\left( \\begin{array}{l} \\frac{1}{2} I-V \\end{array} \\right) \\end{array} \\right]^{1/2} } \\exp\\left[ \\begin{array}{l} \\frac{1}{4} \\lambda&#39;\\left( \\begin{array}{l} \\frac{1}{2} I-V \\end{array} \\right)^{-1}\\lambda \\end{array} \\right]. \\end{array} \\] Proposition 1.2 (Quadratic Gaussian process) Consider vector \\(w_t = (x&#39;_t,vec(x_t x_t&#39;)&#39;)&#39;\\), where \\(x_t\\) is a \\(n\\)-dimensional vector following a Gaussian VAR(1), i.e. \\[ x_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\mu+\\Phi x_t, \\Sigma). \\] If \\(u = (v,V)\\) where \\(v \\in \\mathbb{R}^n\\) and \\(V\\) a square symmetric matrix of size \\(n\\), we have: \\[\\begin{eqnarray*} \\varphi_t(u) &amp;=&amp; \\mathbb{E}_t\\big\\{\\exp\\big[(v&#39;,vec(V)&#39;)\\times w_{t+1}\\big]\\big\\} \\\\ &amp; =&amp; \\exp \\left\\{a_1(v,V)&#39;x_t +vec(a_2(v,V))&#39; vec(x_t&#39;x_t) + b(v,V) \\right\\}, \\end{eqnarray*}\\] where: \\[\\begin{eqnarray*} a_2(u) &amp; = &amp; \\Phi&#39;V (I_n - 2\\Sigma V)^{-1} \\Phi \\nonumber \\\\ a_1(u) &amp; = &amp; \\Phi&#39;\\left[(I_n-2V\\Sigma)^{-1}(v+2V\\mu)\\right] \\nonumber \\\\ b(u) &amp; = &amp; u&#39;(I_n - 2 \\Sigma V)^{-1}\\left(\\mu + \\frac{1}{2} \\Sigma v\\right) +\\\\ &amp;&amp; \\mu&#39;V(I_n - 2 \\Sigma V)^{-1}\\mu - \\frac{1}{2}\\log\\big|I_n - 2\\Sigma V\\big|.\\tag{1.3} \\end{eqnarray*}\\] Proof. We have: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}_t(\\exp(v&#39; x_{t+1} + vec(V)&#39;vec(x_{t+1} x_{t+1}&#39;))) \\\\ &amp;=&amp; \\mathbb{E}_t[\\exp(v&#39; (\\mu + \\Phi x_t + \\Sigma^{1/2}\\varepsilon_{t+1}) + \\\\ &amp;&amp; vec(V)&#39;vec((\\mu + \\Phi x_t + \\Sigma^{1/2}\\varepsilon_{t+1}) (\\mu + \\Phi x_t + \\Sigma^{1/2}\\varepsilon_{t+1})&#39;))] \\\\ &amp;=&amp; \\exp[v&#39; (\\mu + \\Phi x_t) + vec(V)&#39;vec\\{(\\mu + \\Phi x_t)(\\mu + \\Phi x_t)&#39;\\}] \\times \\\\ &amp;&amp; \\mathbb{E}_t[\\exp(v&#39;\\Sigma^{1/2}\\varepsilon_{t+1} +2\\underbrace{ vec(V)&#39; vec\\{(\\mu + \\Phi x_t)(\\varepsilon_{t+1}&#39;{\\Sigma^{1/2}}&#39;)\\}}_{=(\\mu + \\Phi x_t)&#39;V\\Sigma^{1/2}\\varepsilon_{t+1}} +\\\\ &amp;&amp; \\underbrace{vec(V)&#39;vec\\{(\\Sigma^{1/2}\\varepsilon_{t+1})(\\Sigma^{1/2}\\varepsilon_{t+1})&#39;}_{=\\varepsilon_{t+1}&#39;{\\Sigma^{1/2}}&#39;V\\Sigma^{1/2}\\varepsilon_{t+1}}\\}] \\end{eqnarray*}\\] Lemma 1.4 can be used to compute the previous conditional expectation, with \\(\\lambda = {\\Sigma^{1/2}}&#39;(v + 2 V&#39;(\\mu + \\Phi x_t))\\). Some algebra then leads to the result. Proposition 1.3 () Consider the following auto-regressive gamma process: \\[ \\frac{w_{t+1}}{\\mu} \\sim \\gamma(\\nu+z_t) \\quad \\mbox{where} \\quad z_t \\sim \\mathcal{P} \\left( \\frac{\\rho w_t}{\\mu} \\right), \\] with \\(\\nu\\), \\(\\mu\\), \\(\\rho &gt; 0\\). (Alternatively \\(z_t \\sim {\\mathcal{P}}(\\beta w_t)\\), with \\(\\rho = \\beta \\mu\\).) We have: \\(\\varphi_t(u) = exp \\left[ \\begin{array}{l} \\dfrac{\\rho u}{1-u \\mu} w_t - \\nu \\log(1-u \\mu)\\end{array} \\right], \\mbox{ for } u &lt; \\dfrac{1}{\\mu}\\). Proof. Given \\(\\underline{w_t}\\), we have \\(z_t \\sim {\\mathcal P}\\left( \\begin{array}{l} \\frac{\\rho w_t} {\\mu} \\end{array}\\right)\\). We have: \\[\\begin{eqnarray*} \\mathbb{E}[\\exp(u w_{t+1})|\\underline{w_t}] &amp;=&amp; \\mathbb{E}\\left\\{\\mathbb{E}\\left[\\exp \\left(u \\mu \\frac{w_{t+1}}{\\mu}\\right)|\\underline{w_t}, \\underline{z}_t\\right]\\underline{w_t}\\right\\}\\\\ &amp;=&amp; \\mathbb{E}[(1-u\\mu)^{-(\\nu+z_t)}|\\underline{w_t}] \\\\ &amp;=&amp; (1-u\\mu)^{-\\nu}\\mathbb{E}\\{\\exp[-z_t \\log(1-u\\mu)]|\\underline{w_t}\\} \\\\ &amp;=&amp; (1-u\\mu)^{-\\nu} \\exp \\left\\{\\frac{\\rho w_t}{\\mu}[\\exp(-\\log(1-u\\mu)] - \\frac{\\rho w_t}{\\mu}\\right\\}\\\\ &amp;=&amp; \\exp\\left[ \\begin{array}{l} \\frac{\\rho u w_t}{1-u\\mu} - \\nu \\log(1-u\\mu) \\end{array}\\right], \\end{eqnarray*}\\] using the fact that the L.T. of \\(\\gamma(\\nu)\\) is \\((1-u)^{-\\nu}\\) and that the L.T. of \\({\\mathcal P}(\\lambda)\\) is \\(\\exp[\\lambda(\\exp(u)-1)]\\). Proposition 1.4 (Dynamics of a WAR process) If \\(K\\) is an integer, \\(W_{t+1}\\) can be obtained from: \\[\\begin{eqnarray*} \\left\\{ \\begin{array}{ccl} W_{t+1} &amp; =&amp; \\sum^K_{k=1} x_{k,t+1} x&#39;_{k,t+1}\\\\ &amp;&amp;\\\\ x_{k,t+1} &amp; =&amp; M x_{k,t} + \\varepsilon_{k,t+1},\\quad k \\in \\{1,\\dots,K\\}, \\end{array} \\right. \\end{eqnarray*}\\] where \\(\\varepsilon_{k,t+1} \\sim i.i.d. \\mathcal{N}(0, \\Omega)\\) (independent across \\(k\\)’s). In particular, we have: \\[ \\mathbb{E}(W_{t+1}|\\underline{W_t}) = MW_tM&#39;+K \\Omega, \\] i.e. \\(W_t\\) follows a matrix weak AR(1) process. Proof. For \\(K=1\\), \\(W_{t+1}=x_{t+1} x&#39;_{t+1}\\), \\(x_{t+1} = M x_t + \\Omega^{1/2} u_{t+1}\\) and \\(u_{t+1} \\sim i.i.d. \\mathcal{N}(0,Id_L)\\). We have: \\[ \\mathbb{E}[\\exp(Tr \\Gamma W_{t+1})|\\underline{w_t}] = \\mathbb{E}\\{\\mathbb{E}[\\exp(Tr \\Gamma x_{t+1} x&#39;_{t+1})|\\underline{x}_t]|\\underline{w_t}\\} \\] and: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(Tr \\Gamma x_{t+1}x&#39;_{t+1})|\\underline{x}_t] = \\mathbb{E}[\\exp(x&#39;_{t+1}\\Gamma x_{t+1}|\\underline{x}_t] \\\\ &amp;=&amp; \\mathbb{E}[\\exp(M x_t + \\Omega^{1/2} u_{t+1})&#39;\\Gamma(M x_t + \\Omega^{1/2} u_{t+1})/x_t] \\\\ &amp;=&amp; \\exp(x&#39;_tM&#39;\\Gamma M x_t)\\mathbb{E}[\\exp(2 x&#39;_t M&#39;\\Gamma \\Omega^{1/2} u_{t+1}+u&#39;_{t+1}\\Omega^{1/2} \\Gamma \\Omega^{1/2} u_{t+1})/x_t] \\\\ &amp;=&amp; \\frac{exp(x&#39;_tM&#39;\\Gamma M x_t)}{(2\\pi)^{L/2}} \\times \\\\ &amp;&amp; \\int_{\\mathbb{R}^L} \\exp\\left[2x&#39;_tM&#39;\\Gamma \\Omega^{1/2}u_{t+1}-u&#39;_{t+1}\\left( \\frac{1}{2} Id_L-\\Omega^{1/2} \\Gamma \\Omega^{1/2}\\right)u_{t+1}\\right] du_{t+1}. \\end{eqnarray*}\\] Using Lemma 1.3 with \\(\\mu&#39; = 2 x&#39;_t M&#39;\\Gamma \\Omega^{1/2}, Q = \\frac{1}{2} Id_L-\\Omega^{1/2}\\Gamma\\Omega^{1/2}\\) \\ and after some algebra, the RHS becomes: \\[ \\frac{exp[x&#39;_tM&#39;\\Gamma(Id_L-2\\Omega\\Gamma)^{-1}M x_t]}{det[Id_L-2\\Omega^{1/2}\\Gamma\\Omega^{1/2}]} = \\frac{exp Tr[M&#39;\\Gamma(Id_L-2\\Omega^{-1}]M W_t]}{det[Id_L-2\\Omega \\Gamma]^{1/2}}, \\] which depends on \\(x_t\\) through \\(W_t\\), and gives the result for \\(K=1\\); the result for any \\(K\\) integer follows. Lemma 1.5 We have: \\[ \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\exp(A&#39;_{t,h} w_t + B_{t,h}), \\] where \\(A_{t,h} = A^h_{t,h}\\) and \\(B_{t,h} = B^h_{t,h}\\), the \\(A^h_{t,i}, B^h_{t,i}\\) \\(i = 1,\\dots,h\\), being given recursively by: \\[ (i) \\left\\{ \\begin{array}{ccl} A^h_{t,i} &amp;=&amp; a_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}), \\\\ B^h_{t,i} &amp;=&amp; b_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}) + B^h_{t,i-1}, \\\\ A^h_{t,0} &amp;=&amp; 0, B^h_{t,0} = 0. \\end{array} \\right. \\] Proof. For any \\(j=1,\\dots,h\\) we have: \\[ \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\mathbb{E}_t[\\exp(\\gamma&#39;_1 w_{t+1}+\\dots\\gamma&#39;_j w_{t+j}+A^{h&#39;}_{t,h-j}w_{t+j}+B^h_{t,h-j})] \\] where: \\[ (ii) \\left\\{ \\begin{array}{l} A^h_{t,h-j+1} = a_{t+j}(\\gamma_{j} + A^h_{t,h-j}), \\\\ B^h_{t,h-j+1} = b_{t+j}(\\gamma_{j} + A^h_{t,h-j}) + B^h_{t,h-j}, \\\\ A^h_{t,0} = 0, B^h_{t,0} = 0. \\end{array} \\right. \\] Since this is true for \\(j=h\\), and if this is true for \\(j\\), we get: \\[ \\begin{array}{ll} \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) &amp; = \\mathbb{E}_t [\\exp(\\gamma&#39;_1 w_{t+1}+\\dots+\\gamma&#39;_{j-1}w_{t+j-1}+a&#39;_{t+j}(\\gamma_j+A^h_{t,h-j})w_{t+j-1} \\\\ &amp; + b_{t+j}(\\gamma_j+A^h_{t,h-j})+B^h_{t,h-j}], \\end{array} \\] and, therefore, this is true for \\(j-1\\), with \\(A^h_{t,h-j+1}\\) and \\(B^h_{t,h-j+1}\\) given by formulas (ii) above. For \\(j=1\\) we get: \\[\\begin{eqnarray*} \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) &amp;=&amp; \\mathbb{E}_t \\exp(\\gamma&#39;_1 w_{t+1}+A^{h&#39;}_{t,h-1}w_{t+1}+B^h_{t,h-1}) \\\\ &amp;=&amp; \\exp(A&#39;_{t,h} w_t+B_{t,h}), \\end{eqnarray*}\\] Finally note that if we put \\(h-j+1 = i\\), formulas (ii) become (i). Proposition 1.5 (Reverse-order multi-horizon Laplace transform) If the functions \\(a_{t}\\) and \\(b_{t}\\) do not depend on \\(t\\), and if different sequences \\((\\gamma^h_1,\\dots,\\gamma^h_h), h=1,\\dots,H\\) (say) satisfy \\(\\gamma^h_{h+1-i} = u_i\\), for \\(i=1,\\dots,h\\), and for any \\(h \\leq H\\), that is if we want to compute (“reverse order” case): \\[ \\varphi_{t,h}(u_h,\\dots,u_1)=\\mathbb{E}_t[\\exp(u&#39;_{{\\color{red}h}} w_{{\\color{red}t+1}}+\\dots+u&#39;_{{\\color{red}1}} w_{{\\color{red}t+h}})], \\quad h=1,\\dots,H, \\] then we can compute the \\(\\varphi_{t,h}(u_h,\\dots,u_1)\\) for any \\(t\\) and any \\(h \\leq H\\), with only one recursion, i.e. \\(\\varphi_{t,h}(u_h,\\dots,u_1)=\\exp(A&#39;_hw_t+B_h)\\) with: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} A_{h} &amp;=&amp; a(u_{h} + A_{h-1}), \\\\ B_{h} &amp;=&amp; b(u_{h} + A_{h-1}) + B_{h-1}, \\\\ A_{0} &amp;=&amp; 0,\\quad B_{0} = 0. \\end{array} \\right.\\tag{1.19} \\end{equation}\\] Proof. According to Lemma 1.5, we have, in this case: \\[ \\left\\{ \\begin{array}{ccl} A^h_{i} &amp;=&amp; a(u_{i} + A^h_{i-1}), \\\\ B^h_{i} &amp;=&amp; b(u_{i} + A^h_{i-1}) + B^h_{i-1}, \\\\ A^h_{0} &amp;=&amp; 0, \\quad B^h_{0} = 0. \\end{array} \\right. \\] The previous sequences do not dependent on \\(h\\) and are given by Eq. (1.19). Proposition 1.6 (Conditional means and variances of an affine process) Consider an affine process \\(w_t\\). Using the notation of Proposition 1.1, we have: \\[\\begin{eqnarray} \\mathbb{E}_t(w_{t+h}) &amp;=&amp; (I - \\Phi)^{-1}(I - \\Phi^h)\\mu + \\Phi^h w_t \\tag{1.20}\\\\ \\mathbb{V}ar_t(w_{t+h}) &amp;=&amp; \\Sigma(\\mathbb{E}_t(w_{t+h-1}))+\\Phi \\Sigma(\\mathbb{E}_t(w_{t+h-2}))\\Phi&#39; + \\nonumber \\\\ &amp;&amp; \\dots + \\Phi^{h-1} \\Sigma(w_{t}){\\Phi^{h-1}}&#39;. \\tag{1.21} \\end{eqnarray}\\] Eq. (1.21) notably shows that \\(\\mathbb{V}ar_t(w_{t+h})\\) is an affine function of \\(w_t\\). Indeed \\(\\Sigma(.)\\) is an affine function, and the conditional expectations \\(\\mathbb{E}_t(w_{t+h})\\) are affine in \\(w_t\\), as shown by Eq. (1.20). The unconditional mean and variance of \\(w_t\\) are given by: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} \\mathbb{E}(w_t) &amp;=&amp; (I - \\Phi)^{-1}\\mu\\\\ vec[\\mathbb{V}ar(w_t)] &amp;=&amp; (I_{n^2} - \\Phi \\otimes \\Phi)^{-1} vec\\left(\\Sigma[(I - \\Phi)^{-1}\\mu]\\right). \\end{array} \\right.\\tag{1.22} \\end{equation}\\] Proof. Eq. (1.20) is easily deduced from Eq. (1.12), using that \\(\\mathbb{E}_t(\\varepsilon_{t+k})=0\\) for \\(k&gt;0\\). As regards Eq. (1.21): \\[\\begin{eqnarray*} \\mathbb{V}ar_t(w_{t+h}) &amp;=&amp; \\mathbb{V}ar_t\\left(\\Sigma(w_{t+h-1})^{\\frac{1}{2}}\\varepsilon_{t+h}+\\dots + \\Phi^{h-1} \\Sigma(w_{t})^{\\frac{1}{2}}\\varepsilon_{t+1} \\right). \\end{eqnarray*}\\] The conditional expectation at \\(t\\) of all the terms of the sum is equal to zero since, for \\(i \\ge 1\\): \\[ \\mathbb{E}_t\\left[\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i}\\right] = \\mathbb{E}_t[\\underbrace{\\mathbb{E}_{t+i-1}\\{\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i}\\}}_{=\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\mathbb{E}_{t+i-1}\\{\\varepsilon_{t+i}\\}=0}\\}], \\] and \\(\\forall i &lt;j\\), \\[ \\mathbb{C}ov_t\\left[\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i},\\Sigma(w_{t+j-1})^{\\frac{1}{2}}\\varepsilon_{t+j}\\right] = \\mathbb{E}_t\\left[\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i}\\varepsilon_{t+j}&#39;\\Sigma&#39;(w_{t+j-1})^{\\frac{1}{2}}\\right], \\] which can be seen to be equal to zero by conditioning on the information available on date \\(t+j-1\\). Using the same conditioning, we obtain that: \\[\\begin{eqnarray*} \\mathbb{V}ar_t\\left[\\Phi^{h-j}\\Sigma(w_{t+j-1})^{\\frac{1}{2}}\\varepsilon_{t+j}\\right] &amp;=&amp; \\mathbb{E}_t\\left[\\Phi^{h-j}\\Sigma(w_{t+j-1})^{\\frac{1}{2}}\\varepsilon_{t+j}\\varepsilon_{t+j}&#39;\\Sigma&#39;(w_{t+j-1})^{\\frac{1}{2}}{\\Phi^{h-j}}&#39;\\right] \\\\ &amp;=&amp; \\mathbb{E}_t\\left[\\Phi^{h-j}\\Sigma(w_{t+j-1})^{\\frac{1}{2}} \\mathbb{E}_{t+j-1}(\\varepsilon_{t+j}\\varepsilon_{t+j}&#39;)\\Sigma&#39;(w_{t+j-1})^{\\frac{1}{2}}{\\Phi^{h-j}}&#39;\\right] \\\\ &amp;=&amp; \\Phi^{h-j}\\mathbb{E}_t[\\Sigma(w_{t+j-1})]{\\Phi^{h-j}}&#39; \\\\ &amp;=&amp; \\Phi^{h-j}\\Sigma(\\mathbb{E}_t[w_{t+j-1}]){\\Phi^{h-j}}&#39;, \\end{eqnarray*}\\] where the last equality results from the fact that he fact that \\(\\Sigma(.)\\) is affine (see Eq. (1.14)). Proposition 1.7 (Affine property of the GARCH-type process) The process \\(w_t = (w_{1,t}, w_{2,t})\\) defined by: \\[ \\left\\{ \\begin{array}{ccl} w_{1, t+1} &amp;=&amp; \\mu + \\varphi w_{1,t} + \\sigma_{t+1} \\varepsilon_{t+1} \\mid \\varphi \\mid &lt; 1 \\\\ \\sigma^2_{t+1} &amp;=&amp; \\omega + \\alpha \\varepsilon^2_t + \\beta \\sigma^2_t 0 &lt; \\beta &lt; 1, \\alpha &gt; 0, \\omega &gt; 0 \\\\ w_{2,t} &amp;=&amp; \\sigma^2_{t+1}, \\quad \\varepsilon_t \\sim i.i.d. \\mathcal{N}(0,1) \\end{array} \\right. \\] is affine. Proof. Note that \\(w_{2,t}\\) is function of \\(\\underline{w_{1,t}}\\) \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(u_1 w_{1, t+1} + u_2 w_{2, t+1})|\\underline{w_{1,t}}] \\\\ &amp;= &amp; \\exp(u_1 \\mu + u_1 \\varphi w_{1,t} + u_2 \\omega + u_2 \\beta w_{2,t}) \\mathbb{E}[\\exp(u_1 \\sigma_{t+1} \\varepsilon_{t+1} + u_2 \\alpha \\varepsilon^2_{t+1})|\\underline{w_{1,t}}] \\end{eqnarray*}\\] and, using Lemma 1.4: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}[\\exp(u_1 w_{1, t+1} + u_2 w_{2, t+1})|\\underline{w_{1,t}}] \\\\ &amp;= &amp; \\exp(u_1 \\mu + u_1 \\varphi w_{1,t} + u_2 \\omega + u_2 \\beta w_{2t}) \\exp \\left[ - \\frac{1}{2} \\log(1-2 u_2 \\alpha) + \\frac{u^2_1 w_{2,t}}{2(1-2 u_2 \\alpha)} \\right]\\\\ &amp;= &amp; \\exp \\left[ u_1 \\mu + u_2 \\omega - \\frac{1}{2} \\log(1-2u_2\\alpha)+ u_1 \\varphi w_{1,t} + \\left(u_2 \\beta + \\frac{u^2_1}{2(1-2u_2\\alpha)}\\right) w_{2,t}\\right], \\end{eqnarray*}\\] which is exponential affine in \\((w_{1,t}, w_{2,t})\\). Proposition 1.8 (Computation of truncated conditional moments) If \\(\\varphi(z)=\\mathbb{E}[exp(z&#39;w)]\\), we have: \\[\\begin{equation} \\mathbb{E}[\\exp(u&#39;w)\\textbf{1}_{(v&#39;w&lt;\\gamma})] = \\frac{\\varphi(u)}{2} - \\frac{1}{\\pi} \\int^\\infty_o \\frac{{\\mathcal I}m[\\varphi(u+ivx)\\exp(-i\\gamma x)]}{x}dx.\\tag{1.23} \\end{equation}\\] Proof. We want to compute \\(\\tilde{\\varphi}_t(u;v,\\gamma) = \\mathbb{E}_t[\\exp(u&#39;w)\\textbf{1}_{(v&#39;w&lt;\\gamma})]\\). Let us first note that, for given \\(u\\) and \\(v\\), \\(\\tilde{\\varphi}_t(u;v,\\gamma)\\) is a positive increasing bounded function of \\(\\gamma\\) and therefore can be seen as the c.d.f. of a positive finite measure on \\(\\mathbb{R}\\), the Fourier transform of which is: \\[ \\int_{\\mathbb{R}} \\exp(i\\gamma x)d\\tilde{\\varphi}(u;v,\\gamma) = \\mathbb{E} \\int_{\\mathbb{R}} \\exp(i\\gamma x)d\\tilde{\\varphi}_w(u;v,\\gamma), \\] where, for given \\(w, \\tilde{\\varphi}_w(u;v,\\gamma)\\) is the c.d.f. of the mass point \\(\\exp(u&#39;w)\\) at \\(v&#39;w\\). We then get: \\[\\begin{eqnarray*} \\int_{\\mathbb{R}} \\exp(i\\gamma x) d\\tilde{\\varphi}(u;v,\\gamma) &amp;=&amp; \\mathbb{E}[\\exp(ixv&#39;w)exp(u&#39;w)] \\\\ &amp; =&amp; \\mathbb{E}[\\exp(u+ivx)&#39;w] \\\\ &amp; =&amp; \\varphi(u+ivx). \\end{eqnarray*}\\] Let us now compute \\(A(x_0,\\lambda)\\) for any real number \\(\\lambda\\), with: \\[\\begin{eqnarray*} &amp;&amp;A(x_0,\\lambda) \\\\ &amp;=&amp; \\frac{1}{2\\pi} \\int^{x_0}_{-x_0} \\frac{\\exp(i\\lambda x)\\varphi(u-ivx)-\\exp(-i\\lambda)\\varphi(u+ivx)}{ix}dx \\\\ &amp;=&amp; \\frac{1}{2\\pi} \\int^{x_0}_{-x_0}\\left[ \\begin{array}{l} \\int_{\\mathbb{R}} \\frac{\\exp[-ix(\\gamma-\\lambda)]-\\exp[ix(\\gamma-\\lambda)]}{ix}d\\tilde{\\varphi}(u;v,\\gamma) \\end{array} \\right]dx \\\\ &amp;=&amp; \\frac{1}{2\\pi} \\int_{\\mathbb{R}} \\left[ \\begin{array}{l} \\int^{x_0}_{-x_0} \\frac{\\exp[-ix(\\gamma-\\lambda)] -\\exp[ix(\\gamma-\\lambda)]}{ix}dx \\end{array} \\right]d\\tilde{\\varphi}(u;v,\\gamma). \\end{eqnarray*}\\] Now : \\[\\begin{eqnarray*} \\frac{1}{2\\pi} \\int^{x_o}_{-x_o} \\frac{\\exp[-ix(\\gamma-\\lambda)] -\\exp[ix(\\gamma-\\lambda)]}{ix}dx \\\\ = \\frac{-sign(\\gamma-\\lambda)}{\\pi} \\int^{x_o}_{-x_o} \\frac{sin(x\\mid\\gamma-\\lambda\\mid)}{x}dx \\end{eqnarray*}\\] which tends to \\(-sign(\\gamma-\\lambda)\\) when \\(x_0\\rightarrow\\infty\\) (where \\(sign(\\omega)=1\\) if \\(\\omega&gt;0\\), \\(sign(\\omega)=0\\) if \\(\\omega=0\\), \\(sign(\\omega)=-1\\) if \\(\\omega&lt;0\\)). Therefore: \\[ A(\\infty,\\lambda) = - \\int_{\\mathbb{R}} sign(\\gamma-\\lambda)d\\tilde{\\varphi}(u;v,\\gamma) = -\\mathbb{E} \\int_{\\mathbb{R}} sign(\\gamma-\\lambda)d\\tilde{\\varphi}_w(u;\\theta,\\gamma), \\] where \\(\\tilde{\\varphi}_w(u;v,\\gamma)\\) is the c.d.f. of the mass point \\(\\exp(u&#39;w)\\) at \\(v&#39;w\\) and \\[ \\int_{\\mathbb{R}} \\mbox{sign}(\\gamma-\\lambda)d\\tilde{\\varphi}_w(u;v,\\gamma)= \\left\\{ \\begin{array}{ccc} \\exp(u&#39;w) &amp; \\mbox{if} &amp; \\lambda &lt; v&#39;w \\\\ 0 &amp; \\mbox{if}&amp; \\lambda = v&#39;w \\\\ -\\exp(u&#39;w) &amp; \\mbox{if} &amp; \\lambda &gt; v&#39;w. \\end{array} \\right. \\] Therefore, we have: \\[\\begin{eqnarray*} A(\\infty,\\lambda) &amp; =&amp; - \\mathbb{E}[\\exp(u&#39;w)(1-\\textbf{1}_{(v&#39;w&lt;\\lambda)})-\\exp(u&#39;w)\\textbf{1}_{(v&#39;w&lt;\\lambda)}] \\\\ &amp; =&amp; - \\varphi(u) + 2\\tilde{\\varphi}(u;v,\\lambda) \\end{eqnarray*}\\] and, further,: \\[ \\tilde{\\varphi}(u;,v,\\gamma) = \\frac{\\varphi(u)}{2} + \\frac{1}{2} A(\\infty,\\gamma), \\] where \\[\\begin{eqnarray*} \\frac{1}{2} A(\\infty,\\gamma) &amp; =&amp; \\frac{1}{4\\pi} \\int^{\\infty}_{-\\infty} \\frac{\\exp(i\\gamma x)\\varphi(u-ivx)-\\exp(-i\\gamma x)\\varphi(u+ivx)}{ix} dx \\\\ &amp; =&amp; \\frac{1}{2\\pi} \\int^{\\infty}_{o} \\frac{\\exp(i\\gamma x)\\varphi(u-ivx)-\\exp(-i\\gamma x)\\varphi(u+ivx)}{ix} dx \\\\ &amp; =&amp; - \\frac{1}{\\pi} \\int^{\\infty}_{o} \\frac{{\\mathcal I}m[\\exp(-i\\gamma x)\\varphi(u+ivx)]}{x}dx, \\end{eqnarray*}\\] which leads to Eq. (1.23). References "],["pricing-and-risk-neutral-dynamics.html", "Chapter 2 Pricing and risk-neutral dynamics 2.1 Consumption-based Capital Asset Pricing Model (CCAPM) and stochastic discount factor (SDF) 2.2 Recursive utilities 2.3 SDF: Absence of Arbitrage Approach 2.4 The risk-neutral (R.N.) dynamics 2.5 Typology of Econometric Asset Pricing Models 2.6 THE TERM STRUCTURE OF RISK-FREE YIELDS 2.7 The Affine Case 2.8 Gaussian Affine Term Structure Model 2.9 Non-Negative Affine Term Structure Model", " Chapter 2 Pricing and risk-neutral dynamics 2.1 Consumption-based Capital Asset Pricing Model (CCAPM) and stochastic discount factor (SDF) Consider an economy featuring a single good, whose date-\\(t\\) price is \\(q_t\\). There is a representative agent with an external income \\(R_t\\) at \\(t\\), and a portfolio of assets, with an allocation vector \\(\\alpha_{t-1}\\) (decided at \\(t-1\\)). The vector of date-\\(t\\) prices is \\(p_t\\). At any date \\(t+j\\), \\(j=0,1,...\\), the agent will face the budget constraint: \\[ q_{t+j}C_{t+j}+\\alpha&#39;_{t+j}p_{t+j} = R_{t+j} + \\alpha&#39;_{t+j-1}p_{t+j}, \\] where \\(C_{t+j}\\)is her consumption on date \\(t+j\\). The representative agent maximizes her expected utility time-separable preferences \\(\\mathbb{E}_t \\sum^\\infty_{j=0} \\delta^j U(C_{t+j})\\) subject to the budget constraints at \\(t+j\\), \\(j= 0,1,\\dots\\), where \\(U\\) is the utility function and \\(\\delta\\)is time discount factor or subjective discount factor. Note that the latter is different from SDF. Replacing \\(C_{t+j}\\) with \\([R_{t+j}-(\\alpha&#39;_{t+j}-\\alpha&#39;_{t+j-1})p_{t+j}]/q_{t+j}\\), the objective function becomes: \\[ \\mathbb{E}_t \\sum^\\infty_{j=0} \\delta^j U [R_{t+j}/q_{t+j}-(\\alpha&#39;_{t+j}-\\alpha&#39;_{t+j-1})p_{t+j}/q_{t+j}]. \\] The vector of allocation \\(\\alpha_t\\) appears in the first two terms: \\[ U[R_t/q_t-(\\alpha&#39;_t-\\alpha&#39;_{t-1})p_t/q_t] + \\delta \\mathbb{E}_t U[R_{t+1}/q_{t+1}-(\\alpha&#39;_{t+1}-\\alpha&#39;_t)p_{t+1}/q_{t+1}]. \\] As a result, the first order condition associated with vector \\(\\alpha_t\\) reads \\[ \\frac{p_t}{q_t} \\frac{d U(C_t)}{dC} = \\delta \\mathbb{E}_t \\left[ \\begin{array}{l} \\frac{p_{t+1}}{q_{t+1}} \\frac{dU(C_{t+1})}{dC} \\end{array} \\right], \\] or \\[\\begin{equation} p_t = \\mathbb{E}_t(p_{t+1} \\mathcal{M}_{t,t+1}),\\tag{2.1} \\end{equation}\\] where \\(\\mathcal{M}_{t,t+1}\\) is a strictly positive scalar called stochastic discount factor (SDF) between dates \\(t\\) and \\(t+1\\). We have: \\[ \\mathcal{M}_{t,t+1} \\equiv \\delta \\frac{q_t}{q_{t+1}} \\frac{ \\frac{dU(C_{t+1})}{dC}} { \\frac{dU(C_t)}{dC}}. \\] Example 2.1 (Power utility function) A standard case is the one of the power utility \\(U(C) = \\frac{C^{1-\\gamma}-1}{1-\\gamma}\\), where \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. We have \\(U&#39;(C) = C^{-\\gamma} &gt; 0\\) and \\(U&#39;&#39;(C) = - \\gamma C^{-\\gamma-1} &lt; 0\\). As a result, the stochastic discount factor is \\[\\begin{eqnarray} &amp;&amp; \\mathcal{M}_{t,t+1} \\nonumber \\\\ &amp;=&amp; \\frac{q_t}{q_{t+1}} \\delta \\left( \\frac{C_{t+1}}{C_t} \\right)^{-\\gamma} \\nonumber\\\\ &amp;=&amp; \\exp(\\log \\delta + \\log q_t + \\gamma \\log C_t - \\log q_{t+1} - \\gamma \\log C_{t+1}).\\tag{2.2} \\end{eqnarray}\\] The vector of prices \\(p_t\\) satisfies: \\[ p_t = \\delta q_t C^\\gamma_t \\mathbb{E}_t \\left( \\begin{array}{l} \\frac{C^{-\\gamma}_{t+1}}{q_{t+1}} p_{t+1} \\end{array} \\right), \\] or (Euler equation): \\[ \\mathbb{E}_t\\left[ \\begin{array}{l} \\delta\\left( \\begin{array}{l} \\frac{C_{t+1}}{C_t} \\end{array} \\right)^{-\\gamma} \\frac{q_t}{q_{t+1}} \\frac{p_{t+1}}{p_t} - 1 \\end{array} \\right] = 0. \\] According to Eq. (2.1), for any asset \\(j\\): \\[\\begin{equation} p_{j,t} = \\mathbb{E}_t(\\mathcal{M}_{t,t+1} p_{j,t+1}).\\tag{2.3} \\end{equation}\\] Using that \\(p_{j,t+1} = \\mathbb{E}_t(\\mathcal{M}_{t+1,t+2} p_{j,t+2})\\), we get: \\[\\begin{eqnarray*} p_{j,t} &amp;=&amp; \\mathbb{E}_t[\\mathbb{E}_{t+1}(p_{j,t+2}\\mathcal{M}_{t+1,t+2})\\mathcal{M}_{t,t+1}] \\\\ &amp;=&amp; \\mathbb{E}_t(\\mathcal{M}_{t,t+1} \\mathcal{M}_{t+1,t+2}p_{j,t+2}). \\end{eqnarray*}\\] This can be generalized as follows: \\[ p_{j,t} = \\mathbb{E}_t[\\mathcal{M}_{t,t+1} \\dots \\mathcal{M}_{t+h-1,t+h}p_{j,t+h}], \\; \\forall h. \\] 2.2 Recursive utilities XXXX 2.3 SDF: Absence of Arbitrage Approach Consider a period of interest \\({\\mathcal T} = \\{0,1,2,...,T^*\\}\\). As in Chapter 1, vector \\(w_t\\) constitutes the new information in the economy at \\(t\\). The historical, or physical, dynamics of \\(w_t\\), \\(f(\\underline{w_t})\\), is defined by \\(f(w_{t+1}|\\underline{w_t})\\). The physical probability is denoted by \\(\\mathbb{P}\\). \\(L_{2t}, t \\in {\\mathcal T}\\), is the (Hilbert) space of square integrate functions \\(g(\\underline{w_t})\\), and we have \\(L_{2t} \\subset L_{2s}, t&lt; s\\). 2.3.1 Existence and unicity of the SDF Hypothesis 2.1 (Price existence and uniqueness) For any \\(\\underline{w_t}\\), there exists a unique \\(p_t[g(\\underline{w_s})]\\), function of \\(\\underline{w_t}\\), price at \\(t\\) of a payoff \\(g(\\underline{w_s})\\) delivered at \\(s, \\forall t \\le s\\). Hypothesis 2.2 (Linearity and continuity) For all \\(t &lt; s\\), \\(\\underline{w_t}\\), \\(g_1\\), \\(g_2\\), we have \\(p_t[\\lambda_1 g_1(\\underline{w_s}) + \\lambda_2g_2(\\underline{w_s})] = \\lambda_1p_t[g_1(\\underline{w_s})]+\\lambda_2 p_t[g_2(\\underline{w_s})]\\), If \\(g_n(\\underline{w_s}) \\overset{L_{2s}}{\\underset{n\\rightarrow\\infty}{\\longrightarrow}} 0\\), then \\(p_t[g_n(\\underline{w_s})] \\underset{n\\rightarrow\\infty}{\\longrightarrow} 0\\). Hypothesis 2.3 (Absence of Arbitrage Opportunity (AAO)) At any \\(t\\), it is impossible to constitute a portfolio of future payoffs, possibly modified at subsequent dates, such that: the price of the portfolio at \\(t\\) is zero, payoffs at subsequent dates are \\(\\ge 0\\), there is at least one subsequent date \\(s\\) such that the net payoff at \\(s\\) is strictly positive with a non zero conditional probability at \\(t\\). Theorem 2.1 (Riesz representation theorem) Under Assumptions 2.1 and 2.2, for all \\(\\underline{w_t}\\), and \\(s &gt; t\\), there exists \\(\\mathcal{M}_{t,s}(\\underline{w_s}) \\in L_{2s}\\), unique such that, \\(\\forall g(\\underline{w_s}) \\in L_{2s}\\), \\[ p_t[g(\\underline{w_s})] = \\mathbb{E}[\\mathcal{M}_{t,s}(\\underline{w_s})g(\\underline{w_s})|\\underline{w_t}]. \\] In particular the price at \\(t\\) of a zero coupon bond maturing at \\(s\\) is \\(\\mathbb{E}(\\mathcal{M}_{t,s}|\\underline{w_t})\\). Proposition 2.1 (Positivity of M) If Assumption 2.3 is satisfied, then for all \\(t\\) and \\(s\\), \\(\\mathbb{P}(\\mathcal{M}_{t,s}&gt;0|\\underline{w_t})=1\\). Proof. \\(\\Leftarrow\\) is obvious. If \\(\\Rightarrow\\) was not true, the payoff \\(\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}\\), at \\(s\\), would be such that: \\(\\mathbb{P}[\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}=1|\\underline{w_t}] &gt; 0\\) and \\(p_t[\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}] = \\mathbb{E}_t[\\mathcal{M}_{t,s}\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}] \\le 0\\). Proposition 2.2 (Time consistency) For all \\(t &lt; r &lt; s\\), we have \\(\\mathcal{M}_{t,s} = \\mathcal{M}_{t,r} \\mathcal{M}_{r,s}\\), which implies: * \\(\\mathcal{M}_{t,s} = \\mathcal{M}_{t,t+1} \\mathcal{M}_{t+1,t+2}\\dots\\mathcal{M}_{s-1,s}\\) * \\(\\mathcal{M}_{0,t} = \\Pi^{t-1}_{j=0} \\mathcal{M}_{j,j+1}\\) (\\(\\mathcal{M}_{0,t}\\) is called pricing kernel). Proof. Using Lemma 2.1 we have: \\[\\begin{eqnarray*} p_t(g_s) &amp;=&amp; \\mathbb{E}(\\mathcal{M}_{t,s}g_s|\\underline{w_t}) = \\mathbb{E}(\\mathcal{M}_{t,r} p_r(g_s)|\\underline{w_t}) \\\\ &amp;=&amp; \\mathbb{E}[\\mathcal{M}_{t,r}\\mathbb{E}(\\mathcal{M}_{r,s} g_s|\\underline{w_r})|\\underline{w_t}] = \\mathbb{E}(\\mathcal{M}_{t,r} \\mathcal{M}_{r,s} g_s|\\underline{w_t}), \\forall g, \\forall \\underline{w}_{t} \\end{eqnarray*}\\] and, therefore, \\(\\mathcal{M}_{t,s} = \\mathcal{M}_{t,r}\\mathcal{M}_{r,s}\\). Lemma 2.1 For any payoff \\(g_s\\) at \\(s, p_t(g_s) = p_t[p_r(g_s)]\\). Proof. If this was not true, we could construct a sequence of portfolios with a strictly positive payoff at \\(s\\) with zero payoff at any other future date and with price zero at \\(t\\), contradicting Assumption 2.3. Indeed, assuming, for instance, \\(p_t(g_s) &gt; p_t[p_r(g_s)]\\), the payoff at \\(s\\) is defined by the following strategy: (i) at \\(t\\): buy \\(p_r(g_s)\\), (short) sell \\(g_s\\), buy \\(\\frac{p_t(g_s)-p_t[p_r(g_s)]}{\\mathbb{E}(\\mathcal{M}_{t,s}|\\underline{w_t})}\\) zero-coupon bonds maturing at \\(s\\), at global price zero, (ii) at \\(r\\): buy \\(g_s\\) and sell \\(p_r(g_s)\\), generating a zero net payoff, (iii) at \\(s\\), the net payoff is: \\(g_s-g_s+\\frac{p_t(g_s)-p_t[p_r(g_s)]}{\\mathbb{E}(\\mathcal{M}_{t,s}|\\underline{w_t})} &gt; 0\\). Consider an asset whose payoff, on date \\(s\\), is \\(g(\\underline{w_s})\\). We have, \\(\\forall t &lt; s\\): \\[\\begin{equation} \\boxed{p_t[g(\\underline{w_s})] = \\mathbb{E}_t[\\mathcal{M}_{t,t+1}...\\mathcal{M}_{s-1,s}g(\\underline{w_s})].}\\tag{2.4} \\end{equation}\\] In particular, since \\(L_{2,t+1}\\) contains 1, the price at \\(t\\) of a zero-coupon with residual maturity one is given by: \\[ B(t,1) := \\mathbb{E}_t [\\mathcal{M}_{t,t+1}]. \\] Denoting by \\(r_t\\) the continously-compounded interest rate, defined through \\(B(t,1)=\\exp(-r_{t})\\), we get \\[ r_{t}=-\\log \\mathbb{E}_t [\\mathcal{M}_{t,t+1}]. \\] Definition 2.1 (Bank account) The bank account process \\(R_t\\) is defined by \\(R_{t} \\equiv \\exp(r_0+...+r_{t-1}) = \\frac{1}{\\mathbb{E}_0[ \\mathcal{M}_{0,1}]\\times ... \\times \\mathbb{E}_{t-1} [\\mathcal{M}_{t-1,t}]}\\). \\(R_t\\) is the price of an investment initiated on date 0, when it was worth one dollar, and invested on each date at the risk-free rate (for one period). For any price process \\(p_t\\), we have \\(p_t = \\mathbb{E}_t(\\mathcal{M}_{t,s} p_s)\\) (with \\(s&gt;t\\)), or \\(\\mathcal{M}_{0,t} p_t = \\mathbb{E}_t(\\mathcal{M}_{0,s}p_s)\\). That is, \\(\\mathcal{M}_{0,t} p_t\\) is a martingale. In particular \\(\\mathcal{M}_{0,t} R_t\\) is a martingale. 2.3.2 Exponential Affine SDF A specific (tractable) case is that of exponential affine SDF. Assume that \\[ \\mathcal{M}_{t,t+1}(\\underline{w_{t+1}}) = \\exp[\\alpha_t(\\underline{w_t})&#39;w_{t+1}+\\beta_t(\\underline{w_t})] \\] where \\(\\alpha_t\\) defines the factor loadings or sensitivity vector. Using \\(\\mathbb{E}_t[\\mathcal{M}_{t,t+1}]=\\exp(-r_{t})=\\exp[\\psi_t(\\alpha_t)+\\beta_t]\\), we get: \\[\\begin{equation} \\mathcal{M}_{t,t+1} = \\exp[-r_{t}+\\alpha&#39;_tw_{t+1}-\\psi_t(\\alpha_t)].\\tag{2.5} \\end{equation}\\] Example 2.2 (CCAPM/Power utility case) In the CCAPM-power-utility case (see Example 2.1), we have (Eq. (2.2)): \\[ \\mathcal{M}_{t,t+1} = \\exp(\\log \\delta + \\log q_t + \\gamma \\log C_t - \\log q_{t+1} - \\gamma \\log C_{t+1}), \\] where \\(q_t\\) is the price of the consumption good, \\(C_t\\) is the quantity consumed at \\(t\\) and \\(\\delta\\) is the intertemporal discount rate. Hence, in that case, \\(\\mathcal{M}_{t,t+1}\\) is exponential affine in \\(w_{t+1} = (\\log q_{t+1}, \\log C_{t+1})&#39;\\) (and its first lag). 2.4 The risk-neutral (R.N.) dynamics The historical Dynamics is characterized by \\(f(\\underline{w_{T^*}})\\), or by the sequence of conditional p.d.f. \\(f_{t+1}(w_{t+1}|\\underline{w_t})\\), or \\(f_{t+1}(w_{t+1})\\), with respect to (w.r.t.) some measure \\(\\mu\\). We define the conditional risk-neutral p.d.f. w.r.t. the conditional historical probability. For that, we employ the Radon-Nikodym derivative \\(d^{\\mathbb{Q}}_{t+1}(w_{t+1}|\\underline{w_t})\\): \\[ d^{\\mathbb{Q}}_{t+1}(w_{t+1}|\\underline{w_t}) = \\frac{\\mathcal{M}_{t,t+1}(\\underline{w_{t+1}})}{\\mathbb{E}[\\mathcal{M}_{t,t+1}(\\underline{w_{t+1}})|\\underline{w_t}]}, \\] or \\[ d^{\\mathbb{Q}}_{t+1}(w_{t+1})= \\frac{\\mathcal{M}_{t,t+1}}{\\mathbb{E}_t(\\mathcal{M}_{t,t+1})}=\\exp(r_{t}) \\mathcal{M}_{t,t+1}. \\] The risk neutral conditional p.d.f. is: \\[\\begin{eqnarray} f^{\\mathbb{Q}}_{t+1}(w_{t+1}) &amp;=&amp; f_{t+1}(w_{t+1})d^{\\mathbb{Q}}_{t+1}(w_{t+1}) \\nonumber \\\\ &amp;=&amp;f_{t+1} (w_{t+1}) \\mathcal{M}_{t,t+1} (\\underline{w_{t+1}}) \\exp [r_{t} (\\underline{w_t})].\\tag{2.6} \\end{eqnarray}\\] Of course, the conditional historical p.d.f. with respect to the conditional risk-neutral (R.N.) p.d.f. is: \\[ d_{t+1}(w_{t+1}) = \\frac{1}{d^{\\mathbb{Q}}_{t+1}(w_{t+1})} \\quad \\mbox{or}\\quad d_{t+1}(w_{t+1}) = \\frac{\\exp(-r_{t})}{\\mathcal{M}_{t,t+1}}. \\] The p.d.f. of \\(\\mathbb{Q}\\) w.r.t. the historical dynamics \\(\\mathbb{P}\\) is: \\[ \\xi_{T^*} = \\frac{d\\mathbb{Q}}{d\\mathbb{P}} = \\Pi^{T^{*}-1}_{t=0} d^{\\mathbb{Q}}_{t+1}(w_{t+1}) = \\Pi^{T^{*}-1}_{t=0} \\exp(r_{t}) \\mathcal{M}_{t,t+1}. \\] Note that the p.d.f. of the R.N. distribution of \\(\\underline{w_t}\\), w.r.t. the corresponding historical distribution is: \\[ \\xi_t= \\Pi^{t-1}_{\\tau=1} d^{\\mathbb{Q}}_{\\tau+1}(w_{\\tau+1})=\\mathbb{E}_t\\left( \\begin{array}{l} \\frac{d\\mathbb{Q}}{d\\mathbb{P}} \\end{array} \\right) = \\mathbb{E}_t\\xi_{T^*} \\] (and \\(\\xi_t\\) therefore is a \\(\\mathbb{P}\\)-martingale) since: \\[ \\mathbb{E}_t \\left( \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right) = \\Pi^{t-1}_{\\tau = 1} d^{\\mathbb{Q}}_{\\tau + 1} (w_{\\tau+1}) \\mathbb{E}_t \\left( d^{\\mathbb{Q}}_{t+1} (w_{t+1}) \\ldots d^{\\mathbb{Q}}_{T^*} (w_{T^*})\\right). \\] * Equivalent form of the pricing formula (that is Eq. (2.4)): Price at \\(t\\) of a payoff \\(g(\\underline{w_s})\\) at time \\(s&gt;t\\): \\[\\begin{eqnarray} p_t[g(\\underline{w_s})] &amp;=&amp; \\mathbb{E}_t[\\mathcal{M}_{t,t+1}...\\mathcal{M}_{s-1,s}g(\\underline{w_s})] \\\\ &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t[exp(-r_{t}-...-r_{s-1})g(\\underline{w_s})]\\quad \\mbox{(key formula).}\\nonumber \\end{eqnarray}\\] * Or, with simpler notations: \\[\\begin{eqnarray*} p_t &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t[exp(-r_{t}-...-r_{s-1})p_s] \\\\ &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t\\left(\\frac{R_t}{R_s} p_s\\right) \\quad (R_t:\\mbox{bank account}) \\end{eqnarray*}\\] \\(\\Rightarrow\\) \\(\\dfrac{p_t}{R_t} = \\mathbb{E}^{\\mathbb{Q}}_t\\left( \\dfrac{p_s}{R_s}\\right)\\), that is, \\(\\dfrac{p_t}{R_t}\\) is a \\(\\mathbb{Q}\\)-martingale. * In particular: \\[ p_t = \\exp(-r_{t})\\mathbb{E}^{\\mathbb{Q}}_t(p_{t+1}) \\] or, using the arithmetic return of any payoff \\[ \\rho_{A,t+1}= \\frac{p_{t+1}-p_t}{p_t} \\] and the arithmetic return of the riskless asset \\(r_{A,t+1}=exp(r_{t})-1\\), we get (key result): \\[ \\mathbb{E}^{\\mathbb{Q}}_t(\\rho_{A,t+1})=r_{A,t}. \\] * Moreover the excess arithmetic return process \\(\\rho_{A,t+1}-r_{A,t}\\) is a \\(\\mathbb{Q}\\)-martingale difference and, therefore, \\(\\mathbb{Q}\\)-serially uncorrelated. * Case of an exponential affine SDF: \\[ \\begin{array}{ll} d^{\\mathbb{Q}}_{t+1}(w_{t+1}) &amp; = \\frac{\\mathcal{M}_{t,t+1}}{\\mathbb{E}_t(\\mathcal{M}_{t,t+1})} = \\frac{\\exp(\\alpha&#39;_t w_{t+1}+\\beta_t)}{\\exp[\\psi_t(\\alpha_t)+\\beta_t]} = \\frac{\\exp(\\alpha&#39;_t w_{t+1})}{\\varphi_t(\\alpha_t)} \\\\ &amp; = \\exp[\\alpha&#39;_t w_{t+1}-\\psi_t(\\alpha_t)] \\end{array} \\] \\(\\Rightarrow\\) \\(d^{\\mathbb{Q}}_{t+1}(w_{t+1})\\) is also exponential affine. * We have: \\[ f^{\\mathbb{Q}}_{t+1} (w_{t+1}) = \\frac{f_{t+1} (w_{t+1}) \\exp (\\alpha&#39;_t w_{t+1})}{\\varphi_t (\\alpha_t)}. \\] \\(f^{\\mathbb{Q}}_{t+1}\\) Esscher transform of \\(f_{t+1}\\), corresponding to \\(\\alpha_t\\). Laplace transform of the conditional R.N. probability: \\(\\varphi^{\\mathbb{Q}}_t(u|\\underline{w_t})\\), also denoted by \\(\\varphi^{\\mathbb{Q}}_t(u)\\): \\[\\begin{eqnarray*} \\varphi^{\\mathbb{Q}}_t(u) &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(u&#39; w_{t+1}) \\\\ &amp;=&amp; \\mathbb{E}_t \\exp[(u+\\alpha_t)&#39;w_{t+1}-\\psi_t(\\alpha_t)] \\\\ &amp;=&amp; \\exp[\\psi_t(u+\\alpha_t)-\\psi_t(\\alpha_t)] = \\frac{\\varphi_t(u+\\alpha_t)}{\\varphi_t(\\alpha_t)} \\end{eqnarray*}\\] \\(\\Rightarrow\\) (key result) \\[\\begin{equation} \\psi^{\\mathbb{Q}}_t(u) = \\psi_t(u+\\alpha_t)-\\psi_t(\\alpha_t).\\tag{2.7} \\end{equation}\\] * We check that, if \\(\\alpha_t=0\\), \\(\\psi^{\\mathbb{Q}}_t=\\psi_t\\) (since \\(\\psi_t(0)=0)\\). * Back to the historical conditional probability: \\[ d_{t+1}(w_{t+1}) = \\frac{1}{d^{\\mathbb{Q}}_{t+1}(w_{t+1})} = \\exp[-\\alpha&#39;_t w_{t+1}+\\psi_t(\\alpha_t)], \\] * Putting \\(u=-\\alpha_t\\) in the expression of \\(\\psi^{\\mathbb{Q}}_t(u)\\) we get \\[ \\psi^{\\mathbb{Q}}_t(-\\alpha_t)=-\\psi_t(\\alpha_t), \\] and, replacing \\(u\\) by \\(u-\\alpha_t\\), we get (key result): \\[ \\psi_t(u) = \\psi^{\\mathbb{Q}}_t(u-\\alpha_t)-\\psi^{\\mathbb{Q}}_t(-\\alpha_t) \\] Also: \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} d_{t+1}(w_{t+1}) &amp;=&amp; \\exp[-\\alpha&#39;_t(w_{t+1})-\\psi^{\\mathbb{Q}}_t(-\\alpha_t)] \\\\ d^{\\mathbb{Q}}_{t+1}(w_{t+1}) &amp;=&amp; \\exp[\\alpha&#39;_t(w_{t+1})+\\psi^{\\mathbb{Q}}_t(-\\alpha_t)]. \\end{array} \\right. \\end{equation*}\\] 2.5 Typology of Econometric Asset Pricing Models Definition 2.2 (Econometric Asset Pricing Model (EAPM)) An Econometric Asset Pricing Model (EAPM) is defined by the following functions: \\(r_{t}(\\underline{w_t})\\), \\(f(w_{t+1}|\\underline{w_t}))\\) [or \\(\\psi_t(u)\\)], * \\(\\mathcal{M}_{t,t+1}(\\underline{w_{t+1}})\\), * \\(f^{\\mathbb{Q}}(w_{t+1}|\\underline{w_t})\\) [or \\(\\psi^{\\mathbb{Q}}_t(u)\\)]. The previous functions have to to be specified and parameterized. They are linked by: \\[ f^{\\mathbb{Q}}(w_{t+1}|\\underline{w_t}) = f(w_{t+1}|\\underline{w_t}) \\mathcal{M}_{t,t+1}(\\underline{w_{t+1}}) \\exp[r_{t}(\\underline{w_t}))]. \\] We are going to present three ways of specifying an EAPM: the Direct Modelling, the R.N. Constrained Direct Modelling (or Mixed Modelling), the Back Modelling. If \\(\\mathcal{M}_{t,t+1}\\) is exponential affine: \\[ \\mathcal{M}_{t,t+1} (\\underline{w_{t+1}}) = \\exp \\{ -r_{t} (\\underline{w_t}) + \\alpha&#39;_t (\\underline{w_t}) ) w_{t+1} - \\psi_t [\\alpha_t (w_t)]\\}. \\] Once \\(r_{t} (\\underline{w_t})\\) is specified we have to specify \\(\\psi_t, \\alpha_t, \\psi^{\\mathbb{Q}}_t\\) linked by \\[ \\psi^{\\mathbb{Q}}_t (u) = \\psi_t (u+\\alpha_t) - \\psi_t (\\alpha_t). \\] In all approaches, we have to specify the status of the short rate \\(r_{t}\\) (developed in next slide). Remark: The status of the short rate The short rate \\(r_{t}\\) is a function of \\(\\underline{w_t}\\), this function may be known or unknown by the econometrician: It is known in two cases: \\(r_{t}\\) is exogenous \\(\\Rightarrow\\) \\(r_{t}(\\underline{w_t})\\) does not depend on \\(\\underline{w_t}\\), \\(r_{t}\\) is an endogenous factor \\(\\Rightarrow\\) \\(r_{t}\\) is a component of \\(w_t\\). If the function \\(r_{t} (\\underline{w_t})\\) is unknown, it has to be specified parametrically: \\[\\begin{equation*} \\left\\{ r_{t} (\\underline{w_t}, \\tilde{\\theta}), \\tilde{\\theta}\\in \\tilde{\\Theta} \\right\\}, \\end{equation*}\\] where \\(r_{t} (.,.)\\) is a known function. 2.5.1 The Direct Modelling (based on threes steps) Step 1: Specification of the historical dynamics We choose a parametric family for the conditional historical Log-Laplace transform \\(\\psi_t (u | \\underline{w_t})\\): \\[\\begin{equation*} \\left\\{ \\psi_t (u | \\underline{w_t} ; \\theta_1), \\theta_1 \\in \\Theta_1 \\right\\} . \\end{equation*}\\] * {} \\[\\begin{equation*} \\begin{array}{lll} \\mathcal{M}_{t,t+1} &amp;=&amp; \\exp \\left[ - r_{t} (\\underline{w_t}) + \\alpha&#39;_t (\\underline{w_t}) w_{t+1} - \\psi_t (\\alpha_t | \\underline{w_t}) \\right]; \\end{array} \\end{equation*}\\] Once \\(r_{t}\\) has been specified (\\(r_{t}(\\underline{w_t}, \\tilde{\\theta})\\)), \\(\\alpha_t(\\underline{w_t})\\) remains to be specified. We assume that \\(\\alpha_t (\\underline{w_t})\\) belongs to a parametric family: \\[\\begin{equation*} \\left\\{ \\alpha_t (\\underline{w_t} ; \\theta_2), \\theta_2 \\in \\Theta_2 \\right\\} \\end{equation*}\\] \\[\\begin{eqnarray*} \\Rightarrow \\mathcal{M}_{t,t+1}(\\underline{w_{t+1}}, \\theta) &amp;=&amp; \\exp \\left\\{ - r_{t} (\\underline{w_t}, \\tilde{\\theta}) + \\alpha&#39;_t (\\underline{w_t},\\theta_2) w_{t+1} \\right.\\\\ &amp;&amp; \\left. - \\psi_{t} \\left[ \\alpha_t (\\underline{w_t}, \\theta_2) | \\underline{w_t} ; \\theta_1 \\right] \\right\\} \\end{eqnarray*}\\] where \\(\\theta = (\\tilde{\\theta}&#39;, \\theta&#39;_1, \\theta&#39;_2)&#39; \\in \\tilde{\\Theta}\\times \\Theta_1 \\times \\Theta_2 = \\Theta\\); \\(\\tilde{\\Theta}\\) may be reduced to one point (Case i of Remark ST). Step 3: Internal consistency conditions (ICC) For any payoff \\(g(\\underline{w_s})\\) delivered at \\(s&gt;t\\), with price \\(p(\\underline{w_t})\\) at \\(t\\) which is a known function of \\(\\underline{w_t}\\), we must have: \\[\\begin{equation*}\\label{ICCgen} p(\\underline{w_t}) = \\mathbb{E} \\left\\{\\mathcal{M}_{t,t+1} (\\theta) ... \\mathcal{M}_{s-1,s} (\\theta) g(\\underline{w_s}) | \\underline{w_t}, \\theta_1 \\right\\} \\forall \\; \\underline{w_t}, \\theta . \\end{equation*}\\] These ICC pricing conditions may imply strong constraints on \\(\\theta\\), for instance: When components of \\(w_t\\) are returns of some assets: \\[ \\mbox{If }w_{1,t} = \\log \\dfrac{p_{1,t}}{p_{1,t-1}}, \\mbox{ then }\\mathbb{E}_t \\mathcal{M}_{t,t+1} \\exp (e&#39;_1 w_{t+1}) = 1. \\] or interest rates with various maturities: \\[\\begin{eqnarray*} &amp;&amp;\\mbox{If }w_{1,t} = -\\frac{1}{h}\\log B(t,h), \\\\ &amp;&amp; \\mbox{ then } e&#39;_1 w_{t} = - \\frac{1}{h} \\log \\mathbb{E}_t (\\mathcal{M}_{t,t+1}\\times \\dots \\times \\mathcal{M}_{t+h-1,t+h}). \\end{eqnarray*}\\] The previous three steps imply the specification of the R.N. dynamics: \\[\\begin{equation*} \\psi^{\\mathbb{Q}} (u | \\underline{w_t}, \\theta_1, \\theta_2) = \\psi_t \\left[ u + \\alpha_t (\\underline{w_t}, \\theta_2) | \\underline{w_t}, \\theta_1 \\right] - \\psi_t \\left[ \\alpha_t (\\underline{w_t}, \\theta_2) | \\underline{w_t}, \\theta_1 \\right]. \\end{equation*}\\] 2.5.2 The R.N. constrained direct modelling (or mixed modelling) Step 1: Specification of \\(\\mathbb{P}\\)-dynamics We select a family \\(\\{ \\psi_t (u | \\underline{w_t},\\theta_1), \\theta_1 \\in \\Theta_1 \\}\\): [to explain, for instance, some stylized fact about \\(w_t\\)]. Step 2: Specification of \\(\\mathbb{Q}\\)-dynamics We select a family \\(\\{\\psi^{\\mathbb{Q}}_t (u | \\underline{w_t}, \\theta^*),\\theta^* \\in \\Theta^* \\}\\) and possibly \\(\\{r_{t}(\\underline{w_t},\\tilde{\\theta}),\\tilde{\\theta}\\in\\tilde{\\Theta}\\}\\) [to have explicit or quasi explicit pricing formula]. Step 3: Internal Consistency Conditions (ICC) Once the parameterization \\((\\tilde{\\theta}, \\theta_1, \\theta^*) \\in \\tilde{\\Theta} \\times \\Theta^*_1\\) is defined, ICCs may be imposed: \\[\\begin{eqnarray*} &amp;&amp;\\mbox{If }w_{1,t} = \\log \\frac{p_{1,t}}{p_{1,t-1}}, \\mbox{ then } \\exp(-r_t)\\mathbb{E}^{\\mathbb{Q}}_t \\exp (e_{1}&#39; w_{t+1}) = 1\\\\ &amp;&amp;\\mbox{If }w_{1,t} = B(t,h), \\mbox{ then } e_{1}&#39; w_{t} = \\mathbb{E}_t^{\\mathbb{Q}} \\exp(-r_t - \\dots - r_{t+h-1}). \\end{eqnarray*}\\] The SDF is a by-product and, if we want an exponential affine SDF, for any pair \\((\\psi^{\\mathbb{Q}}_t, \\psi_t)\\) belonging to these families, there must exist a unique function \\(\\alpha_t (\\underline{w_t})\\) denoted by \\(\\alpha_t (w_t ; \\theta_1, \\theta^*)\\), and satisfying: \\[\\begin{equation*} \\psi^{\\mathbb{Q}}_t (u | \\underline{w_t}) = \\psi_t \\left[ u + \\alpha_t (w_t) | \\underline{w_t} \\right] - \\psi_t \\left[ \\alpha_t (\\underline{w_t}) | \\underline{w_t} \\right]. \\end{equation*}\\] 2.5.3 The Back Modelling (based on three steps) Step 1: Specification of the R.N. dynamics} [and possibly of \\(r_{t}(\\underline{w_t})\\)]: \\(\\psi^{\\mathbb{Q}}_t (u | \\underline{w_t}; \\theta^*_1)\\). Step 2: Internal consistency conditions (ICC) } –if relevant– are taken into account: \\[\\begin{equation*} \\begin{array}{lll} &amp;&amp; p(\\underline{w_t}) = \\mathbb{E}^{\\mathbb{Q}}_t \\left[ \\exp (-r_{t} (\\underline{w_t}, \\tilde{\\theta}) - \\ldots - r_{s-1} (\\underline{w_s}, \\tilde{\\theta}))g (\\underline{w_s}) | \\underline{w_t} , \\theta^*_1 \\right] ,\\\\ &amp;&amp; \\forall \\underline{w_t} , \\tilde{\\theta} , \\theta^*_1. \\end{array} \\end{equation*}\\] Step 3: Choice of the specification of \\(\\alpha_t(\\underline{w_t})\\)} (without any constraint), providing the family \\(\\{ \\alpha_t (\\underline{w_t}, \\theta^*_2),\\theta^*_2\\in \\Theta^*_2 \\}\\). * The historical dynamics is a by-product: \\[\\begin{equation*} \\psi_t(u | \\underline{w_t} ; \\theta^*_1, \\theta^*_2) = \\psi^{\\mathbb{Q}}_t \\left[ u - \\alpha_t (\\underline{w_t}, \\theta^*_2) | \\underline{w_t} ; \\theta^*_1 \\right] - \\psi^{\\mathbb{Q}}_t \\left[ - \\alpha_t (\\underline{w_t}, \\theta^{ *}_2) | \\underline{w_t}, \\theta^*_1 \\right]. \\end{equation*}\\] 2.6 THE TERM STRUCTURE OF RISK-FREE YIELDS library(ecb) rates &lt;- get_data( #&quot;YC.B.U2.EUR.4F.G_N_A.SV_C_YM.SR_10Y&quot;, &quot;FM.M.U2.EUR.4F.BB.R_U2_10Y.YLDA&quot;, filter=list(startPeriod=&quot;1991-12&quot;)) plot(as.Date(paste(rates$obstime,&quot;-15&quot;,sep=&quot;&quot;)),rates$obsvalue,type=&quot;l&quot;) spf &lt;- get_data( &quot;SPF.Q.U2.HICP.POINT.LT.Q.AVG&quot;, filter=list(startPeriod=&quot;1991-12&quot;)) spf$obstime &lt;- gsub(&quot;Q1&quot;, &quot;02-15&quot;, spf$obstime) spf$obstime &lt;- gsub(&quot;Q2&quot;, &quot;05-15&quot;, spf$obstime) spf$obstime &lt;- gsub(&quot;Q3&quot;, &quot;08-15&quot;, spf$obstime) spf$obstime &lt;- gsub(&quot;Q4&quot;, &quot;11-15&quot;, spf$obstime) lines(as.Date(paste(spf$obstime,&quot;-15&quot;,sep=&quot;&quot;)),spf$obsvalue,lty=3) Figure 2.1: Simulation of a quadratic processes \\(x_t\\). 2.6.1 Introduction Risk-free = with no default (and/or liquidity) risks. An important share of the term-structure literature pertains to the modelling of risk-free yields. Some models involve macro factors (in \\(w_t\\)). Some do not (yield-only models). Basic formula (see Eq. (1.10) in Example 1.18): \\[\\begin{eqnarray} B(t,h) &amp;=&amp; \\exp(-r_{t}) \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t+1}-\\dots-r_{t+h-1})\\\\ R(t,h) &amp;=&amp; - \\frac{1}{h} \\log B(t,h). \\tag{2.8} \\end{eqnarray}\\] Models used to forecast yields, or to compute term premiums = components of yields that would not exist if investors were not risk-averse. If this was the case (Expectation Hypothesis, EH), then \\(\\mathcal{M}_{t,t+1} = \\exp(- r_t) \\Rightarrow\\) ``\\(\\mathbb{P} \\equiv \\mathbb{Q}\\)’’ and \\(B(t,h)\\) would become: \\[\\begin{equation} \\exp(-r_{t}) \\mathbb{E}_t \\exp(-r_{t+1}-\\dots-r_{t+h-1}).\\tag{2.9} \\end{equation}\\] implying that the maturity-\\(h\\) yield-to-maturity would be: \\[ R^{EH}(t,h) = -\\frac{1}{h}\\log \\left( \\mathbb{E}_t \\exp(-r_t-\\dots-r_{t+h-1})\\right) \\approx \\frac{1}{h}\\mathbb{E}_t(r_t + \\dots + r_{t+h-1}). \\] \\(\\Rightarrow\\) Term premium given by: \\[\\begin{eqnarray} TP_{t,h} &amp;=&amp; \\underbrace{- \\frac{1}{h} \\log \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t+1}-\\dots-r_{t+h-1})}_{=R(t,h)} - \\nonumber \\\\ &amp;&amp; \\underbrace{- \\frac{1}{h} \\log \\mathbb{E}_t \\exp(-r_{t+1}-\\dots-r_{t+h-1}).}_{=R^{EH}(t,h)}\\tag{2.10} \\end{eqnarray}\\] * Interpretation of \\(TP_{t,h}\\)? * Under EH, investors are willing to buy a maturity-\\(h\\) bond as long as its expected return is – up to Jensen’s inequality – equal to the average of future short-term rates (Hence the definition of \\(R^{EH}(t,h)\\)). * \\(TP_{t,h}&gt;0\\) means that, due to risk aversion, investors are willing to buy the maturity-\\(h\\) bond only if its return is, on average, higher than expected future short-term rates. \\(\\Rightarrow\\) Reflects the fact that investors consider that long-term bonds tend to lose value in bad states of the world. 2.6.2 Risk premiums Recall Eq. ((2.3)), satisfied by any asset \\(j\\): \\[ p_{jt} = \\mathbb{E}_t(\\mathcal{M}_{t,t+1} p_{j,t+1}). \\] Can be rewritten: \\[ p_{jt} = \\mathbb{C}ov_t(\\mathcal{M}_{t,t+1}, p_{j,t+1}) + \\mathbb{E}_t(\\mathcal{M}_{t,t+1})\\mathbb{E}_t( p_{j,t+1}) \\] or \\[\\begin{equation} p_{jt} = \\underbrace{\\exp(-r_t)\\mathbb{E}_t( p_{j,t+1})}_{=p^{EH}_{jt}} + \\underbrace{\\mathbb{C}ov_t(\\mathcal{M}_{t,t+1}, p_{j,t+1})}_{\\mbox{Risk premium}}.\\tag{2.11} \\end{equation}\\] If investors were not risk-averse, then we would have \\(p_{jt} = p^{EH}_{jt}\\). S.D.F. is high (resp. low) in bad (resp. good) states of the world (states of high marginal utility in the equilibrium approach). \\(\\Rightarrow\\) \\(p_{jt}&lt; p^{EH}_{jt}\\) if asset \\(j\\) tends to pay less in bad states of the world (i.e. if \\(\\mathbb{C}ov_t(\\mathcal{M}_{t,t+1}, p_{j,t+1})&lt;0\\)). 2.6.3 Swap rates Bonds issued by top-rated (Aaa/AAA) countries are often considered to be risk-free. Because of call-margins mechanisms, swap rates are also used as risk-free benchmarks Duffie and Stein (2015). Interest Rate Swap (IRS): Contract at \\(t\\) between a receiver (of fixed rate) and a payer (of fixed rate); the receiver pays a floating rate to the payer. The dates of payment are \\(t+ \\tau\\), \\(t + 2\\tau\\), …, \\(t + n\\tau\\), where \\(\\tau\\) is a period expressed in years (typically 1/2 or 1/4) and \\(n\\) is the number of payments. \\(h = n \\tau\\) is the maturity, or tenor of the swap contract. The payoffs of the fixed leg are \\(\\tau S\\), where \\(S\\) is the annualized payment (or swap rate). On date \\(t+j\\tau\\), the payoff of the floating leg is \\(\\tau L(t+(j-1)\\tau,\\tau)\\), where \\(L\\), the annualized linear rate is given by: \\[ L(t+(j-1)\\tau,\\tau) = \\frac{1 - B(t+(j-1)\\tau,\\tau)}{\\tau B(t+(j-1)\\tau,\\tau)}. \\] Price at \\(t\\) of the fixed leg: \\[ \\sum_{j=1}^n \\tau S B(t,j\\tau). \\] Price at \\(t\\) of the floating leg: The payoff at date \\(t+j\\tau\\), that is \\(\\frac{1 - B(t+(j-1)\\tau,\\tau)}{B(t+(j-1)\\tau,\\tau)}\\) is know on date \\(t+(j-1)\\tau\\), so its price at date \\(t+(j-1)\\tau\\) is \\(1 - B(t+(j-1)\\tau,\\tau)\\), and its price at \\(t\\) therefore is \\(B(t,(j-1)\\tau) - B(t,j\\tau)\\). Summing over \\(j=1,\\dots,n\\), the date-\\(t\\) price of the floating leg is \\(1 - B(t,n\\tau)\\) (independent of the payment dates). Since the price of the contract is zero at date \\(t\\), we have: \\[ \\sum_{j=1}^n \\tau S B(t,j\\tau) = 1 - B(t,n\\tau) \\Rightarrow S = \\frac{1 - B(t,n\\tau)}{ \\tau \\sum_{j=1}^n B(t,j\\tau)}, \\] or \\[ S(t,h) = \\frac{1 - B(t,h)}{\\tau \\sum_{j=1}^{h/\\tau} B(t,j\\tau)}. \\] Note: In the context of an affine model, all the terms appearing in the previous formula are available in closed-form (see Example 1.18). 2.7 The Affine Case What do we get when \\(w_t\\) is affine (under \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\))? If the nominal short-term rate \\(r_t = \\omega_0 + \\omega&#39;_1 w_t\\), then: \\[\\begin{eqnarray*} B(t,h) &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp (-r_{t}-\\dots-r_{t+h-1})\\\\ &amp;=&amp; \\exp(-h\\omega_0 - \\omega&#39;_1 w_t) \\color{blue}{\\mathbb{E}^{\\mathbb{Q}}_t \\exp (- \\omega&#39;_1 w_{t+1}-\\dots- \\omega&#39;_1 w_{t+h-1})}. \\end{eqnarray*}\\] The (blue) expectation is easily computed using the recursive equations of XXX (see Example 1.18), leading to: \\[\\begin{equation} R(t,h)= - \\frac{1}{h} \\log B(t,h) = A_h&#39;w_t + B_h.\\tag{2.12} \\end{equation}\\] It is easily seen that we can also get: \\[\\begin{equation} R^{EH}(t,h) = {A^{EH}_h}&#39;w_t + B^{EH}_h.\\tag{2.13} \\end{equation}\\] Moreover, if inflation \\(\\pi_{t} = \\bar\\omega_0 + \\bar\\omega&#39;_1 w_t\\), then real yields are given by: \\[\\begin{eqnarray*} \\bar{B}(t,h) &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t}-\\dots-r_{t+h-1}+\\pi_{t+1}+\\dots+\\pi_{t+h}) \\end{eqnarray*}\\] (see Example 1.20) which also leads to: \\[\\begin{equation} \\bar{R}(t,h) = - \\frac{1}{h} \\log \\bar{B}(t,h) = \\bar{A}_h&#39;w_t + \\bar{B}_h.\\tag{2.14} \\end{equation}\\] Eqs. ((2.12)) and ((2.13)) imply that term premiums are affine in \\(w_t\\) (see Eq. (2.10)). Specifically: \\[ TP(t,h) = R(t,h) - E^{EH}(t,h) = B_h - B_h^{EH} + (A_h - A_h^{EH})&#39;w_t. \\] Expected excess returns resulting from holding zero-coupon bonds are also affine in \\(w_t\\). Indeed, holding a maturity-\\(h\\) zero-coupon bond for one period provides the following expected gross return: \\[ \\mathbb{E}_t\\left(\\frac{B(t+1,h-1)}{B(t,h)}\\right) = \\mathbb{E}_t\\left(\\exp(B_{h-1} - B_h + A_{h-1}&#39;w_{t+1} - A_h&#39;w_{t})\\right), \\] which is clearly exponential affine in \\(w_t\\) if \\(w_t\\) is an affine process. Therefore, the expected excess return, that is: \\[ \\log \\mathbb{E}_t\\left(\\frac{B(t+1,h-1)}{B(t,h)}\\right) - r_t \\] is also affine in \\(w_t\\) in this context. The fact that excess returns are affine in this context is exploited in the estimation approach proposed by Adrian, Crump and Moench (2013). Moreover, conditional expectations of future interest rates (real or nominal) and of term premiums are affine in \\(w_t\\). In particular: \\[\\begin{equation} \\mathbb{E}_t[R(t+k,h)] = \\mathbb{E}_t[{A_h}&#39;w_{t+k} + B_h] = {A_h}&#39;\\mathbb{E}_t(w_{t+k}) + B_h,\\tag{2.15} \\end{equation}\\] and \\(\\mathbb{E}_t(w_{t+k})\\) is affine in \\(w_t\\) (see Remark XXX, Eq. (1.15)). \\(\\Rightarrow\\) This can notably be used at the estimation stage, if one wants to fit survey data. Conditional variances of future interest rates (real or nominal) and of term premiums are affine in \\(w_t\\). In particular: \\[\\begin{equation} \\mathbb{V}ar_t[R(t+k,h)] = \\mathbb{V}ar_t[{A_h}&#39;w_{t+k} + B_h] = {A_h}&#39;\\mathbb{V}ar_t(w_{t+k})A_h,\\tag{2.16} \\end{equation}\\] where the components of \\(\\mathbb{V}ar_t(w_{t+k})\\) (and therefore \\(\\mathbb{V}ar_t[R(t+k,h)]\\)) is affine in \\(w_t\\) (see Remark XXX, Eq. (1.16)). This can notably be used at the estimation stage, if one wants to fit (proxies of) conditional variances. Remark: Forward Rate Agreement (FRA) In an affine model, forward rates are also linear in the state vector \\(w_t\\). An interest rate forward contract is a contract in which the rate to be paid or received on a specific obligation for a set period, beginning in the future, is set at contract initiation. Denote by \\(f(t,h_1,h_2)\\) the forward interest rate, set on date \\(t\\), for the period between \\(t+h_1\\) and \\(t+h_2\\). Consider two strategies (decided on date \\(t\\)): A. Buy a zero-coupon bond of maturity \\(h_2\\) (price \\(B(t,h_2)\\)) and sell zero-coupon bonds of maturity \\(h_1\\) for the same amount (yielding a payoff of \\(B(t,h_2)/B(t,h_1)\\) on date \\(t+h_1\\)). B. Enter a forward rate agreement between dates \\(t+h_1\\) and \\(t+h_2\\), whereby you receive 1 on date \\(t+h_2\\). * The two strategies deliver the same payoffs on date \\(t\\) (the payoff is zero) and on date \\(t+h_2\\) (the payoff is 1). By absence of arbitrage, the payoffs on date \\(t+h_1\\) have to be the same. Therefore \\[\\begin{eqnarray*} \\exp(-(h_2 - h_1)f(t,h_1,h_2)) &amp;=&amp; B(t,h_2)/B(t,h_1) \\\\ \\Rightarrow f(t,h_1,h_2) &amp;=&amp; \\frac{1}{h_2 - h_1}(\\log[B(t,h_1)] - \\log[B(t,h_2)])\\\\ &amp;=&amp; \\frac{h_2 R(t,h_2) - h_1 R(t,h_1)}{h_2 - h_1}. \\end{eqnarray*}\\] In an affine model, the maximum Sharpe ratio is easily computed. Eq. (2.11) implies that \\[ \\mathbb{E}_t\\underbrace{\\left(\\frac{p_{j,t+1}}{p_{j,t}} - \\exp(r_t)\\right)}_{=xs_{j,t+1},\\mbox{ excess return}} = - \\exp(r_t) \\mathbb{C}ov_t\\left(\\mathcal{M}_{t,t+1},\\frac{p_{j,t+1}}{p_{j,t}}\\right), \\] and, using \\(|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\\), we get the Hansen-Jagannathan (1991) bound: \\[\\begin{equation} \\underbrace{\\frac{\\mathbb{E}_t(xs_{j,t+1})}{\\sqrt{\\mathbb{V}ar_t(xs_{j,t+1})}}}_{\\mbox{Sharpe ratio}} \\le \\underbrace{\\frac{\\sqrt{\\mathbb{V}ar_t(\\mathcal{M}_{t,t+1})}}{\\mathbb{E}_t(\\mathcal{M}_{t,t+1})}}_{\\mbox{Maximum Sharpe ratio}}. \\end{equation}\\] If the s.d.f. is given by \\(\\mathcal{M}_{t,t+1} = \\exp[-r_{t}+\\alpha&#39;_tw_{t+1}-\\psi_t(\\alpha_t)]\\) (Eq. (2.5)), and using that \\(\\mathbb{E}_t(\\mathcal{M}_{t,t+1}^2)=\\exp(-2r_t+\\psi_t(2\\alpha_t)-2\\psi_t(\\alpha_t))\\) we get: \\[ \\mbox{Maximum Sharpe ratio} = \\sqrt{\\exp(\\psi_t(2\\alpha_t)-2\\psi_t(\\alpha_t)) - 1}. \\] 2.8 Gaussian Affine Term Structure Model Gaussian Affine Term Structure Model (GATSM) = Workhorse model, widely used in academic and economic-policy circles. \\(w_t\\) affine under \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\) (hence affine yields term premiums). Components of \\(w_t\\) valued in \\(\\mathbb{R}\\): easy to introduce macro-factors. The state vector \\(w_t\\) follows a Gaussian VAR(1): \\[\\begin{equation} w_{t+1} = \\mu + \\Phi w_{t} + \\Sigma^{1/2} \\varepsilon_{t+1}, \\mbox{ where } \\varepsilon_{t} \\sim i.i.d. \\mathcal{N}(0,Id).\\tag{2.17} \\end{equation}\\] (single lag without loss of generality since a VAR(p) admits a VAR(1) representation). This implies the following Laplace transform for \\(w_t\\) (see Example ??): \\[ \\psi_t(u) = \\log \\mathbb{E}_t(\\exp(u&#39;w_{t+1})|\\underline{w_t}) = \\color{blue}{u&#39;\\mu + u&#39;\\Phi w_t + \\frac{1}{2}u&#39;\\Sigma&#39;u}. \\] Using the notations of Eq. (2.5), the s.d.f. is defined as: \\[ \\mathcal{M}_{t,t+1} = \\exp(- r_t + \\alpha_t&#39;w_{t+1} - \\psi_t(\\alpha_t)), \\mbox{ where } \\alpha_t = \\alpha_0 + \\alpha_1&#39;w_t. \\] In that case, using Eq. (2.7), we get: \\[\\begin{eqnarray*} \\psi_t^{\\mathbb{Q}}(u) &amp;=&amp; \\psi_t(u + \\alpha_t) - \\psi_t(\\alpha_t)\\\\ &amp;=&amp; (u + \\alpha_t)&#39;\\mu + (u + \\alpha_t)&#39;\\Phi w_t + \\frac{1}{2}(u + \\alpha_t)&#39;\\Sigma(u + \\alpha_t) \\\\ &amp;&amp; - \\left(\\alpha_t&#39;\\mu + \\alpha_t&#39;\\Phi w_t + \\frac{1}{2}\\alpha_t&#39;\\Sigma\\alpha_t\\right) \\\\ &amp;=&amp; \\color{blue}{u&#39; \\left(\\mu + \\Sigma \\alpha_0 \\right) + u&#39;(\\Phi + \\Sigma \\alpha_1&#39;)w_t + \\frac{1}{2}u&#39;\\Sigma&#39;u}. \\end{eqnarray*}\\] The \\(\\mathbb{Q}\\)-dynamics of \\(w_t\\) is (from Example ??): \\[ w_{t+1} = \\mu + \\Sigma \\alpha_0 + (\\Phi + \\Sigma \\alpha_1&#39;) w_{t} + \\Sigma^{1/2} \\varepsilon^*_{t+1}, \\mbox{ where } \\varepsilon^*_{t} \\sim i.i.d. \\mathcal{N}^{\\mathbb{Q}}(0,Id). \\] With affine specifications of the nominal short term rate (\\(r_{t} = \\omega_0 + \\omega&#39;_1 w_t\\)) and of the inflation rate (\\(\\pi_{t} = \\bar\\omega_0 + \\bar\\omega&#39;_1 w_t\\)), we obtain affine formulas for nominal and real yields of any maturity (Eqs. (2.12) and (2.14)). Example 2.3 (Kim and Wright (2005)) This model is a three-factor yield-only model (no macro variables, except inflation in one variant of the model), where the short-term rate reads \\(r_t = \\omega_0 + \\omega_{1,1} w_{1,t} +\\omega_{1,2} w_{2,t} +\\omega_{1,3} w_{3,t}\\). The model estimated by Kalman filter (see Subsection,??; the state-space model (Def. ??) includes survey-based variables (see Subsection ??). Outputs are regularly updated by the Federal Reserve Board. Monthly data on the 6-month and 12-month-ahead forecasts of the three-month T-Bill yield from Blue Chip Financial Forecasts and semiannual data on the average expected three-month T-Bill yield from 6 to 11 years. Example 2.4 (Ang and Piazzesi (2003)) Ang and Piazzesi (2003): one of the first paper mixing latent and macrovariables. The set up is also of the form Eq. ((2.17)) except that the VAR features several lags. \\(w_t = [f^{o}_{1,t},f^{o}_{2,t},f^{u}_{1,t},f^{u}_{2,t},f^{u}_{3,t}]&#39;\\) where: \\(f^{o}_{1,t}\\) is the first Principal Component of a set of 3 price indexes (growth rates) \\(f^{o}_{2,t}\\) is the first Principal Component of a set of 4 real activity proxies (HELP, EMPLOY, IP, UE). \\(f^{u}_{i,t}\\) are unobserved, or latent, factors. Nominal short-term rate follows a Taylor rule. Latent factors are estimated via inversion techniques (Subsection ??). Example 2.5 (Joslin, Priebsch and Singleton (2014)) Models stating that \\(r_t\\) affine in macro-factors imply that macro-factors are spanned by the yield curve: macro-factors should be perfectly explained by yields of different maturities. Not the case in the data. Joslin Priebsch and Singleton (2014): unspanned macro risks. \\(w_t = [\\mathcal{P}_t&#39;,\\mathcal{M}_t&#39;]&#39;\\), where \\(\\mathcal{P}_t\\) are yield factors and \\(\\mathcal{M}_t\\) are macro factors. Model: \\[\\begin{eqnarray*} r_t &amp;=&amp; \\omega_{0} + \\omega_{\\mathcal{P}}&#39;\\mathcal{P}_t \\\\ \\left[\\begin{array}{c}\\mathcal{P}_t \\\\ \\mathcal{M}_t \\end{array}\\right] &amp;=&amp; \\left[\\begin{array}{cc}\\Phi_{\\mathcal{P}\\mathcal{P}}&amp;\\Phi_{\\mathcal{P}\\mathcal{M}} \\\\ \\Phi_{\\mathcal{M}\\mathcal{P}}&amp;\\Phi_{\\mathcal{M}\\mathcal{M}} \\end{array}\\right] \\left[\\begin{array}{c}\\mathcal{P}_{t-1} \\\\ \\mathcal{M}_{t-1} \\end{array}\\right] + \\Sigma \\varepsilon_t \\\\ \\left[\\begin{array}{c}\\mathcal{P}_t \\\\ \\mathcal{M}_t \\end{array}\\right] &amp;=&amp; \\mu + \\left[\\begin{array}{cc}\\Phi^{\\mathbb{Q}}_{\\mathcal{P}\\mathcal{P}}&amp;{\\color{red}0} \\\\ \\Phi^{\\mathbb{Q}}_{\\mathcal{M}\\mathcal{P}}&amp;\\Phi^{\\mathbb{Q}}_{\\mathcal{M}\\mathcal{M}} \\end{array}\\right] \\left[\\begin{array}{c}\\mathcal{P}_{t-1} \\\\ \\mathcal{M}_{t-1} \\end{array}\\right] + \\Sigma \\varepsilon^{\\mathbb{Q}}_t, \\end{eqnarray*}\\] where \\(\\varepsilon_t\\) and \\(\\varepsilon^{\\mathbb{Q}}_t\\) are \\(\\mathcal{N}(0,Id)\\) under \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\), respectively. \\(\\mathcal{M}_t\\) does Granger-cause \\(\\mathcal{P}_t\\) under \\(\\mathbb{Q}\\) and \\(r_t\\) is affine in \\(\\mathcal{P}_t\\) (only) \\(\\Rightarrow\\) yields \\(R(t,h)\\) are affine in \\(\\mathcal{P}_t\\) (only). * However \\(\\mathcal{M}_t\\) does not Granger-cause \\(\\mathcal{P}_t\\) under \\(\\mathbb{P}\\) \\(\\Rightarrow\\) macro-shocks affect the yield curve. Example 2.6 (Ang, Boivin, Dong and Loo-Kung (2011)) Ang, Boivin, Dong and Loo-Kung (2011): macro-finance model based on a quadratic framework. Taylor rule with time-varying parameters: \\[ r_t = \\omega_0 + a_t g_t + b_t \\pi_t, \\] where \\(x_t=(g_t,\\pi_t,a_t,b_t)&#39;\\) follows a Gaussian VAR. This is the context described in Example 1.7. \\(r_t\\) is linear in \\(w_t = (x_t,vec(x_t x_t&#39;)&#39;)&#39;\\). Specifically: \\[ r_t = \\omega_0 + \\omega_1&#39;w_t, \\] with \\(\\omega_1 = [v,vec(V)]&#39;\\), where \\[ v = \\left[ \\begin{array}{c} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right] \\quad \\mbox{and} \\quad V = \\left[ \\begin{array}{cccc} 0 &amp; 0&amp; 1/2&amp;0\\\\ 0&amp; 0&amp; 0&amp;1/2\\\\ 1/2&amp; 0&amp; 0&amp;0\\\\ 0&amp;1/2 &amp;0 &amp;0 \\end{array} \\right]. \\] 2.9 Non-Negative Affine Term Structure Model In the presence of physical currency, absence of arbitrage opportunity and of storing cost of cash, nominal interest rates should be \\(\\ge 0\\). Many standard models (e.g. Gaussian ATSM) are non consistent with non-negative nominal yields. Shadow-rate approach Black (1995): The short term rate is given by: \\[ r_t = \\max(s_t,0), \\] where \\(s_t\\) is the shadow short-term interest rate. Pricing formula not available in closed-form. Approximation formula available [e.g. Krippner (2013), Priebsch (2013), Wu and Xia (2016). Monfort, Pegoraro, Renne and Roussellet (2017) introduce an affine framework where the short-term rate can stay at zero for a prolonged period of time and with a stochastic lift-off probability. Example 2.7 (Monfort, Pegoraro, Renne and Roussellet (2017)) Vectorial Auto-Regressive Gamma (VARG): Multivariate extension of Example 1.8. Conditionally on \\(\\underline{w_t}\\), the \\(n\\) components of \\(w_{t+1}\\) are independent and distributed as follows: \\[\\begin{equation} \\frac{w_{i,t+1}}{\\mu_i} \\sim \\gamma(\\nu_i+z_{i,t}) \\quad \\mbox{where} \\quad z_{i,t} \\sim {\\mathcal P} \\left( \\alpha_i + \\beta_i&#39; w_t \\right).\\tag{2.18} \\end{equation}\\] * If \\(\\mu = (\\mu_1,\\dots,\\mu_n)&#39;\\), \\(\\alpha = (\\alpha_1,\\dots,\\alpha_n)&#39;\\), \\(\\nu = (\\nu_1,\\dots,\\nu_n)&#39;\\) and \\(\\beta = (\\beta_1,\\dots,\\beta_n)\\), then \\[\\begin{eqnarray*} \\varphi_t(u) &amp;=&amp; \\exp\\left[\\left(\\frac{u \\odot \\mu}{1 - u \\odot \\mu}\\right)&#39;\\beta&#39; w_t \\right.\\\\ &amp;&amp; \\left. + \\alpha&#39;\\left(\\frac{u \\odot \\mu}{1 - u \\odot \\mu}\\right) - \\nu&#39;\\log(1 - u \\odot \\mu)\\right], \\end{eqnarray*}\\] where \\(\\odot\\) denotes the element-by-element multiplication and, where, with abuse of notation, the division and log operators work element-by-element when applied to vectors. * Four factors. \\(\\nu_1 = \\nu_2 = 0 \\Rightarrow\\) \\(w_{1,t}\\) and \\(w_{2,t}\\) can stay at zero. * The short-term rate \\(r_t\\) is an affine combination of \\(w_{1,t}\\) and \\(w_{2,t}\\) only, hence can stay at zero. Specification: \\(r_t = \\omega&#39;w_t = \\omega_{1} w_{1,t} + \\omega_{2} w_{2,t}\\). * \\(w_{3,t}\\) and \\(w_{4,t}\\) Granger-cause \\(r_t\\) (they cause \\(w_{1,t}\\) and \\(w_{2,t}\\)) \\(\\Rightarrow\\) for \\(h \\ge 2\\) \\(R(t,h)\\) is a non-zero combination of the four components of \\(w_t\\). * (when \\(r_t=0\\)) the lift-off probability depends on \\(w_{3,t}\\) and \\(w_{4,t}\\). * Closed-form solutions for lift-off probabilities. Based on Lemma @ref(lemma:mass}: \\[ \\mathbb{P}_t(\\alpha&#39;w_{t+h}=0) = \\lim_{u \\rightarrow -\\infty} \\varphi_{t,h}(0,\\dots,0,u\\alpha), \\] where \\(\\varphi_{t,h}\\) is the multi-horizon Laplace transform defined in Eq. (1.8), which can be computed as explained in Remark XXX. We have: \\[\\begin{equation} \\left\\{ \\begin{array}{l} \\mathbb{P}_t(r_{t+h}&gt;0) = 1 - \\lim_{u \\rightarrow -\\infty} \\varphi_{t,h}(0,\\dots,0,u\\omega) \\\\ \\\\ \\mathbb{P}_t(r_{t+1}=0,\\dots,r_{t+h}=0) = \\lim_{u \\rightarrow -\\infty} \\varphi_{t,h}(u\\omega,\\dots,u\\omega,u\\omega) \\equiv p_{h}\\\\ \\\\ \\mathbb{P}_t(r_{t+1}=0,\\dots,r_{t+h-1}=0,r_{t+h}&gt;0) = p_{h-1} - p_h. \\end{array} \\right. \\end{equation}\\] * Other lift-off probabilities, of the type \\(\\mathbb{P}_t[R(t+h,k)&gt;threshold]\\), can be derived from Eq. (1.18). Model estimated by Kalman filtering techniques (see Subsection ??). Measurement equations: (levels of) yields, but also measurement equations involving: survey-based forecasts of yields (see Subsection ?? and (e-GARCH-based) proxies of conditional variances (see Eq. (1.16)). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
