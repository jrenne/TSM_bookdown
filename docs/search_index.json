[["index.html", "Introduction to Term Structure Models Introduction to Term Structure Models", " Introduction to Term Structure Models Jean-Paul Renne and Alain Monfort 2024-01-25 Introduction to Term Structure Models Modeling dynamic term structures serves as a practical and indispensable tool in the realm of finance. It enables investors, institutions, and policymakers to make informed decisions, manage risk effectively, and allocate resources wisely. By understanding how interest rates and yields evolve over time, these models offer a clear lens through which to assess market trends and price financial instruments accurately. This course has been developed by Jean-Paul Renne and Alain Monfort. It is illustrated by R codes using various packages that can be obtained from CRAN. This TSModels package is available on GitHub. To install it, one need to employ the devtools library: install.packages(&quot;devtools&quot;) # in case this library has not been loaded yet library(devtools) install_github(&quot;jrenne/TSModels&quot;) library(AEC) Useful (R) links: Download R: R software: https://cran.r-project.org (the basic R software) RStudio: https://www.rstudio.com (a convenient R editor) Tutorials: Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (by Oscar Torres-Reyna) R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (by Emmanuel Paradis) My own tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/ "],["ChapterAffine.html", "Chapter 1 Affine processes 1.1 Information in the Economy: The “factors” 1.2 Dynamic models and Laplace transform (L.T.) 1.3 Laplace Transform and moments/cumulants 1.4 Additional properties of the Laplace transform 1.5 Affine (or Car) processes 1.6 Markov chains 1.7 Wishart autoregressive (WAR) processes 1.8 Building affine processes 1.9 Multi-horizon Laplace transform 1.10 VAR representation and conditional moments 1.11 Truncated Laplace transforms of affine processes 1.12 Appendices", " Chapter 1 Affine processes 1.1 Information in the Economy: The “factors” On each date \\(t=1,2,\\dots,T\\), agents receive new information by observing factors, also called states. We denote the (\\(K\\)-dimensional) vector of factors by \\(w_t\\). Vector \\(w_t\\) is usually random. On date \\(t\\), vector \\(w_t\\) is supposed to be perfectly observed by the agents (investors), but can be only partially observed, or unobserved by the econometrician. Naturally, \\(w_t\\) can be decomposed into different sub-vectors of different natures. For instance, we can have \\(w_t = (y_t&#39;, z_t&#39;)&#39;\\) with * \\(y_t\\): observable vector of (geometric) returns, * \\(z_t\\): regime, unobserved by the econometrician. Some of the components of \\(w_t\\) can be prices. For instance, one component could be a short-term rate, a stock return, or an exchange rate. It can also include macroeconomic variables (inflation, GDP growth), or agent-specific variables. 1.2 Dynamic models and Laplace transform (L.T.) The objective of a dynamic model is to describe the random changes in \\(w_t\\). The dynamics can be historical or risk-neutral (see Chapter ??). The dynamics we consider are parametric, in the sense that the conditional distribution \\(w_{t+1}|\\underline{w_t}\\) (with \\(\\underline{w_t}=\\{w_t,w_{t-1},\\dots\\}\\)) depends on a vector of parameters \\(\\theta\\). In practice, it may be the case that \\(\\theta\\) is unknown by the econometrician (see Chapter ??). The choice (or estimation) of a conditional distribution is equivalent to the choice (or estimation) of a conditional Laplace transforms: \\[\\begin{equation} \\varphi(u|\\underline{w_t},\\theta) = \\mathbb{E}_{\\theta}[\\exp(u&#39;w_{t+1})|\\underline{w_t}], \\quad u \\in \\mathbb{R}^K,\\tag{1.1} \\end{equation}\\] or a conditional log Laplace transforms: \\[ \\psi(u|\\underline{w_t},\\theta) = \\log\\{\\mathbb{E}_{\\theta}[\\exp(u&#39;w_{t+1})|\\underline{w_t}]\\}, \\quad u \\in \\mathbb{R}^K. \\] Example 1.1 (Conditionally Bernoulli process) If \\(w_{t+1}|\\underline{w_t} \\sim {\\mathcal{I}} [p(\\underline{w_t},\\theta)]\\), then: \\[ \\varphi(u|w_t)= \\mathbb{E}[\\exp(u w_{t+1}) \\mid \\underline{w_t}] = p_t \\exp(u) + 1-p_t \\] with \\(p_t = p(\\underline{w_t}, \\theta)\\). Example 1.2 (Conditionally Binomial process) If \\(w_{t+1}|\\underline{w_t} \\in {\\mathcal{B}}(n, p_t)\\), then: \\[ \\varphi(u|w_t)=[p_t \\exp(u) + 1-p_t]^n. \\] Example 1.3 (Conditionally Poisson process) If \\(w_{t+1}|\\underline{w_t} \\sim {\\mathcal{P}}(\\lambda_t)\\), then: \\[\\begin{eqnarray*} \\varphi(u|w_t) &amp; =&amp; \\sum^\\infty_{j=0} \\dfrac{1}{j!} \\exp(-\\lambda_t) \\lambda^j_t \\exp(uj) = \\exp(-\\lambda_t) exp[\\lambda_t \\exp(u)] \\\\ &amp; =&amp; \\exp\\{\\lambda_t[\\exp(u)-1]\\}. \\end{eqnarray*}\\] Example 1.4 (Conditionally normal (or Gaussian) process) If \\(w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}\\left(m(\\underline{w_t},\\theta), \\Sigma(\\underline{w_t},\\theta)\\right)\\), then: \\[ u&#39;w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}\\left(u&#39;m(\\underline{w_t},\\theta), u&#39;\\Sigma(\\underline{w_t},\\theta)u\\right). \\] \\[ \\Rightarrow \\left\\{ \\begin{array}{ccc} \\varphi(u|\\underline{w_t},\\theta) &amp;=&amp; \\exp\\left[ \\begin{array}{l} u&#39;m(\\underline{w_t},\\theta)+ \\frac{1}{2} u&#39;\\Sigma(\\underline{w_t},\\theta)u\\end{array} \\right]\\\\ \\psi(u|\\underline{w_t},\\theta) &amp;=&amp; u&#39;m(\\underline{w_t},\\theta) + \\frac{1}{2} u&#39;\\Sigma(\\underline{w_t},\\theta)u. \\end{array} \\right. \\] 1.3 Laplace Transform and moments/cumulants Here are some properties of the Laplace transform (Eq. (1.1)): \\(\\varphi(0|\\underline{w_t},\\theta) = 1\\) and \\(\\psi(0|\\underline{w_t},\\theta)=0\\). It is defined in a convex set \\(E\\) (containing \\(0\\)). If the interior of \\(E\\) is non empty, all the (conditional) moments exist. As mentioned above, knowing the (conditional) Laplace transform is equivalent to knowing the (conditional) moments—if they exist. In the scalar case, we have that: the moment of order \\(n\\) closely relates to the \\(n^{th}\\) derivatives of \\(\\varphi\\): \\[ \\left[ \\begin{array}{l} \\dfrac{\\partial^n \\varphi(u|\\underline{w_t},\\theta)}{\\partial u^n} \\end{array} \\right]_{u=0} = \\mathbb{E}_{\\theta}[w^n_{t+1}|\\underline{w_t}], \\] the cumulant of order \\(n\\) closely relates to the \\(n^{th}\\) derivatives of \\(\\psi\\): \\[ \\left[ \\begin{array}{l} \\dfrac{\\partial^n \\psi(u|\\underline{w_t},\\theta)}{\\partial u^n} \\end{array} \\right]_{u=0} = K_n(\\underline{w_t},\\theta). \\] In particular, what precedes implies that: \\[ \\left\\{ \\begin{array}{ccc} K_1(\\underline{w_t},\\theta) &amp;=&amp; \\mathbb{E}_{\\theta}[w_{t+1}|\\underline{w_t}]\\\\ K_2(\\underline{w_t}, \\theta) &amp;=&amp; \\mathbb{V}ar_{\\theta}[w_{t+1}|\\underline{w_t}]. \\end{array} \\right. \\] Accordingly, \\(\\varphi\\) and \\(\\psi\\) are respectively called conditional moment and cumulant generating function. In the multivariate case, we have: \\[\\begin{eqnarray*} \\left[\\begin{array}{l} \\dfrac{\\partial \\psi}{\\partial u} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} &amp;=&amp; \\mathbb{E}_{\\theta}[w_{t+1}|\\underline{w_t}] \\\\ \\left[\\begin{array}{l} \\dfrac{\\partial^2 \\psi}{\\partial u\\partial u&#39;} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} &amp;=&amp; \\mathbb{V}ar_{\\theta}[w_{t+1}|\\underline{w_t}]. \\end{eqnarray*}\\] Example 1.4 (Conditionally normal (or Gaussian) process) Consider Example 1.4. Applying the previous formula, we have, in the scalar case: \\(\\psi(u|\\underline{w_t},\\theta)=u m(\\underline{w_t},\\theta) + \\frac{1}{2}u^2\\sigma^2(\\underline{w_t},\\theta)\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial \\psi}{\\partial u} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = m(\\underline{w_t},\\theta)\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial^2 \\psi}{\\partial u^2} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = \\sigma^2(\\underline{w_t},\\theta)\\). and, in the multidimensional normal case: \\(\\psi(u|\\underline{w_t},\\theta)=u&#39; m(\\underline{w_t},\\theta) + \\frac{1}{2}u&#39;\\Sigma(\\underline{w_t},\\theta)u\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial \\psi}{\\partial u} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = m(\\underline{w_t},\\theta)\\). \\(\\left[\\begin{array}{l} \\dfrac{\\partial^2 \\psi}{\\partial u\\partial u&#39;} (u|\\underline{w_t},\\theta) \\end{array} \\right]_{u=0} = \\Sigma(\\underline{w_t},\\theta)\\). In both cases, cumulants of order \\(&gt;2\\) equal to \\(0\\). 1.4 Additional properties of the Laplace transform Here are additional properties of multivariate Laplace transform: If \\(w_t=(w&#39;_{1t},w&#39;_{2t})&#39;\\) \\(, u=(u&#39;_1, u&#39;_2)&#39;\\): \\[\\begin{eqnarray*} \\mathbb{E}_{\\theta}[\\exp(u&#39;_1 w_{1,t+1}|\\underline{w_t})&amp;=&amp;\\varphi(u_1,0|\\underline{w_t},\\theta)] \\\\ \\mathbb{E}_{\\theta}[\\exp(u&#39;_2 w _{2,t+1}|\\underline{w_t})&amp;=&amp;\\varphi(0,u_2|\\underline{w_t},\\theta)]. \\end{eqnarray*}\\] If \\(w_t=(w&#39;_{1t},w&#39;_{2t})&#39;\\), and if \\(w_{1t}\\) and \\(w_{2t}\\) are conditionally independent: \\[\\begin{eqnarray*} \\varphi(u|\\underline{w_t},\\theta) &amp;=&amp; \\varphi(u_1,0|\\underline{w_t},\\theta)\\times\\varphi(0,u_2|\\underline{w_t},\\theta) \\\\ \\psi(u|\\underline{w_t},\\theta) &amp;=&amp; \\psi(u_1,0|\\underline{w_t},\\theta)+\\psi(0,u_2|\\underline{w_t},\\theta). \\end{eqnarray*}\\] If \\(w_{1t}\\) and \\(w_{2t}\\) have the same size and if \\[ \\varphi(u_1, u_2|\\underline{w_t},\\theta) = \\mathbb{E}_\\theta[\\exp(u&#39;_1 w_{1, t+1} + u&#39;_2 w_{2,t+1}|\\underline{w_t}], \\] then the conditional Laplace transform of \\(w_{1, t+1} + w_{2, t+1}\\) given \\(\\underline{w_t}\\) is \\(\\varphi(u, u|\\underline{w_t},\\theta)\\). In particular, if \\(w_{1t}\\) and \\(w_{2t}\\) are conditionally independent and have the same size, the conditional Laplace transform and Log-Laplace transform of \\(w_{1,t+1}+w_{2,t+1}\\) are respectively: \\[ \\varphi(u,0|\\underline{w_t},\\theta)\\times \\varphi(0, u|\\underline{w_t},\\theta), \\quad \\mbox{and}\\quad \\psi(u,0|\\underline{w_t},\\theta)+ \\psi(0, u|\\underline{w_t},\\theta). \\] Lemma 1.1 (Conditional zero probability for non-negative processes) If \\(w_t\\) is univariate and nonnegative its (conditional) Laplace transform \\(\\varphi_t(u) = \\mathbb{E}_t[\\exp(u w_{t+1})]\\) is defined for \\(u \\leq 0\\) and \\[ \\mathbb{P}_t(w_{t+1} = 0) = \\lim_{u\\rightarrow - \\infty} \\varphi_t(u). \\] Proof. We have \\(\\varphi_t(u) = \\mathbb{P}_t(w_{t+1} = 0) + \\int_{w_{t+1}&gt; 0} \\exp(u w_{t+1}) d\\mathbb{P}_t(w_{t+1})\\). The Lebesgue theorem ensures that the last integral converges to zero when \\(u\\) goes to \\(-\\infty\\). Lemma 1.2 (Conditional zero probability for non-negative multivariate processes) Assume that: \\(w_{1,t}\\) is valued in \\(\\mathbb{R}^{d}\\) (\\(d \\geq 1\\)), \\(w_{2,t}\\) is valued in \\(\\mathbb{R}^+ = [0, + \\infty )\\), \\(\\mathbb{E}_t \\left[ \\exp \\left( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} \\right) \\right]\\) exists for a given \\(u_1\\) and \\(u_2 \\leq 0\\). Then, we have: \\[\\begin{equation} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1}) \\textbf{1}_{\\{w_{2,t+1} = 0 \\}} \\right] = \\underset{u_2 \\rightarrow -\\infty}{\\lim} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} ) \\right].\\tag{1.2} \\end{equation}\\] Proof. We have that \\[\\begin{eqnarray*} &amp;&amp;\\underset{u_2 \\rightarrow -\\infty}{\\lim} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} ) \\right] \\\\ &amp;=&amp; \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1}) \\textbf{1}_{\\{w_{2,t+1} = 0 \\}} \\right] +\\\\ &amp;&amp; \\underset{u_2 \\rightarrow -\\infty}{\\lim} \\mathbb{E}_t \\left[ \\exp( u_1 &#39; w_{1,t+1} + u_2 w_{2,t+1} ) \\textbf{1}_{\\{w_{2,t+1} &gt; 0 \\}} \\right] , \\end{eqnarray*}\\] and since in the second term on the right-hand side \\(\\exp(u_2 w_{2,t+1}) \\textbf{1}_{\\{w_{2,t+1} &gt; 0 \\}} \\rightarrow 0\\) when \\(u_2 \\rightarrow -\\infty\\), Eq. (1.2) is a consequence of the Lebesgue theorem. 1.5 Affine (or Car) processes In term structure applications, we will often consider affine processes (Definitions 1.1 and 1.2). These processes are indeed such that their multi-horizon Laplace transform are simple to compute (Lemma 1.5 and Proposition 1.5), which is key to compute bond prices. 1.5.1 Car processes of order one Here is the definition of a compound auto-regressive (Car) process of order one: Definition 1.1 (Affine process of order 1) A multivariate process \\(w_{t+1}\\) is Affine of order 1 [or \\(Car(1)\\)] if \\[ \\varphi_t(u)=\\mathbb{E}_t[\\exp(u&#39;w_{t+1})]=\\exp[a(u)&#39;w_t+b(u)] \\] for some functions \\(a(.)\\) and \\(b(.)\\). These functions are univariate if \\(w_{t+1}\\) (and therefore \\(u\\)) is scalar. Note that \\(a(.)\\) and \\(b(.)\\) may be deterministic functions of time (e.g., Chikhani and Renne (2023)). A first key example is that of the Gaussian auto-regressive processes: Example 1.5 (Univariate AR(1) Gaussian process) If \\(w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\nu+\\rho w_t, \\sigma^2)\\), then: \\[ \\varphi_t(u) = \\exp\\left( \\begin{array}{l} u \\rho w_t + u \\nu + u^2 \\frac{\\sigma^2}{2} \\end{array} \\right) = \\exp[a(u)&#39;w_t+b(u)], \\] \\[ \\mbox{with }\\left\\{ \\begin{array}{ccc} a(u) &amp;=&amp; u \\rho\\\\ b(u) &amp;=&amp; u \\nu + u^2 \\dfrac{\\sigma^2}{2}. \\end{array} \\right. \\] Example 1.6 (Gaussian VAR) If \\(w_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\mu+\\Phi w_t, \\Sigma)\\), then: \\[ \\varphi_t(u) = \\exp\\left( \\begin{array}{l} u&#39; (\\mu + \\Phi w_t) + \\frac{1}{2} u&#39; \\Sigma u \\end{array} \\right) = \\exp[a(u)&#39;w_t+b(u)], \\] \\[ \\mbox{with }\\left\\{ \\begin{array}{ccl} a(u) &amp;=&amp; \\Phi&#39;u\\\\ b(u) &amp;=&amp; u&#39; \\mu + \\frac{1}{2} u&#39; \\Sigma u = u&#39; \\mu + \\frac{1}{2}(u \\otimes u)&#39; vec(\\Sigma). \\end{array} \\right. \\] Example 1.7 (Quadratic Gaussian process) Consider vector \\(w_t = (x&#39;_t,vec(x_t x_t&#39;)&#39;)&#39;\\), where \\(x_t\\) is a \\(n\\)-dimensional vector following a Gaussian VAR(1), i.e. \\[ x_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\mu+\\Phi x_t, \\Sigma). \\] Proposition 1.2 shows that if \\(u = (v,V)\\) where \\(v \\in \\mathbb{R}^n\\) and \\(V\\) a square symmetric matrix of size \\(n\\), we have: \\[\\begin{eqnarray*} \\varphi_t(u) &amp;=&amp; \\mathbb{E}_t\\big\\{\\exp\\big[(v&#39;,vec(V)&#39;)\\times w_{t+1}\\big]\\big\\} \\\\ &amp; =&amp; \\exp \\left\\{a_1(v,V)&#39;x_t +vec(a_2(v,V))&#39; vec(x_t&#39;x_t) + b(v,V) \\right\\}, \\end{eqnarray*}\\] where: \\[\\begin{eqnarray*} a_2(u) &amp; = &amp; \\Phi&#39;V (I_n - 2\\Sigma V)^{-1} \\Phi \\nonumber \\\\ a_1(u) &amp; = &amp; \\Phi&#39;\\left[(I_n-2V\\Sigma)^{-1}(v+2V\\mu)\\right] \\nonumber \\\\ b(u) &amp; = &amp; u&#39;(I_n - 2 \\Sigma V)^{-1}\\left(\\mu + \\frac{1}{2} \\Sigma v\\right) +\\\\ &amp;&amp; \\mu&#39;V(I_n - 2 \\Sigma V)^{-1}\\mu - \\frac{1}{2}\\log\\big|I_n - 2\\Sigma V\\big|.\\tag{1.3} \\end{eqnarray*}\\] Quadratic processes can be used to construct positive process. Indeed, one can determine linear combinations of the components of \\(w_t\\) (\\(\\alpha&#39;w_t\\), say) that are such that \\(\\alpha&#39;w_t \\ge 0\\). For instance, if \\(x_t\\) is scalar, \\(\\alpha&#39;w_t = x_t^2\\) if \\(\\alpha = (0,1)&#39;\\). This is illustrated by Figure 1.1. T &lt;- 200 phi &lt;- .9;sigma &lt;- 1 x.t &lt;- 0; x &lt;- NULL for(t in 1:T){ x.t &lt;- phi*x.t + sigma*rnorm(1) x &lt;- c(x,x.t)} par(mfrow=c(1,2),plt=c(.1,.95,.15,.85)) plot(x,type=&quot;l&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;x_t&quot;) plot(x^2,type=&quot;l&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;x_t^2&quot;) Figure 1.1: Simulation of a quadratic processes \\(x_t\\). Another example of nonnegative process is that of the auto-regressive Gamma process (Christian Gourieroux and Jasiak 2006) and its extension (Alain Monfort et al. 2017). Example 1.8 (Autoregressive gamma process, ARG(1)) An ARG process is defined as follows: \\[ \\frac{w_{t+1}}{\\mu} \\sim \\gamma(\\nu+z_t) \\quad \\mbox{where} \\quad z_t \\sim \\mathcal{P} \\left( \\frac{\\rho w_t}{\\mu} \\right), \\] with \\(\\nu\\), \\(\\mu\\), \\(\\rho &gt; 0\\). (Alternatively \\(z_t \\sim {\\mathcal{P}}(\\beta w_t)\\), with \\(\\rho = \\beta \\mu\\).) Proposition 1.3 shows that we have \\(\\varphi_t(u) = \\exp[a(u)&#39;w_t+b(u)]\\) with \\[ \\left\\{ \\begin{array}{ccc} a(u) &amp;=&amp; \\dfrac{\\rho u}{1-u \\mu}\\\\ b(u) &amp;=&amp; -\\nu \\log(1-u \\mu). \\end{array} \\right. \\] One can simulate ARG processes by using this web-interface (select the “ARG” panel). It can be shown that: \\[ \\left\\{ \\begin{array}{ccc} \\mathbb{E}(w_{t+1}|\\underline{w_t}) &amp;=&amp; \\nu \\mu + \\rho w_t \\\\ \\mathbb{V}ar(w_{t+1}|\\underline{w_t}) &amp;=&amp; \\nu \\mu^2 + 2 \\mu \\rho w_t. \\end{array} \\right. \\] and that: \\[ w_{t+1}=\\nu\\mu+\\rho w_t+\\varepsilon_{t+1}, \\] where \\(\\varepsilon_{t+1}\\) is a martingale difference \\(\\Rightarrow\\) \\(w_{t+1}\\) is a weak \\(AR(1)\\). Alain Monfort et al. (2017) porpose the extended ARG process and the ARG\\(_0\\) process. The latter is such that \\(\\nu = 0\\) and \\(\\beta w_t\\) is replaced with \\(\\alpha + \\beta w_t\\), i.e.: \\[\\begin{equation} \\frac{w_{t+1}}{\\mu} \\sim \\gamma(z_t),\\quad z_t \\sim {\\mathcal{P}}(\\alpha + \\beta w_t).\\tag{1.4} \\end{equation}\\] It is easily seen that we then have: \\[ \\varphi_t(u) = exp \\left[\\frac{\\beta \\mu u}{1-u \\mu} w_t + \\frac{\\alpha \\mu u}{1-u \\mu} \\right]. \\] The ARG\\(_0\\) process features a point mass at zero, with conditional probability \\(exp(-\\alpha - \\beta w_t)\\). Note that 0 is absorbing if \\(\\alpha = 0\\). Figure 1.2 displays the simulated path of an ARG\\(_0\\) process (since we set \\(\\nu\\) to zero). Note that function simul.ARG is included in the TSModels package. library(TSModels) W &lt;- simul.ARG(300,mu=.5,nu=0,rho=.9,alpha=.1) plot(W,type=&quot;l&quot;) Figure 1.2: Simulation of an ARG0 processes. Certain affine processes are valued in specific sets (e.g., integers). It is the case of compound Poisson proceses: Example 1.9 (Compound Poisson process) A compound Poisson process is defined as follows (with \\(\\gamma &gt; 0\\), \\(0 &lt; \\pi&lt; 1\\), and \\(\\lambda &gt; 0\\)): \\[\\frac{w_{t+1}}{\\gamma} = z_{t+1} + \\varepsilon_{t+1}, \\] where \\(z_{t+1}\\) and \\(\\varepsilon_{t+1}\\) conditionally independent, and where \\(z_{t+1} \\sim {\\mathcal B} \\left(\\frac{w_t}{\\gamma},\\pi\\right)\\), with \\(\\varepsilon_{t+1} \\sim {\\mathcal P}(\\lambda)\\). This process is valued in \\(\\{j \\gamma, j \\in \\mathbb{N}, \\gamma \\in \\mathbb{R}^+\\}\\) and we have: \\[ \\varphi_t(u) = \\exp\\left\\{ \\begin{array}{l} \\dfrac{w_t}{\\gamma} \\log[\\pi \\exp(u\\gamma)+1-\\pi]-\\lambda[1-\\exp(u \\gamma)] \\end{array} \\right\\}, \\] i.e., \\(\\varphi_t(u) = \\exp\\left(a(u)w_t+b(u)\\right)\\) with \\[ \\left\\{ \\begin{array}{ccl} a(u)&amp;=&amp; \\frac{1}{\\gamma} \\log[\\pi \\exp(u \\gamma)+1-\\pi],\\\\ b(u) &amp;=&amp; -\\lambda[1-\\exp(u \\gamma)]. \\end{array} \\right. \\] We also have: \\(w_{t+1} = \\pi w_t + \\lambda \\gamma + \\eta_{t+1}\\), where \\(\\eta_{t+1}\\) is a martingale difference. One can simulate such processes by using this web-interface (select the “Compound Poisson” panel). Figure 1.3 makes use of function simul.compound.poisson (in package TSModels) to simulate a compound Poisson process. library(TSModels) W &lt;- simul.compound.poisson(100,Gamma=.5,Pi=0.5,lambda=.9) plot(W,type=&quot;l&quot;) Figure 1.3: Simulation of a Compound Poisson process. 1.5.2 Car processes of order \\(p\\) Let us now define Car processes of order \\(p\\): Definition 1.2 (Affine process of order p) A multivariate process \\(w_{t+1}\\) is affine of order \\(p\\) [or \\(Car(p)\\)] if there exist functions \\(a_1(.),\\dots,a_p(.)\\), and \\(b(.)\\) such that: \\[ \\varphi_t(u)=\\mathbb{E}_t[\\exp(u&#39; w_{t+1})]=\\exp[a_1(u)&#39;w_t+\\dots+a_p(u)&#39;w_{t+1-p}+b(u)]. \\] It can be seen that if \\(w_t\\) is \\(Car(p)\\), then \\(W_t = [w_t&#39;, w_{t-1}&#39;,\\dots,w_{t-p+1}&#39;]&#39;\\) is \\(Car(1)\\).1 Therefore, without loss of generality we can assume \\(p = 1\\). The standard Car(\\(p\\)) processes are auto-regressive processes of order \\(p\\). These processes satisfy the definition of index affine processes: Definition 1.3 (Univariate index affine process of order p) Let \\(\\exp[a(u)w_t+b(u)]\\) be the conditional Laplace transform of a univariate affine process of order 1, the process \\(w_{t+1}\\) is an index-affine process of order \\(p\\) if: \\[ \\varphi_t(u)=\\mathbb{E}_t[\\exp(u w_{t+1})]=\\exp[a(u)(\\beta_1 w_t+\\dots+\\beta_p w_{t+1-p})+b(u)]. \\] Examples 1.10 and 1.11 are two examples of index affine processes. Example 1.10 (Gaussian AR(p) process) This example extends Example 1.5. Consider a Gaussian AR(p) process \\(w_t\\); that is: \\[ w_{t+1} = \\nu + \\varphi_1 w_t +\\dots+ \\varphi_p w_{t+1-p}+\\sigma \\varepsilon_{t+1},\\quad \\varepsilon_{t+1} \\sim i.i.d. \\mathcal{N}(0,1). \\] We have: \\[ \\varphi_t(u) = \\exp \\left[ u \\rho (\\beta_1 w_t+\\dots+\\beta_p w_{t+1-p})+u\\nu + u^2 \\frac{\\sigma^2}{2}\\right] \\] with \\(\\varphi_i = \\rho \\beta_i\\). Example 1.11 (ARG(p) process (positive)) This example extends Example 1.8. An ARG process of order \\(p\\) is defined as follows: \\[ \\frac{w_{t+1}}{\\mu} \\sim \\gamma(\\nu+z_t) \\quad \\mbox{where} \\quad z_t \\sim \\mathcal{P} \\left( \\beta_1 w_t+\\dots+\\beta_p w_{t+1-p} \\right), \\] with \\(\\nu\\), \\(\\mu\\), \\(\\beta_i &gt; 0\\). We have: \\[ \\varphi_t(u) = \\exp\\left[\\frac{\\rho u}{1-u \\mu} (\\beta_1 w_t+\\dots+\\beta_p w_{t+1-p})-\\nu \\log(1-u\\mu)\\right], \\] Process \\(w_t\\) admits the following AR(\\(p\\)) representation: \\[ w_{t+1} = \\nu\\mu + \\varphi_1 w_t +\\dots+ \\varphi_p w_{t+1-p}+\\varepsilon_{t+1}, \\] with \\(\\varphi_i = \\beta_i\\mu\\) and where \\(\\varepsilon_{t+1}\\) is a martingale difference. 1.6 Markov chains In this subsection, we show that the family of affine processes includes (some) regime-switching models. We consider a time-homogeneous Markov chain \\(z_t\\), valued in the set of columns of \\(Id_J\\), the identity matrix of dimension \\(J \\times J\\). The transition probabilities are denoted by \\(\\pi(e_i, e_j)\\), with \\(\\pi(e_i, e_j) = \\mathbb{P}(z_{t+1}=e_j | z_t=e_i)\\). With these notations: \\[ \\mathbb{E}[\\exp(v&#39;z_{t+1})|z_t=e_i,\\underline{z_{t-1}}] = \\sum^J_{j=1} \\exp(v&#39;e_j)\\pi (e_i, e_j). \\] Hence, we have: \\[ \\varphi_t(v) = \\exp[a_z(v)&#39;z_t], \\] with \\[ a_z(v)= \\left[ \\begin{array}{c} \\log \\left(\\sum^J_{j=1} \\exp(v&#39;e_j) \\pi(e_1,e_j)\\right)\\\\ \\vdots\\\\ \\log \\left(\\sum^J_{j=1} \\exp(v&#39;e_j) \\pi(e_J,e_j)\\right) \\end{array}\\right]. \\] This proves that \\(z_t\\) is an affine process. One can simulate a two-regime Markov chain by using this web-interface (select the “Markov-Switching” panel). 1.7 Wishart autoregressive (WAR) processes WAR are matrix processes, valued in the space of \\((L \\times L)\\) symmetric positive definite matrices. Definition 1.4 (Wishart autoregressive (WAR) processes) Let \\(W_{t+1}\\) be a \\(WAR_L(K, M, \\Omega)\\) process. It is defined by: \\[\\begin{eqnarray} &amp;&amp;\\mathbb{E}[\\exp Tr(\\Gamma W_{t+1})|\\underline{W_t}] \\tag{1.5}\\\\ &amp;=&amp; \\exp\\left\\{Tr[M&#39;\\Gamma(Id-2\\Omega \\Gamma)^{-1}M W_t] - \\frac{K}{2} \\log [det(Id-2\\Omega \\Gamma)]\\right\\}, \\nonumber \\end{eqnarray}\\] where \\(\\Gamma\\) is a symmetric matrix,2 \\(K\\) is a positive scalar, \\(M\\) is a \\((L \\times L)\\) matrix, and \\(\\Omega\\) is a \\((L \\times L)\\) symmetric positive definite matrix. If \\(K\\) is an integer, Proposition 1.4 (in the appendix) shows that \\(W_{t+1}\\) can be obtained from: \\[\\begin{eqnarray*} \\left\\{ \\begin{array}{ccl} W_{t+1} &amp; =&amp; \\sum^K_{k=1} x_{k,t+1} x&#39;_{k,t+1}\\\\ &amp;&amp;\\\\ x_{k,t+1} &amp; =&amp; M x_{k,t} + \\varepsilon_{k,t+1},\\quad k \\in \\{1,\\dots,K\\}, \\end{array} \\right. \\end{eqnarray*}\\] where \\(\\varepsilon_{k,t+1} \\sim i.i.d. \\mathcal{N}(0, \\Omega)\\) (independent across \\(k\\)’s). The proposition also shows that we have: \\[ \\mathbb{E}(W_{t+1}|\\underline{W_t}) = MW_tM&#39;+K \\Omega, \\] i.e. \\(W_t\\) follows a matrix weak AR(1) process. In particular case, where \\(L=1\\) (univariate case), we have that: \\[\\begin{eqnarray*} \\mathbb{E}[\\exp(u W_{t+1})|\\underline{W_t}] = \\exp\\left[ \\frac{u m^2}{1-2\\omega u}W_t - \\frac{K}{2} \\log(1-2\\omega u)\\right]. \\end{eqnarray*}\\] Hence, when \\(L=1\\), the Wishart process boils down to an \\(ARG(1)\\) process (Example 1.8) with \\(\\rho = m^2\\), \\(\\mu = 2\\omega\\), \\(\\nu = \\frac{K}{2}\\). 1.8 Building affine processes 1.8.1 Univariate affine processes with stochastic parameters Some univariate affine processes can be extended if they satisfy certain conditions. Specifically, consider a univariate affine process whose conditional L.T. is of the form: \\[\\begin{equation} \\mathbb{E}_t \\exp(u y_{t+1}) = \\exp[a_0(u)y_t+b_0(u)\\delta],\\tag{1.6} \\end{equation}\\] where \\(\\delta = (\\delta_1,\\dots,\\delta_m)&#39; \\in \\mathcal{D}\\). This process can be generalized by making \\(\\delta\\) stochastic (while staying in the affine family). More precisely assume that: \\[ \\mathbb{E}[\\exp(u y_{t+1})|\\underline{y_t}, \\underline{z_{t+1}}] = \\exp[a_0(u)y_t+b_0(u)&#39;\\Lambda z_{t+1}], \\] where \\(\\Lambda\\) is a \\((m\\times k)\\) matrix, with \\(\\Lambda z_{t+1} \\in \\mathcal{D}\\). In this case, if: \\[ \\mathbb{E}[\\exp(v&#39; z_{t+1})|\\underline{y_t}, \\underline{z_{t}}] = \\exp[a_1(v)&#39;z_t+b_1(v)], \\] then \\(w_{t+1} = (y_{t+1}, z&#39;_{t+1})&#39;\\) is affine.3 Example 1.12 (Gaussian AR(p)) Using the notation of Example 1.10, it comes that an AR(p) processes satisfies Eq. (1.6) with \\(b_0(u) = \\left(u, \\; \\frac{u^2}{2}\\right)&#39;\\) and \\(\\delta = (\\nu,\\sigma^2)&#39; \\in \\mathcal{D}=\\mathbb{R} \\times \\mathbb{R}^+\\). In that case, \\(\\delta\\) (the vector of conditional mean and variance) can be replaced by … \\(\\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right)\\), where \\(z_{1,t+1}\\) and \\(z_{2,t+1}\\) are independent AR(1) (see Example 1.5) and ARG(1) (see Example 1.8) processes, respectively. \\(\\left( \\begin{array}{ll} \\lambda&#39;_1 &amp; 0 \\\\ 0 &amp; \\lambda&#39;_2 \\end{array} \\right)\\)\\(\\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right)\\), where \\(z_{1,t+1}\\) and \\(z_{2,t+1}\\) are independent Markov chains. \\(\\left( \\begin{array}{l} \\lambda&#39;_1 \\\\ \\lambda&#39;_2 \\end{array}\\right)z_{t+1}\\), where \\(z_{t+1}\\) is a Markov chain. Example 1.11 (ARG(p) model) \\(b_0(u)= - \\nu \\log(1-u\\mu)\\), \\(\\delta=\\nu\\). \\(\\nu\\) (\\(\\ge 0\\)) can be specified for instance as a Markov chain or an ARG. 1.8.2 Multivariate affine processes One can construct multivariate affine processes by employing the so-called recursive approach. Let us illustrate this by considering the bivariate case. (The multivariate generalization is straightforward.) Consider \\(w_t = \\left(\\begin{array}{c} w_{1,t}\\\\ w_{2,t} \\end{array} \\right)\\),and assume that we have: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}[\\exp(u_1 w_{1,t+1}|\\underline{w_{1,t}}, \\underline{w_{2,t}})]\\\\ &amp;=&amp; \\exp[a_{11}(u_1)w_{1,{\\color{red}{t}}}+a_{12}(u_1)w_{2,{\\color{red}{t}}}+b_{1}(u_1)], \\end{eqnarray*}\\] and: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(u_2 w_{2,t+1}|\\underline{w_{1,t+1}}, \\underline{w_{2,t}})]\\\\ &amp;= &amp; \\exp[a_0(u_2)w_{1,{\\color{red}{t+1}}}+a_{21}(u_2)w_{1,{\\color{red}{t}}}+a_{22}(u_2)w_{2,{\\color{red}{t}}}+b_2(u_2)]. \\end{eqnarray*}\\] Then \\(w_t\\) is an affine process.4 The dynamics of the two components of \\(w_t\\) are of the form: \\[\\begin{eqnarray*} w_{1,t+1} &amp;=&amp; \\alpha_1 \\hspace{1.55cm} + \\alpha_{11}w_{1,t} + \\alpha_{12}w_{2,t} + \\varepsilon_{1,t+1} \\\\ w_{2,t+1} &amp;=&amp; \\alpha_2 + \\alpha_{0}w_{1,t+1} + \\alpha_{21}w_{1,t} + \\alpha _{22} w_{2,t} + \\varepsilon_{2,t+1} \\end{eqnarray*}\\] Note that \\(\\varepsilon_{1,t+1}\\) and \\(\\varepsilon_{2,t+1}\\) are non-correlated martingale differences. In the general case, they are conditionally heteroskedastic. What precedes is at play in \\(VAR\\) model; Alain Monfort et al. (2017) employ this approach to build vector auto-regressive gamma (VARG) processes. 1.8.3 Extending multivariate stochastic processes Consider the same framework as in Section 1.8.1 when \\(y_t\\) is a \\(n\\)-dimensional vector. That is, replace Eq. (1.6) with: \\[\\begin{equation} \\mathbb{E}_t \\exp(u&#39; y_{t+1}) = \\exp[a_0(u)&#39;y_t+b_0(u)\\delta],\\tag{1.7} \\end{equation}\\] and further assume that \\(\\delta\\) is stochastic and depends on \\(z_t\\), such that: \\[ \\mathbb{E}[\\exp(u y_{t+1})|\\underline{y_t}, \\underline{z_{t+1}}] = \\exp[a_0(u)y_t+b_0(u)&#39;\\Lambda z_{t+1}], \\] where \\(\\Lambda\\) is a \\((m\\times k)\\) matrix, with \\(\\Lambda z_{t+1} \\in \\mathcal{D}\\). In this case, if: \\[ \\mathbb{E}[\\exp(v&#39; z_{t+1})|\\underline{y_t}, \\underline{z_{t}}] = \\exp[a_1(v)&#39;z_t+b_1(v)], \\] then \\(w_{t+1} = (y_{t+1}, z&#39;_{t+1})&#39;\\) is affine. Example 1.13 (Stochastic parameters Gaussian VAR(1)) This example extends Example 1.6. Using the same notations as in the latter example 1.6, we have \\[ b_0(u) = \\left(u&#39;, \\frac{1}{2} (u \\otimes u)&#39;\\right)&#39; \\quad \\mbox{and} \\quad\\delta = (\\mu&#39;, vec(\\Sigma)&#39;)&#39; \\in \\mathbb{R}^n \\times vec(\\mathcal{S}), \\] where \\(\\mathcal{S}\\) is the set of symmetric positive semi-definite matrices. Vector \\(\\delta\\) can be replaced by: \\[ \\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right), \\] where \\(z_{1,t+1}\\) is, for instance, a Gaussian VAR process. \\(z_{2,t+1}\\) is obtained by applying the \\(vec\\) operator to a Wishart process, replaced by \\(\\Lambda_2 z_{2,t+1}\\), where \\(\\Lambda_2\\) is a \\((n^2 \\times J)\\) matrix whose columns are \\(vec(\\Sigma_j)\\), \\(j \\in \\{1,\\dots,J\\}\\), the \\(\\Sigma_j\\) being \\((n \\times n)\\) positive semi-definite, a standardized \\(J\\)-dimensional VARG process (multivariate extension of Example 1.8). Example 1.14 (Regime-switching VAR(1)) One can also use this approach to construct (affine) regime-switching VAR processes (which is another extension of Example 1.6 (see, e.g., Christian Gourieroux et al. (2014)). For that, replace \\(\\delta\\) with \\(\\left( \\begin{array}{ll} \\Lambda_1 &amp; 0 \\\\ 0 &amp; \\Lambda_2 \\end{array} \\right)\\)\\(\\left( \\begin{array}{l} z_{1,t+1} \\\\ z_{2,t+1} \\end{array} \\right)\\), where \\(\\Lambda_1\\) is a \\((n \\times J_1)\\) matrix and \\(z_{1,t+1}\\) is a Markov chain valued in the set of selection vectors of size \\(J_1\\) (see Subsection 1.6), \\(\\Lambda_2\\) is the same matrix as in Example 1.13 and \\(z_{2,t+1}\\) is a Markov chain valued in the set of selection vectors of size \\(J_2\\). or \\(\\left( \\begin{array}{l} \\Lambda_1 \\\\ \\Lambda_2 \\end{array}\\right)z_{t+1}\\), where \\(\\Lambda_1\\) and\\(\\Lambda_2\\) are the same matrices as above with \\(J_1=J_2=J\\), and \\(z_{t+1}\\) is a Markov chain valued in the set of selection vectors of size \\(J\\). 1.8.4 Extended affine processes Some processes are not affine, but may be sub-components of an affine process. This can be useful to compute their conditional moments and multi-horizon Laplace transform (as one can use the formulas presented above for that, using the enlarged—affine—vector). Let us formally define an extended affine process: Definition 1.5 (Extended Affine Processes) A process \\(w_{1,t}\\) is extended affine if there exists a process \\(w_{2,t} = g(\\underline{w_{1,t}})\\) such that \\((w&#39;_{1,t}, w&#39;_{2,t})&#39;\\) is affine (of order 1). For an extended affine processes, \\(\\varphi_{1,t}(u) = \\mathbb{E}[\\exp(u&#39;w_{1,t+1})|\\underline{w_{1,t}}]\\) can be obtained from: \\[\\begin{eqnarray*} \\varphi_t(u_1, u_2) &amp;=&amp; \\mathbb{E}[\\exp(u&#39;_1w_{1,t+1}+u&#39;_2 w_{2,t+1)}|\\underline{w_{1,t}}, \\underline{w_{2,t}}] \\\\ &amp;=&amp; \\exp[a&#39;_1(u_1,u_2)w_{1,t} + a&#39;_2(u_1,u_2)w_{2,t}+b(u_1,u_2)] \\end{eqnarray*}\\] by: \\[ \\varphi_{1,t}(u) = \\varphi_t(u, 0) = \\exp[a&#39;_1(u,0)w_{1,t}+a&#39;_2(u,0)g(\\underline{w_{1,t}}) + b(u, 0)]. \\] In particular \\(w_{1,t}\\) may be non-Markovian. Similarly the multi-horizon Laplace transform (see Section 1.9) \\[ \\mathbb{E}[\\exp(\\gamma&#39;_{1}w_{1,t+1}+\\dots+\\gamma&#39;_{h}w_{1,t+h})|\\underline{w_{1,t}}] \\] can be obtained from the knowledge of the extended multi-horizon Laplace transform: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}_t[\\exp(\\{\\gamma&#39;_{1,1}w_{1,t+1}+\\gamma&#39;_{2,1}w_{2,t+1}\\}+\\dots+ \\{\\gamma&#39;_{1,h}w_{1,t+h}+\\gamma&#39;_{2,h}w_{2,t+h}\\}] \\\\ &amp;=&amp; \\exp[A&#39;_{1,t,h}(\\gamma^h_1, \\gamma^h_2)w_{1,t}+A&#39;_{2,t,h}(\\gamma^h_1, \\gamma^h_2)w_{2,t}+B_{t,h}(\\gamma^h_1, \\gamma^h_2)], \\end{eqnarray*}\\] (with \\(\\gamma^h_1 = (\\gamma&#39;_{1,1},\\dots, \\gamma&#39;_{1,h})&#39;\\), and \\(\\gamma^h_2 = (\\gamma&#39;_{2,1},\\dots, \\gamma&#39;_{2,h})&#39;\\)). We indeed have: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(\\gamma&#39;_{1}w_{1,t+1}+\\dots+\\gamma&#39;_{h}w_{1,t+h})|\\underline{w_{1,t}}]\\\\ &amp;=&amp; \\exp[A&#39;_{1,t,h}(\\gamma^h,0) w_{1,t} + A&#39;_{2,t,h}(\\gamma^h,0)g (\\underline{w_{1,t}}) + B_{t,h}(\\gamma^h,0)], \\end{eqnarray*}\\] with \\(\\gamma^h = (\\gamma_1&#39;,\\dots,\\gamma_h&#39;)&#39;\\). Example 1.15 (Affine process of order p) If \\(\\{w_{1,t}\\}\\) is affine or order \\(p&gt;1\\), then \\((w_{1,t},\\dots,w_{1,t-p+1})\\) is affine of order 1, but \\(\\{w_{1,t}\\}\\) is not affine. That is, in that case, \\(w_{2,t} = (w&#39;_{1,t-1}, \\dots.w&#39;_{1,t-p+1})&#39;\\). This a kind of extreme case since \\(w_{2,t}\\) belongs to the information at \\(t-1\\), which implies \\(a_2(u_1, u_2) = u_2\\). Example 1.16 (Gaussian ARMA process) Consider an \\(ARMA(1,1)\\) process \\[ w_{1,t} - \\varphi w_{1,t-1} = \\varepsilon_t-\\theta \\varepsilon_{t-1}, \\] with \\(|\\varphi | &lt; 1\\), \\(|\\theta| &lt; 1\\), and \\(\\varepsilon_t \\sim i.i.d. \\mathcal{N}(0, \\sigma^2)\\). \\(w_{1,t}\\) is not Markovian. Now, take \\(w_{2,t} = \\varepsilon_t = (1-\\theta L)^{-1}(1-\\varphi L)w_{1,t}\\). We have: \\[ \\left( \\begin{array}{l} w_{1,t+1} \\\\ w_{2,t+1} \\end{array} \\right) = \\left( \\begin{array}{ll} \\varphi &amp; -\\theta \\\\ 0 &amp; 0 \\end{array} \\right) \\left( \\begin{array}{l} w_{1,t} \\\\ w_{2,t} \\end{array} \\right) + \\left( \\begin{array}{l} 1 \\\\ 1 \\end{array} \\right) \\varepsilon_{t+1}. \\] Hence \\((w_{1,t}, w_{2,t})&#39;\\) is Gaussian \\(VAR(1)\\), and, therefore, it is affine of order 1. This is easily extended to \\(ARMA(p,q)\\) and \\(VARMA(p,q)\\) processes. Example 1.17 (GARCH type process) Consider process \\(w_{1,t}\\), defined by: \\[ w_{1,t+1} = \\mu + \\varphi w_{1,t} + \\sigma_{t+1} \\varepsilon_{t+1}, \\] where \\(|\\varphi| &lt; 1\\) and \\(\\varepsilon_t \\sim i.i.d. \\mathcal{N}(0,1)\\), and \\[ \\sigma^2_{t+1} = \\omega + \\alpha \\varepsilon^2_t + \\beta \\sigma^2_t, \\] where \\(0 &lt; \\beta &lt; 1\\). Consider \\(w_{2,t} = \\sigma^2_{t+1}\\) (which is a non-linear function of \\(\\underline{w_{1,t}}\\)). Proposition 1.7 shows that: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}\\left[\\exp(u_1 w_{1,t+1} + u_2 w_{2,t+1})|\\underline{w_{1,t}}\\right] \\\\ &amp;=&amp; \\exp\\left[u_1 \\mu + u_2 \\omega - \\frac{1}{2} \\log(1-2 u_2 \\alpha) \\right. \\\\ &amp;&amp;\\left. + u_1 \\varphi w_{1,t} + (u_2\\beta + \\frac{u^2_1}{2(1-2u_2\\alpha)}) w_{2,t}\\right], \\end{eqnarray*}\\] which is exponential affine in \\((w_{1,t}, w_{2,t})\\). 1.9 Multi-horizon Laplace transform 1.9.1 Recursive computation and direct pricing implications In this subsection, we show that multi-horizon Laplace transforms of affine processes can be calculated recursively. Various examples will show how this can be exploited to price long-dated financial instruments. Let us consider a multivariate process \\(w_{t}\\), affine of order one. (As explained in Subsection 1.5.2, this includes the case of the order \\(p\\) case.) For the sake of generality, we consider the case where functions \\(a(.)\\), \\(b(.)\\) are possibly deterministic functions of time, denoted in this case \\(a_{t+1}(.)\\) and \\(b_{t+1}(.)\\): \\[ \\mathbb{E}_t \\exp[(u&#39;w_{t+1})] = \\exp[a&#39;_{t+1}(u)w_t+b_{t+1}(u)]. \\] The multi-horizon Laplace transform of associated with date \\(t\\) and horizon \\(h\\) is defined by: \\[\\begin{equation} \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\mathbb{E}_t[\\exp(\\gamma&#39;_1w_{t+1}+\\dots+\\gamma&#39;_h w_{t+h})].\\tag{1.8} \\end{equation}\\] Lemma 1.5 (in the appendix) shows that we have: \\[ \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\exp(A&#39;_{t,h} w_t + B_{t,h}), \\] where \\(A_{t,h} = A^h_{t,h}\\) and \\(B_{t,h} = B^h_{t,h}\\), the \\(A^h_{t,i}, B^h_{t,i}\\) \\(i = 1,\\dots,h\\), being given recursively by: \\[ \\left\\{ \\begin{array}{ccl} A^h_{t,i} &amp;=&amp; a_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}), \\\\ B^h_{t,i} &amp;=&amp; b_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}) + B^h_{t,i-1}, \\\\ A^h_{t,0} &amp;=&amp; 0, B^h_{t,0} = 0. \\end{array} \\right. \\] If the functions \\(a_{t}\\) and \\(b_{t}\\) do not depend on \\(t\\), these recursive formulas do not depend on \\(t\\), and we get \\(\\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h)\\), for any \\(t\\), with only one recursion for each \\(h\\). Moreover, if the functions \\(a_{t}\\) and \\(b_{t}\\) do not depend on \\(t\\), and if different sequences \\((\\gamma^h_1,\\dots,\\gamma^h_h), h=1,\\dots,H\\) (say) satisfy \\(\\gamma^h_{h+1-i} = u_i\\), for \\(i=1,\\dots,h\\), and for any \\(h \\leq H\\), that is if we want to compute (reverse-order case): \\[\\begin{equation} \\varphi_{t,h}(u_h,\\dots,u_1)=\\mathbb{E}_t[\\exp(u&#39;_{{\\color{red}h}} w_{{\\color{red}t+1}}+\\dots+u&#39;_{{\\color{red}1}} w_{{\\color{red}t+h}})], \\quad h=1,\\dots,H,\\tag{1.9} \\end{equation}\\] then Proposition 1.5 (in the appendix) shows that we can compute the \\(\\varphi_{t,h}(u_h,\\dots,u_1)\\) for any \\(t\\) and any \\(h \\leq H\\) with only one recursion. That is \\(\\varphi_{t,h}(u_h,\\dots,u_1)=\\exp(A&#39;_hw_t+B_h)\\) with: \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} A_{h} &amp;=&amp; a(u_{h} + A_{h-1}), \\\\ B_{h} &amp;=&amp; b(u_{h} + A_{h-1}) + B_{h-1}, \\\\ A_{0} &amp;=&amp; 0,\\quad B_{0} = 0. \\end{array} \\right. \\end{equation*}\\] As mentioned above, what precedes has useful implications to price long-dated financial instruments such as nominal and real bonds (Examples 1.18 and 1.20, respectively), or futures (Example 1.21). Example 1.18 (Nominal interest rates) Let \\(B(t,h)\\) denote the date-\\(t\\) price of a nominal zero-coupon bond of maturity \\(h\\). We Have: \\[\\begin{equation} B(t,h) = \\mathbb{E}^{\\mathbb{Q}}_t exp (-r_{t}-\\dots-r_{t+h-1}),\\tag{1.10} \\end{equation}\\] where \\(r_{t}\\) is the nominal short rate between \\(t\\) and \\(t+1\\) (observed at \\(t\\)), and the associated (continuously-compounded) yield-to-maturity is given by: \\[\\begin{equation} R(t,h) = - \\frac{1}{h} \\log B(t,h), \\quad h=1,\\dots,H. \\end{equation}\\] If \\(r_t = \\omega&#39;w_t\\) (say), then: \\[ B(t,h) = \\exp(-r_{t}) \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-\\omega&#39; w_{t+1} - \\dots - \\omega&#39; w_{t+h-1}). \\] One can then price this bond by directly employing Eq. (1.9), with \\(u_1 = 0\\) and \\(u_i = - \\omega\\), \\(i = 2,\\dots, H\\). The price \\(B(t,h)\\) is exponential affine in \\(w_t\\), the associated yield-to-maturity \\(R(t,h)=-1/h\\log B(t,h)\\) is affine in \\(w_t\\). Example 1.19 (No-arbitrage Nelson-Siegel model) In this example, we employ the results of Example 1.18 in the context described by Christensen, Diebold, and Rudebusch (2009). Specifically, we consider a three factor model following a Gaussian VAR (see Example 1.6): \\[ w_t = \\left[\\begin{array}{c}X_{1,t}\\\\X_{2,t}\\\\X_{3,t}\\end{array}\\right] = \\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0\\\\ 0&amp;1-\\lambda&amp;\\lambda\\\\ 0&amp;0&amp;1-\\lambda\\end{array}\\right] \\left[\\begin{array}{c}X_{1,t-1}\\\\X_{2,t-1}\\\\X_{2,t-1}\\end{array}\\right] + \\left[\\begin{array}{ccc} \\sigma_{11} &amp; 0 &amp; 0\\\\ \\sigma_{21}&amp;\\sigma_{22}&amp;0\\\\ \\sigma_{31}&amp;\\sigma_{32}&amp;\\sigma_{33}\\end{array}\\right] \\left[\\begin{array}{c}\\varepsilon_{1,t}\\\\\\varepsilon_{2,t}\\\\\\varepsilon_{3,t}\\end{array}\\right], \\] where \\(\\left[\\begin{array}{c}\\varepsilon_{1,t}\\\\\\varepsilon_{2,t}\\\\\\varepsilon_{3,t}\\end{array}\\right] \\sim \\,i.i.d.\\, \\mathcal{N}(0,Id)\\). The nominal short-term rate is given by \\(r_t = X_{1,t}+X_{2,t}\\). In that case, we can use the results of Example 1.18 with \\(\\omega = (-1,-1,0)&#39;\\). The following lines of code do that: library(TSModels) lambda &lt;- .05 Phi &lt;- diag(c(1,1-lambda,1-lambda));Phi[2,3] &lt;- lambda Sigma &lt;- .0005 * diag(3) psi.parameterization=list(mu=matrix(0,3,1),Phi=Phi,Sigma=Sigma) u1 &lt;- matrix(0,3,1) u2 &lt;- matrix(c(-1,-1,0),ncol=1) H &lt;- 20 AB &lt;- reverse.MHLT(psi.GaussianVAR,u1 = u1,u2 = u2,H = H, psi.parameterization = psi.parameterization) AB$A[1:2,,] &lt;- AB$A[1:2,,] - 1 # add terms corresponding to exp(-r_t) a.yield &lt;- - AB$A / array((1:H) %x% rep(1,3),c(3,1,H)) b.yield &lt;- - AB$B / array((1:H) %x% rep(1,3),c(1,1,H)) plot(a.yield[1,,],type=&quot;l&quot;,lwd=2,ylim=c(0,1), xlab=&quot;Maturity&quot;,ylab=&quot;Factor loadings&quot;) lines(a.yield[2,,],col=&quot;red&quot;,lwd=2,lty=2) lines(a.yield[3,,],col=&quot;blue&quot;,lwd=2,lty=3) Figure 1.4: Factor loadings in the context of a no-arbitrage nelson-Siegel model (Christensen, Diebold and Rudebusch, 2009). The first factor (black solid line) is a level factor. The second and third factors (red dashed line and blue dotted line, respectively) are slope factors. In the previous example, note the use of function reverse.MHLT (in package TSModels), that notably takes a L.T. as an argument (psi). In the previous example, we consider a Gaussian VAR, and we therefore assign psi.GaussianVAR to psi. We then need to provide function reverse.MHLT with the arguments of the psi function. These arguments are provided in the form of a list (input psi.parameterization). Example 1.20 (Real interest rates) Denote by \\(q_t\\) the price index on date \\(t\\) and by \\(\\pi_{t+1} = \\log \\dfrac{q_{t+1}}{q_t}\\) the inflation rate on date \\(t+1\\). We have: \\[\\begin{eqnarray*} \\bar{R}(t,h) &amp; =&amp; - \\frac{1}{h} \\log \\bar{B}(t,h), \\quad h=1,\\dots,H \\\\ \\\\ \\bar{B}(t,h) &amp; =&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t}-\\dots-r_{t+h-1} + \\pi_{t+1}+\\dots+\\pi_{t+h}), \\\\ \\\\ &amp; =&amp; \\exp(-r_{t}) \\times \\\\ &amp;&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(-r_{t+1}-\\dots-r_{t+h-1}+\\pi_{t+1}+\\dots+\\pi_{t+h}) \\end{eqnarray*}\\] If \\(r_t = \\omega&#39;w_t\\) and \\(\\pi_t = \\bar\\omega&#39;w_t\\), then \\(\\bar{B}(t,h)\\) is given by: \\[\\begin{eqnarray*} \\exp(-r_{t}) \\mathbb{E}^{\\mathbb{Q}}_t exp[(\\bar\\omega-\\omega)&#39;w_{t+1}+\\dots+(\\bar\\omega-\\omega)&#39;w_{t+h-1}+\\bar\\omega&#39; w_{t+h}] \\end{eqnarray*}\\] One can then price this bond by directly employing Eq. (1.9), with \\(u_1 = \\bar\\omega\\) and \\(u_i = \\bar\\omega-\\omega\\), \\(i = 2,\\dots, H\\). Example 1.21 (Futures) Denote by \\(F(t,h)\\) the date-\\(t\\) price of a future of maturity \\(h\\) (see Section XXX). That is \\(F(t,h) = \\mathbb{E}^{\\mathbb{Q}}_t (S_{t+h})\\), \\(h=1,\\dots,H\\), where \\(S_t\\) is the date-\\(t\\) price of the underlying asset. If \\(w_t = (\\log S_t, x&#39;_t)&#39;\\) then \\(F(t,h) = \\mathbb{E}^{\\mathbb{Q}}_t \\exp(e&#39;_1 w_{t+h})\\). This can be calculated by using Eq. (1.9) with \\(u_1 = e_1\\), and \\(u_i = 0\\), for \\(i=2,\\dots,H\\). If \\(w_t = (y_t, x&#39;_t)&#39;\\) with \\(y_t = \\log\\frac{S_t}{S_{t-1}}\\), then \\(F(t,h) = S_t \\mathbb{E}^{\\mathbb{Q}}_t \\exp(e&#39;_1 w_{t+1}+\\dots+e&#39;_1 w_{t+h})\\). This can be calculated by using Eq. (1.9) with \\(u_i = e&#39;_1\\), \\(i=1,\\dots,H\\). 1.9.2 Exponential payoff Consider an asset providing the payoff \\(\\exp(\\nu&#39; w_{t+h})\\) on date \\(t+h\\). Its price is given by: \\[ P(t,h;\\nu) = \\mathbb{E}^{\\mathbb{Q}}_t[\\exp(-r_{t}-\\dots-r_{t+h-1}) \\exp(\\nu&#39; w_{t+h})]. \\] If \\(r_t = \\omega&#39;w_t\\), we have: \\[ P(t,h;\\nu) = \\exp(-r_{t})\\mathbb{E}^{\\mathbb{Q}}_t \\left(exp[-\\omega&#39; w_{t+1}-\\dots-\\omega&#39; w_{t+h-1}+ \\nu&#39; w_{t+h}]\\right), \\] which can be calculated by Eq. (1.9), with \\(u_1 = \\nu\\) and \\(u_i = -\\omega\\) for \\(i = 2,\\dots,H\\). What precedes can be extended to the case where the payoff (settled on date \\(t+h\\)) is of the form: \\[ (\\nu_1&#39;w_{t+h}) \\exp(\\nu_2&#39; w_{t+h}). \\] Indeed, we have \\[ \\left[\\frac{\\partial \\exp[(s \\nu_1+ \\nu_2)&#39;w_{t+h}]}{\\partial s}\\right]_{s=0} = (\\nu_1&#39;w_{t+h}) \\exp(\\nu_2&#39; w_{t+h}). \\] Therefore: \\[\\begin{eqnarray} &amp;&amp;\\mathbb{E}_t^{\\mathbb{Q}}[\\exp(-r_t - \\dots - r_{t+h-1})(\\nu_1&#39;w_{t+h}) \\exp(\\nu_2&#39; w_{t+h})] \\nonumber\\\\ &amp;=&amp; \\left[ \\frac{\\partial P(t,h;s \\nu_1 + \\nu_2)}{\\partial s} \\right]_{s=0}.\\tag{1.11} \\end{eqnarray}\\] This method is easily extended to price payoffs of the form \\((\\nu_1&#39;w_{t+h})^k \\exp(\\nu_2&#39; w_{t+h})\\), with \\(k \\in \\mathbb{N}\\). 1.10 VAR representation and conditional moments An important property of affine processes is that their dynamics can be written as a vector-autoregressive process. This is useful to compute conditional moments of the process. Proposition 1.1 (VAR representation of an affine process' dynamics) If \\(w_t\\) is the affine process whose Laplace transform is defined in Def. 1.1, then its dynamics admits the following vectorial autoregressive representation: \\[\\begin{equation} w_{t+1} = \\mu + \\Phi w_{t} + \\Sigma^{\\frac{1}{2}}(w_t) \\varepsilon_{t+1},\\tag{1.12} \\end{equation}\\] where \\(\\varepsilon_{t+1}\\) is a difference of martingale sequence whose conditional covariance matrix is the identity matrix and where \\(\\mu\\), \\(\\Phi\\) and \\(\\Sigma(w_t) = \\Sigma^{\\frac{1}{2}}(w_t){\\Sigma^{\\frac{1}{2}}(w_t)}&#39;\\) satisfy: \\[\\begin{equation} \\mu = \\left[\\frac{\\partial }{\\partial u}b(u)\\right]_{u=0}, \\quad \\Phi= \\left[\\frac{\\partial }{\\partial u}a(u)&#39;\\right]_{u=0}\\tag{1.13} \\end{equation}\\] \\[\\begin{equation} \\Sigma(w_t) = \\left[\\frac{\\partial }{\\partial u\\partial u&#39;}b(u)\\right]_{u=0} + \\left[\\frac{\\partial }{\\partial u\\partial u&#39;}a(u)&#39;w_t\\right]_{u=0}.\\tag{1.14} \\end{equation}\\] Proof. When \\(w_t\\) is affine, its (conditional) cumulant generating function is of the form \\(\\psi(u)=a(u)&#39;w_t+b(u)\\). The result directly follows from the formulas given in Section 1.3. Proposition 1.6 (in the appendix) shows that the conditional means and variances of \\(w_t\\) are given by: \\[\\begin{eqnarray} \\mathbb{E}_t(w_{t+h}) &amp;=&amp; (I - \\Phi)^{-1}(I - \\Phi^h)\\mu + \\Phi^h w_t \\tag{1.15}\\\\ \\mathbb{V}ar_t(w_{t+h}) &amp;=&amp; \\Sigma(\\mathbb{E}_t(w_{t+h-1}))+\\Phi \\Sigma(\\mathbb{E}_t(w_{t+h-2}))\\Phi&#39; + \\nonumber \\\\ &amp;&amp; \\dots + \\Phi^{h-1} \\Sigma(w_{t}){\\Phi^{h-1}}&#39;. \\tag{1.16} \\end{eqnarray}\\] Eq. (1.16) notably implies that \\(\\mathbb{V}ar_t(w_{t+h})\\) is an affine function of \\(w_t\\). Indeed \\(\\Sigma(.)\\) is an affine function, and the conditional expectations \\(\\mathbb{E}_t(w_{t+h})\\) are affine in \\(w_t\\), as shown by Eq. (1.15). The unconditional means and variances are given by: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} \\mathbb{E}(w_t) &amp;=&amp; (I - \\Phi)^{-1}\\mu\\\\ vec[\\mathbb{V}ar(w_t)] &amp;=&amp; (I_{n^2} - \\Phi \\otimes \\Phi)^{-1} vec\\left(\\Sigma[(I - \\Phi)^{-1}\\mu]\\right). \\end{array} \\right.\\tag{1.17} \\end{equation}\\] 1.11 Truncated Laplace transforms of affine processes In this section, we show how one can employ Fourier transforms to compute truncated conditional moments of affine processes. For that, let us introduce the following notation: \\[ w_{t+1,T} = (w&#39;_{t+1}, w&#39;_{t+2},\\dots, w&#39;_T)&#39; \\] with \\(w_t\\) affine \\(n\\)-dimensional process. We want to compute: \\[ \\tilde{\\varphi}_t(u ; v, \\gamma) = \\mathbb{E}_t[\\exp(u&#39;w_{t+1,T})\\textbf{1}_{\\{v&#39;w_{t+1,T}&lt;\\gamma\\}}]. \\] Consider the complex untruncated conditional Laplace transform: \\[ \\varphi_t(z) = \\mathbb{E}_t[\\exp(z&#39;w_{t+1,T})],\\quad z \\in \\mathbb{C}^{nT}, \\] computed using the same recursive algorithm as in the real case (see Section 1.9). Darrell Duffie, Pan, and Singleton (2000) have shown that we have (see also Proposition 1.8 in the appendix): \\[\\begin{equation} \\tilde{\\varphi}_t(u ; v, \\gamma) = \\frac{\\varphi_t(u)}{2} - \\frac{1}{\\pi} \\int^\\infty_0 \\frac{Im[\\varphi_t(u+ivx) \\exp(-i\\gamma x)]}{x} dx.\\tag{1.18} \\end{equation}\\] where \\(Im\\) means imaginary part. Note that the integral in Eq. (1.18) is one dimensional (whatever the dimension of \\(w_t\\)). As shown in the following example, this can be exploited to price options. Example 1.22 (Option pricing) Pricing calls and puts amounts to conditional expectations of the type (with \\(k &gt; 0\\)): \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}_t\\left([\\exp(u&#39;_1 w_{t+1,T})-k \\exp(u&#39;_2 w_{t+1,T})]^+\\right) \\\\ &amp;= &amp; \\mathbb{E}_t\\left([\\exp(u&#39;_1 w_{t+1,T})-k \\exp(u&#39;_2 w_{t+1,T})]\\textbf{1}_{\\{[\\exp(u_1-u_2)&#39;w_{t+1,T}] &gt; k \\}}\\right) \\\\ &amp;= &amp; \\tilde{\\varphi}_t(u_1 ; u_2-u_1, - \\log k) - k \\tilde{\\varphi}_t(u_2 ; u_2-u_1, - \\log k). \\end{eqnarray*}\\] Example 1.23 (Exogenous short rate) Consider an asset whose date-\\(t\\) price is \\(p_t\\). Denote its geometric asset return by \\(y_t\\), i.e., \\(y_t = \\log(p_t/p_{t-1})\\). Consider an option written on this asset, with a strike equal \\(k p_t\\). If interest rates are deterministic, the option price, for a maturity \\(h\\), is given by: \\[ p_t \\exp(-r_{t}-\\dots-r_{t+h-1}) \\mathbb{E}^{\\mathbb{Q}}_t[\\exp u&#39;_1 w_{t+1, t+h} - k]^+ \\] with \\(u_1 = e \\otimes e_1\\), where \\(e\\) is the \\(h\\)-dimensional vector with components equal to 1, and \\(e_1\\) is the \\(n\\)-vector selecting the 1st component (\\(y_t\\) being the 1st component of \\(w_t\\), say). Example 1.24 (Endogenous short rate) Consider the same context as in Example 1.23, but with a stochastic (endogenous) short-term rate. For instance, assume that \\(r_{t+1} = \\omega_0 + \\omega&#39;_1 w_t\\). The option price then is: \\[\\begin{eqnarray*} &amp;&amp; p_t \\mathbb{E}^{\\mathbb{Q}}_t \\left[ \\exp(-\\omega_0 - \\omega&#39;_1 w_t-\\dots- \\omega_0 - \\omega&#39;_1 w_{t+h-1}) [\\exp(u&#39;_1 w_{t+1,t+h})-k]^+ \\right]\\\\ &amp;= &amp; p_t \\exp(-h \\omega_0 - \\omega&#39;_1 w_t)\\mathbb{E}^{\\mathbb{Q}}_t\\left(\\left[\\exp(\\tilde{u}&#39;_1w_{t+1,t+h})-k \\exp(u_2 w_{t+1, t+h})\\right]^+\\right), \\end{eqnarray*}\\] with \\(\\tilde{u}&#39;_1 = u_1 + u_2\\), [\\(u_1 = e \\otimes e_1\\) as before], and \\(u_2 = (-\\omega&#39;_1,\\dots, -\\omega&#39;_1, 0)&#39;\\). Example 1.25 (Numerical example: Conditional cumulated distribution function (c.d.f.)) Let us use the model used in Example 1.19. Suppose we want to compute the conditional distribution of the average interest rate over the next \\(H\\) periods, i.e., \\(\\frac{1}{H}(r_{t+1}+\\dots+r_{t+H})\\). Hence, we want to compute \\(\\mathbb{E}_t[\\textbf{1}_{\\{v&#39;w_{t+1,T}&lt;\\gamma\\}}]\\) with \\(v&#39;w_{t+1,T}=\\frac{1}{H}(r_{t+1}+\\dots+r_{t+H})\\). H &lt;- 10 X &lt;- matrix(c(0.01,.02,0),3,1) x &lt;- exp(seq(-10,10,length.out=1000)) u1 &lt;- matrix(c(1/H,1/H,0),3,1) %*% matrix(1i*x,nrow=1);u2 &lt;- u1 AB &lt;- reverse.MHLT(psi.GaussianVAR,u1 = u1,u2 = u2,H = H, psi.parameterization = psi.parameterization) s1 &lt;- matrix(exp(t(X) %*% AB$A[,,H] + AB$B[,,H]),ncol=1) dx &lt;- matrix(x-c(0,x[1:length(x)-1]),length(x),1) gamma &lt;- seq(-.2,.3,length.out=1000) fx &lt;- outer(x,gamma,function(r,c){Im(s1[,1]*exp(-1i*r*c))/r})*dx[,1] f &lt;- 1/2 - 1/pi * apply(fx,2,sum) plot(gamma,f,type=&quot;l&quot;,xlab=&quot;&quot;,lwd=2) Figure 1.5: Conditional cumulated distribution function (c.d.f.) of \\(\\frac{1}{H}(r_{t+1}+\\dots+r_{t+H})\\). 1.12 Appendices Lemma 1.3 If \\(\\mu \\in \\mathbb{R}^L\\) and \\(Q\\) is a \\((L \\times L)\\) matrix symmetric positive definite, then: \\[ \\int_{\\mathbb{R}^{L}} \\exp(-u&#39;Q u + \\mu&#39;u)du = \\frac{\\pi^{L/2}}{(det Q)^{1/2}} exp \\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right). \\] Proof. The integral is: \\[\\begin{eqnarray*} &amp;&amp; \\int_{\\mathbb{R}^{L}} exp \\left[ \\begin{array}{l} - (u - \\frac{1}{2} Q^{-1} \\mu)&#39; Q (u - \\frac{1}{2} Q^{-1} \\mu)&#39; \\end{array} \\right] exp\\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right)du \\\\ &amp;=&amp; \\frac{\\pi^{L/2}}{(det Q)^{1/2}} exp\\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right) \\end{eqnarray*}\\] [using the formula for the unit mass of \\(\\mathcal{N}( 0.5Q^{-1}\\mu,(2Q)^{-1})\\)]. Lemma 1.4 If \\(\\varepsilon_{t+1} \\sim \\mathcal{N}(0,Id)\\), we have \\[ \\mathbb{E}_t \\left(\\exp[\\lambda&#39;\\varepsilon_{t+1}+\\varepsilon&#39;_{t+1} V \\varepsilon_{t+1}]\\right) = \\frac{1}{[\\det(I-2V)]^{1/2}} \\exp\\left[ \\frac{1}{2} \\lambda&#39;(I-2V)^{-1}\\lambda \\right]. \\] Proof. We have \\[ \\mathbb{E}_t \\exp(\\lambda&#39;\\varepsilon_{t+1}+\\varepsilon&#39;_{t+1}V\\varepsilon_{t+1}) = \\frac{1}{(2\\pi)^{n/2}} \\int_{\\mathbb{R}^{n}} \\exp\\left[ \\begin{array}{l} -u&#39;\\left( \\begin{array}{l} \\frac{1}{2} I-V \\end{array} \\right)u+\\lambda&#39;u \\end{array} \\right]du \\] From Lemma 1.3, if \\(u\\in\\mathbb{R}^n\\), then \\[ \\int_{\\mathbb{R}^{n}} \\exp(-u&#39; Q u+\\mu&#39;u) du = \\frac{\\pi^{n/2}}{(\\det Q)^{1/2}} \\exp\\left( \\begin{array}{l} \\frac{1}{4} \\mu&#39;Q^{-1}\\mu \\end{array} \\right). \\] Therefore: \\[ \\begin{array}{l} \\mathbb{E}_t \\exp(\\lambda&#39;\\varepsilon_{t+1}+\\varepsilon&#39;_{t+1}V\\varepsilon_{t+1}) \\\\ = \\frac{1}{2^{n/2} \\left[ \\begin{array}{l} \\det \\left( \\begin{array}{l} \\frac{1}{2} I-V \\end{array} \\right) \\end{array} \\right]^{1/2} } \\exp\\left[ \\begin{array}{l} \\frac{1}{4} \\lambda&#39;\\left( \\begin{array}{l} \\frac{1}{2} I-V \\end{array} \\right)^{-1}\\lambda \\end{array} \\right]. \\end{array} \\] Proposition 1.2 (Quadratic Gaussian process) Consider vector \\(w_t = (x&#39;_t,vec(x_t x_t&#39;)&#39;)&#39;\\), where \\(x_t\\) is a \\(n\\)-dimensional vector following a Gaussian VAR(1), i.e. \\[ x_{t+1}|\\underline{w_t} \\sim \\mathcal{N}(\\mu+\\Phi x_t, \\Sigma). \\] If \\(u = (v,V)\\) where \\(v \\in \\mathbb{R}^n\\) and \\(V\\) a square symmetric matrix of size \\(n\\), we have: \\[\\begin{eqnarray*} \\varphi_t(u) &amp;=&amp; \\mathbb{E}_t\\big\\{\\exp\\big[(v&#39;,vec(V)&#39;)\\times w_{t+1}\\big]\\big\\} \\\\ &amp; =&amp; \\exp \\left\\{a_1(v,V)&#39;x_t +vec(a_2(v,V))&#39; vec(x_t&#39;x_t) + b(v,V) \\right\\}, \\end{eqnarray*}\\] where: \\[\\begin{eqnarray*} a_2(u) &amp; = &amp; \\Phi&#39;V (I_n - 2\\Sigma V)^{-1} \\Phi \\nonumber \\\\ a_1(u) &amp; = &amp; \\Phi&#39;\\left[(I_n-2V\\Sigma)^{-1}(v+2V\\mu)\\right] \\nonumber \\\\ b(u) &amp; = &amp; u&#39;(I_n - 2 \\Sigma V)^{-1}\\left(\\mu + \\frac{1}{2} \\Sigma v\\right) +\\\\ &amp;&amp; \\mu&#39;V(I_n - 2 \\Sigma V)^{-1}\\mu - \\frac{1}{2}\\log\\big|I_n - 2\\Sigma V\\big|.\\tag{1.3} \\end{eqnarray*}\\] Proof. We have: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}_t(\\exp(v&#39; x_{t+1} + vec(V)&#39;vec(x_{t+1} x_{t+1}&#39;))) \\\\ &amp;=&amp; \\mathbb{E}_t[\\exp(v&#39; (\\mu + \\Phi x_t + \\Sigma^{1/2}\\varepsilon_{t+1}) + \\\\ &amp;&amp; vec(V)&#39;vec((\\mu + \\Phi x_t + \\Sigma^{1/2}\\varepsilon_{t+1}) (\\mu + \\Phi x_t + \\Sigma^{1/2}\\varepsilon_{t+1})&#39;))] \\\\ &amp;=&amp; \\exp[v&#39; (\\mu + \\Phi x_t) + vec(V)&#39;vec\\{(\\mu + \\Phi x_t)(\\mu + \\Phi x_t)&#39;\\}] \\times \\\\ &amp;&amp; \\mathbb{E}_t[\\exp(v&#39;\\Sigma^{1/2}\\varepsilon_{t+1} +2\\underbrace{ vec(V)&#39; vec\\{(\\mu + \\Phi x_t)(\\varepsilon_{t+1}&#39;{\\Sigma^{1/2}}&#39;)\\}}_{=(\\mu + \\Phi x_t)&#39;V\\Sigma^{1/2}\\varepsilon_{t+1}} +\\\\ &amp;&amp; \\underbrace{vec(V)&#39;vec\\{(\\Sigma^{1/2}\\varepsilon_{t+1})(\\Sigma^{1/2}\\varepsilon_{t+1})&#39;}_{=\\varepsilon_{t+1}&#39;{\\Sigma^{1/2}}&#39;V\\Sigma^{1/2}\\varepsilon_{t+1}}\\}] \\end{eqnarray*}\\] Lemma 1.4 can be used to compute the previous conditional expectation, with \\(\\lambda = {\\Sigma^{1/2}}&#39;(v + 2 V&#39;(\\mu + \\Phi x_t))\\). Some algebra then leads to the result. Proposition 1.3 () Consider the following auto-regressive gamma process: \\[ \\frac{w_{t+1}}{\\mu} \\sim \\gamma(\\nu+z_t) \\quad \\mbox{where} \\quad z_t \\sim \\mathcal{P} \\left( \\frac{\\rho w_t}{\\mu} \\right), \\] with \\(\\nu\\), \\(\\mu\\), \\(\\rho &gt; 0\\). (Alternatively \\(z_t \\sim {\\mathcal{P}}(\\beta w_t)\\), with \\(\\rho = \\beta \\mu\\).) We have: \\(\\varphi_t(u) = exp \\left[ \\begin{array}{l} \\dfrac{\\rho u}{1-u \\mu} w_t - \\nu \\log(1-u \\mu)\\end{array} \\right], \\mbox{ for } u &lt; \\dfrac{1}{\\mu}\\). Proof. Given \\(\\underline{w_t}\\), we have \\(z_t \\sim {\\mathcal P}\\left( \\begin{array}{l} \\frac{\\rho w_t} {\\mu} \\end{array}\\right)\\). We have: \\[\\begin{eqnarray*} \\mathbb{E}[\\exp(u w_{t+1})|\\underline{w_t}] &amp;=&amp; \\mathbb{E}\\left\\{\\mathbb{E}\\left[\\exp \\left(u \\mu \\frac{w_{t+1}}{\\mu}\\right)|\\underline{w_t}, \\underline{z}_t\\right]\\underline{w_t}\\right\\}\\\\ &amp;=&amp; \\mathbb{E}[(1-u\\mu)^{-(\\nu+z_t)}|\\underline{w_t}] \\\\ &amp;=&amp; (1-u\\mu)^{-\\nu}\\mathbb{E}\\{\\exp[-z_t \\log(1-u\\mu)]|\\underline{w_t}\\} \\\\ &amp;=&amp; (1-u\\mu)^{-\\nu} \\exp \\left\\{\\frac{\\rho w_t}{\\mu}[\\exp(-\\log(1-u\\mu)] - \\frac{\\rho w_t}{\\mu}\\right\\}\\\\ &amp;=&amp; \\exp\\left[ \\begin{array}{l} \\frac{\\rho u w_t}{1-u\\mu} - \\nu \\log(1-u\\mu) \\end{array}\\right], \\end{eqnarray*}\\] using the fact that the L.T. of \\(\\gamma(\\nu)\\) is \\((1-u)^{-\\nu}\\) and that the L.T. of \\({\\mathcal P}(\\lambda)\\) is \\(\\exp[\\lambda(\\exp(u)-1)]\\). Proposition 1.4 (Dynamics of a WAR process) If \\(K\\) is an integer, \\(W_{t+1}\\) can be obtained from: \\[\\begin{eqnarray*} \\left\\{ \\begin{array}{ccl} W_{t+1} &amp; =&amp; \\sum^K_{k=1} x_{k,t+1} x&#39;_{k,t+1}\\\\ &amp;&amp;\\\\ x_{k,t+1} &amp; =&amp; M x_{k,t} + \\varepsilon_{k,t+1},\\quad k \\in \\{1,\\dots,K\\}, \\end{array} \\right. \\end{eqnarray*}\\] where \\(\\varepsilon_{k,t+1} \\sim i.i.d. \\mathcal{N}(0, \\Omega)\\) (independent across \\(k\\)’s). In particular, we have: \\[ \\mathbb{E}(W_{t+1}|\\underline{W_t}) = MW_tM&#39;+K \\Omega, \\] i.e. \\(W_t\\) follows a matrix weak AR(1) process. Proof. For \\(K=1\\), \\(W_{t+1}=x_{t+1} x&#39;_{t+1}\\), \\(x_{t+1} = M x_t + \\Omega^{1/2} u_{t+1}\\) and \\(u_{t+1} \\sim i.i.d. \\mathcal{N}(0,Id_L)\\). We have: \\[ \\mathbb{E}[\\exp(Tr \\Gamma W_{t+1})|\\underline{w_t}] = \\mathbb{E}\\{\\mathbb{E}[\\exp(Tr \\Gamma x_{t+1} x&#39;_{t+1})|\\underline{x}_t]|\\underline{w_t}\\} \\] and: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(Tr \\Gamma x_{t+1}x&#39;_{t+1})|\\underline{x}_t] = \\mathbb{E}[\\exp(x&#39;_{t+1}\\Gamma x_{t+1}|\\underline{x}_t] \\\\ &amp;=&amp; \\mathbb{E}[\\exp(M x_t + \\Omega^{1/2} u_{t+1})&#39;\\Gamma(M x_t + \\Omega^{1/2} u_{t+1})/x_t] \\\\ &amp;=&amp; \\exp(x&#39;_tM&#39;\\Gamma M x_t)\\mathbb{E}[\\exp(2 x&#39;_t M&#39;\\Gamma \\Omega^{1/2} u_{t+1}+u&#39;_{t+1}\\Omega^{1/2} \\Gamma \\Omega^{1/2} u_{t+1})/x_t] \\\\ &amp;=&amp; \\frac{exp(x&#39;_tM&#39;\\Gamma M x_t)}{(2\\pi)^{L/2}} \\times \\\\ &amp;&amp; \\int_{\\mathbb{R}^L} \\exp\\left[2x&#39;_tM&#39;\\Gamma \\Omega^{1/2}u_{t+1}-u&#39;_{t+1}\\left( \\frac{1}{2} Id_L-\\Omega^{1/2} \\Gamma \\Omega^{1/2}\\right)u_{t+1}\\right] du_{t+1}. \\end{eqnarray*}\\] Using Lemma 1.3 with \\(\\mu&#39; = 2 x&#39;_t M&#39;\\Gamma \\Omega^{1/2}, Q = \\frac{1}{2} Id_L-\\Omega^{1/2}\\Gamma\\Omega^{1/2}\\) \\ and after some algebra, the RHS becomes: \\[ \\frac{exp[x&#39;_tM&#39;\\Gamma(Id_L-2\\Omega\\Gamma)^{-1}M x_t]}{det[Id_L-2\\Omega^{1/2}\\Gamma\\Omega^{1/2}]} = \\frac{exp Tr[M&#39;\\Gamma(Id_L-2\\Omega^{-1}]M W_t]}{det[Id_L-2\\Omega \\Gamma]^{1/2}}, \\] which depends on \\(x_t\\) through \\(W_t\\), and gives the result for \\(K=1\\); the result for any \\(K\\) integer follows. Lemma 1.5 We have: \\[ \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\exp(A&#39;_{t,h} w_t + B_{t,h}), \\] where \\(A_{t,h} = A^h_{t,h}\\) and \\(B_{t,h} = B^h_{t,h}\\), the \\(A^h_{t,i}, B^h_{t,i}\\) \\(i = 1,\\dots,h\\), being given recursively by: \\[ (i) \\left\\{ \\begin{array}{ccl} A^h_{t,i} &amp;=&amp; a_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}), \\\\ B^h_{t,i} &amp;=&amp; b_{t+h+1-i}(\\gamma_{h+1-i} + A^h_{t,i-1}) + B^h_{t,i-1}, \\\\ A^h_{t,0} &amp;=&amp; 0, B^h_{t,0} = 0. \\end{array} \\right. \\] Proof. For any \\(j=1,\\dots,h\\) we have: \\[ \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) = \\mathbb{E}_t[\\exp(\\gamma&#39;_1 w_{t+1}+\\dots\\gamma&#39;_j w_{t+j}+A^{h&#39;}_{t,h-j}w_{t+j}+B^h_{t,h-j})] \\] where: \\[ (ii) \\left\\{ \\begin{array}{l} A^h_{t,h-j+1} = a_{t+j}(\\gamma_{j} + A^h_{t,h-j}), \\\\ B^h_{t,h-j+1} = b_{t+j}(\\gamma_{j} + A^h_{t,h-j}) + B^h_{t,h-j}, \\\\ A^h_{t,0} = 0, B^h_{t,0} = 0. \\end{array} \\right. \\] Since this is true for \\(j=h\\), and if this is true for \\(j\\), we get: \\[ \\begin{array}{ll} \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) &amp; = \\mathbb{E}_t [\\exp(\\gamma&#39;_1 w_{t+1}+\\dots+\\gamma&#39;_{j-1}w_{t+j-1}+a&#39;_{t+j}(\\gamma_j+A^h_{t,h-j})w_{t+j-1} \\\\ &amp; + b_{t+j}(\\gamma_j+A^h_{t,h-j})+B^h_{t,h-j}], \\end{array} \\] and, therefore, this is true for \\(j-1\\), with \\(A^h_{t,h-j+1}\\) and \\(B^h_{t,h-j+1}\\) given by formulas (ii) above. For \\(j=1\\) we get: \\[\\begin{eqnarray*} \\varphi_{t,h}(\\gamma_1,\\dots,\\gamma_h) &amp;=&amp; \\mathbb{E}_t \\exp(\\gamma&#39;_1 w_{t+1}+A^{h&#39;}_{t,h-1}w_{t+1}+B^h_{t,h-1}) \\\\ &amp;=&amp; \\exp(A&#39;_{t,h} w_t+B_{t,h}), \\end{eqnarray*}\\] Finally note that if we put \\(h-j+1 = i\\), formulas (ii) become (i). Proposition 1.5 (Reverse-order multi-horizon Laplace transform) If the functions \\(a_{t}\\) and \\(b_{t}\\) do not depend on \\(t\\), and if different sequences \\((\\gamma^h_1,\\dots,\\gamma^h_h), h=1,\\dots,H\\) (say) satisfy \\(\\gamma^h_{h+1-i} = u_i\\), for \\(i=1,\\dots,h\\), and for any \\(h \\leq H\\), that is if we want to compute (“reverse order” case): \\[ \\varphi_{t,h}(u_h,\\dots,u_1)=\\mathbb{E}_t[\\exp(u&#39;_{{\\color{red}h}} w_{{\\color{red}t+1}}+\\dots+u&#39;_{{\\color{red}1}} w_{{\\color{red}t+h}})], \\quad h=1,\\dots,H, \\] then we can compute the \\(\\varphi_{t,h}(u_h,\\dots,u_1)\\) for any \\(t\\) and any \\(h \\leq H\\), with only one recursion, i.e. \\(\\varphi_{t,h}(u_h,\\dots,u_1)=\\exp(A&#39;_hw_t+B_h)\\) with: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} A_{h} &amp;=&amp; a(u_{h} + A_{h-1}), \\\\ B_{h} &amp;=&amp; b(u_{h} + A_{h-1}) + B_{h-1}, \\\\ A_{0} &amp;=&amp; 0,\\quad B_{0} = 0. \\end{array} \\right.\\tag{1.19} \\end{equation}\\] Proof. According to Lemma 1.5, we have, in this case: \\[ \\left\\{ \\begin{array}{ccl} A^h_{i} &amp;=&amp; a(u_{i} + A^h_{i-1}), \\\\ B^h_{i} &amp;=&amp; b(u_{i} + A^h_{i-1}) + B^h_{i-1}, \\\\ A^h_{0} &amp;=&amp; 0, \\quad B^h_{0} = 0. \\end{array} \\right. \\] The previous sequences do not dependent on \\(h\\) and are given by Eq. (1.19). Proposition 1.6 (Conditional means and variances of an affine process) Consider an affine process \\(w_t\\). Using the notation of Proposition 1.1, we have: \\[\\begin{eqnarray} \\mathbb{E}_t(w_{t+h}) &amp;=&amp; (I - \\Phi)^{-1}(I - \\Phi^h)\\mu + \\Phi^h w_t \\tag{1.20}\\\\ \\mathbb{V}ar_t(w_{t+h}) &amp;=&amp; \\Sigma(\\mathbb{E}_t(w_{t+h-1}))+\\Phi \\Sigma(\\mathbb{E}_t(w_{t+h-2}))\\Phi&#39; + \\nonumber \\\\ &amp;&amp; \\dots + \\Phi^{h-1} \\Sigma(w_{t}){\\Phi^{h-1}}&#39;. \\tag{1.21} \\end{eqnarray}\\] Eq. (1.21) notably shows that \\(\\mathbb{V}ar_t(w_{t+h})\\) is an affine function of \\(w_t\\). Indeed \\(\\Sigma(.)\\) is an affine function, and the conditional expectations \\(\\mathbb{E}_t(w_{t+h})\\) are affine in \\(w_t\\), as shown by Eq. (1.20). The unconditional mean and variance of \\(w_t\\) are given by: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} \\mathbb{E}(w_t) &amp;=&amp; (I - \\Phi)^{-1}\\mu\\\\ vec[\\mathbb{V}ar(w_t)] &amp;=&amp; (I_{n^2} - \\Phi \\otimes \\Phi)^{-1} vec\\left(\\Sigma[(I - \\Phi)^{-1}\\mu]\\right). \\end{array} \\right.\\tag{1.22} \\end{equation}\\] Proof. Eq. (1.20) is easily deduced from Eq. (1.12), using that \\(\\mathbb{E}_t(\\varepsilon_{t+k})=0\\) for \\(k&gt;0\\). As regards Eq. (1.21): \\[\\begin{eqnarray*} \\mathbb{V}ar_t(w_{t+h}) &amp;=&amp; \\mathbb{V}ar_t\\left(\\Sigma(w_{t+h-1})^{\\frac{1}{2}}\\varepsilon_{t+h}+\\dots + \\Phi^{h-1} \\Sigma(w_{t})^{\\frac{1}{2}}\\varepsilon_{t+1} \\right). \\end{eqnarray*}\\] The conditional expectation at \\(t\\) of all the terms of the sum is equal to zero since, for \\(i \\ge 1\\): \\[ \\mathbb{E}_t\\left[\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i}\\right] = \\mathbb{E}_t[\\underbrace{\\mathbb{E}_{t+i-1}\\{\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i}\\}}_{=\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\mathbb{E}_{t+i-1}\\{\\varepsilon_{t+i}\\}=0}\\}], \\] and \\(\\forall i &lt;j\\), \\[ \\mathbb{C}ov_t\\left[\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i},\\Sigma(w_{t+j-1})^{\\frac{1}{2}}\\varepsilon_{t+j}\\right] = \\mathbb{E}_t\\left[\\Sigma(w_{t+i-1})^{\\frac{1}{2}}\\varepsilon_{t+i}\\varepsilon_{t+j}&#39;\\Sigma&#39;(w_{t+j-1})^{\\frac{1}{2}}\\right], \\] which can be seen to be equal to zero by conditioning on the information available on date \\(t+j-1\\). Using the same conditioning, we obtain that: \\[\\begin{eqnarray*} \\mathbb{V}ar_t\\left[\\Phi^{h-j}\\Sigma(w_{t+j-1})^{\\frac{1}{2}}\\varepsilon_{t+j}\\right] &amp;=&amp; \\mathbb{E}_t\\left[\\Phi^{h-j}\\Sigma(w_{t+j-1})^{\\frac{1}{2}}\\varepsilon_{t+j}\\varepsilon_{t+j}&#39;\\Sigma&#39;(w_{t+j-1})^{\\frac{1}{2}}{\\Phi^{h-j}}&#39;\\right] \\\\ &amp;=&amp; \\mathbb{E}_t\\left[\\Phi^{h-j}\\Sigma(w_{t+j-1})^{\\frac{1}{2}} \\mathbb{E}_{t+j-1}(\\varepsilon_{t+j}\\varepsilon_{t+j}&#39;)\\Sigma&#39;(w_{t+j-1})^{\\frac{1}{2}}{\\Phi^{h-j}}&#39;\\right] \\\\ &amp;=&amp; \\Phi^{h-j}\\mathbb{E}_t[\\Sigma(w_{t+j-1})]{\\Phi^{h-j}}&#39; \\\\ &amp;=&amp; \\Phi^{h-j}\\Sigma(\\mathbb{E}_t[w_{t+j-1}]){\\Phi^{h-j}}&#39;, \\end{eqnarray*}\\] where the last equality results from the fact that he fact that \\(\\Sigma(.)\\) is affine (see Eq. (1.14)). Proposition 1.7 (Affine property of the GARCH-type process) The process \\(w_t = (w_{1,t}, w_{2,t})\\) defined by: \\[ \\left\\{ \\begin{array}{ccl} w_{1, t+1} &amp;=&amp; \\mu + \\varphi w_{1,t} + \\sigma_{t+1} \\varepsilon_{t+1} \\mid \\varphi \\mid &lt; 1 \\\\ \\sigma^2_{t+1} &amp;=&amp; \\omega + \\alpha \\varepsilon^2_t + \\beta \\sigma^2_t 0 &lt; \\beta &lt; 1, \\alpha &gt; 0, \\omega &gt; 0 \\\\ w_{2,t} &amp;=&amp; \\sigma^2_{t+1}, \\quad \\varepsilon_t \\sim i.i.d. \\mathcal{N}(0,1) \\end{array} \\right. \\] is affine. Proof. Note that \\(w_{2,t}\\) is function of \\(\\underline{w_{1,t}}\\) \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}[\\exp(u_1 w_{1, t+1} + u_2 w_{2, t+1})|\\underline{w_{1,t}}] \\\\ &amp;= &amp; \\exp(u_1 \\mu + u_1 \\varphi w_{1,t} + u_2 \\omega + u_2 \\beta w_{2,t}) \\mathbb{E}[\\exp(u_1 \\sigma_{t+1} \\varepsilon_{t+1} + u_2 \\alpha \\varepsilon^2_{t+1})|\\underline{w_{1,t}}] \\end{eqnarray*}\\] and, using Lemma 1.4: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}[\\exp(u_1 w_{1, t+1} + u_2 w_{2, t+1})|\\underline{w_{1,t}}] \\\\ &amp;= &amp; \\exp(u_1 \\mu + u_1 \\varphi w_{1,t} + u_2 \\omega + u_2 \\beta w_{2t}) \\exp \\left[ - \\frac{1}{2} \\log(1-2 u_2 \\alpha) + \\frac{u^2_1 w_{2,t}}{2(1-2 u_2 \\alpha)} \\right]\\\\ &amp;= &amp; \\exp \\left[ u_1 \\mu + u_2 \\omega - \\frac{1}{2} \\log(1-2u_2\\alpha)+ u_1 \\varphi w_{1,t} + \\left(u_2 \\beta + \\frac{u^2_1}{2(1-2u_2\\alpha)}\\right) w_{2,t}\\right], \\end{eqnarray*}\\] which is exponential affine in \\((w_{1,t}, w_{2,t})\\). Proposition 1.8 (Computation of truncated conditional moments) If \\(\\varphi(z)=\\mathbb{E}[exp(z&#39;w)]\\), we have: \\[\\begin{equation} \\mathbb{E}[\\exp(u&#39;w)\\textbf{1}_{(v&#39;w&lt;\\gamma})] = \\frac{\\varphi(u)}{2} - \\frac{1}{\\pi} \\int^\\infty_o \\frac{{\\mathcal I}m[\\varphi(u+ivx)\\exp(-i\\gamma x)]}{x}dx.\\tag{1.23} \\end{equation}\\] Proof. We want to compute \\(\\tilde{\\varphi}_t(u;v,\\gamma) = \\mathbb{E}_t[\\exp(u&#39;w)\\textbf{1}_{(v&#39;w&lt;\\gamma})]\\). Let us first note that, for given \\(u\\) and \\(v\\), \\(\\tilde{\\varphi}_t(u;v,\\gamma)\\) is a positive increasing bounded function of \\(\\gamma\\) and therefore can be seen as the c.d.f. of a positive finite measure on \\(\\mathbb{R}\\), the Fourier transform of which is: \\[ \\int_{\\mathbb{R}} \\exp(i\\gamma x)d\\tilde{\\varphi}(u;v,\\gamma) = \\mathbb{E} \\int_{\\mathbb{R}} \\exp(i\\gamma x)d\\tilde{\\varphi}_w(u;v,\\gamma), \\] where, for given \\(w, \\tilde{\\varphi}_w(u;v,\\gamma)\\) is the c.d.f. of the mass point \\(\\exp(u&#39;w)\\) at \\(v&#39;w\\). We then get: \\[\\begin{eqnarray*} \\int_{\\mathbb{R}} \\exp(i\\gamma x) d\\tilde{\\varphi}(u;v,\\gamma) &amp;=&amp; \\mathbb{E}[\\exp(ixv&#39;w)exp(u&#39;w)] \\\\ &amp; =&amp; \\mathbb{E}[\\exp(u+ivx)&#39;w] \\\\ &amp; =&amp; \\varphi(u+ivx). \\end{eqnarray*}\\] Let us now compute \\(A(x_0,\\lambda)\\) for any real number \\(\\lambda\\), with: \\[\\begin{eqnarray*} &amp;&amp;A(x_0,\\lambda) \\\\ &amp;=&amp; \\frac{1}{2\\pi} \\int^{x_0}_{-x_0} \\frac{\\exp(i\\lambda x)\\varphi(u-ivx)-\\exp(-i\\lambda)\\varphi(u+ivx)}{ix}dx \\\\ &amp;=&amp; \\frac{1}{2\\pi} \\int^{x_0}_{-x_0}\\left[ \\begin{array}{l} \\int_{\\mathbb{R}} \\frac{\\exp[-ix(\\gamma-\\lambda)]-\\exp[ix(\\gamma-\\lambda)]}{ix}d\\tilde{\\varphi}(u;v,\\gamma) \\end{array} \\right]dx \\\\ &amp;=&amp; \\frac{1}{2\\pi} \\int_{\\mathbb{R}} \\left[ \\begin{array}{l} \\int^{x_0}_{-x_0} \\frac{\\exp[-ix(\\gamma-\\lambda)] -\\exp[ix(\\gamma-\\lambda)]}{ix}dx \\end{array} \\right]d\\tilde{\\varphi}(u;v,\\gamma). \\end{eqnarray*}\\] Now : \\[\\begin{eqnarray*} \\frac{1}{2\\pi} \\int^{x_o}_{-x_o} \\frac{\\exp[-ix(\\gamma-\\lambda)] -\\exp[ix(\\gamma-\\lambda)]}{ix}dx \\\\ = \\frac{-sign(\\gamma-\\lambda)}{\\pi} \\int^{x_o}_{-x_o} \\frac{sin(x\\mid\\gamma-\\lambda\\mid)}{x}dx \\end{eqnarray*}\\] which tends to \\(-sign(\\gamma-\\lambda)\\) when \\(x_0\\rightarrow\\infty\\) (where \\(sign(\\omega)=1\\) if \\(\\omega&gt;0\\), \\(sign(\\omega)=0\\) if \\(\\omega=0\\), \\(sign(\\omega)=-1\\) if \\(\\omega&lt;0\\)). Therefore: \\[ A(\\infty,\\lambda) = - \\int_{\\mathbb{R}} sign(\\gamma-\\lambda)d\\tilde{\\varphi}(u;v,\\gamma) = -\\mathbb{E} \\int_{\\mathbb{R}} sign(\\gamma-\\lambda)d\\tilde{\\varphi}_w(u;\\theta,\\gamma), \\] where \\(\\tilde{\\varphi}_w(u;v,\\gamma)\\) is the c.d.f. of the mass point \\(\\exp(u&#39;w)\\) at \\(v&#39;w\\) and \\[ \\int_{\\mathbb{R}} \\mbox{sign}(\\gamma-\\lambda)d\\tilde{\\varphi}_w(u;v,\\gamma)= \\left\\{ \\begin{array}{ccc} \\exp(u&#39;w) &amp; \\mbox{if} &amp; \\lambda &lt; v&#39;w \\\\ 0 &amp; \\mbox{if}&amp; \\lambda = v&#39;w \\\\ -\\exp(u&#39;w) &amp; \\mbox{if} &amp; \\lambda &gt; v&#39;w. \\end{array} \\right. \\] Therefore, we have: \\[\\begin{eqnarray*} A(\\infty,\\lambda) &amp; =&amp; - \\mathbb{E}[\\exp(u&#39;w)(1-\\textbf{1}_{(v&#39;w&lt;\\lambda)})-\\exp(u&#39;w)\\textbf{1}_{(v&#39;w&lt;\\lambda)}] \\\\ &amp; =&amp; - \\varphi(u) + 2\\tilde{\\varphi}(u;v,\\lambda) \\end{eqnarray*}\\] and, further,: \\[ \\tilde{\\varphi}(u;,v,\\gamma) = \\frac{\\varphi(u)}{2} + \\frac{1}{2} A(\\infty,\\gamma), \\] where \\[\\begin{eqnarray*} \\frac{1}{2} A(\\infty,\\gamma) &amp; =&amp; \\frac{1}{4\\pi} \\int^{\\infty}_{-\\infty} \\frac{\\exp(i\\gamma x)\\varphi(u-ivx)-\\exp(-i\\gamma x)\\varphi(u+ivx)}{ix} dx \\\\ &amp; =&amp; \\frac{1}{2\\pi} \\int^{\\infty}_{o} \\frac{\\exp(i\\gamma x)\\varphi(u-ivx)-\\exp(-i\\gamma x)\\varphi(u+ivx)}{ix} dx \\\\ &amp; =&amp; - \\frac{1}{\\pi} \\int^{\\infty}_{o} \\frac{{\\mathcal I}m[\\exp(-i\\gamma x)\\varphi(u+ivx)]}{x}dx, \\end{eqnarray*}\\] which leads to Eq. (1.23). References "],["pricing-and-risk-neutral-dynamics.html", "Chapter 2 Pricing and risk-neutral dynamics 2.1 Consumption-based Capital Asset Pricing Model (CCAPM) and stochastic discount factor (SDF) 2.2 Recursive utilities 2.3 SDF: Absence of Arbitrage Approach 2.4 The risk-neutral (R.N.) dynamics 2.5 Typology of econometric asset-pricing models", " Chapter 2 Pricing and risk-neutral dynamics In brief: bla bla bla bla. 2.1 Consumption-based Capital Asset Pricing Model (CCAPM) and stochastic discount factor (SDF) Consider an economy featuring a single good, whose date-\\(t\\) price is \\(q_t\\). There is a representative agent with an external income \\(R_t\\) at \\(t\\), and a portfolio of assets, with an allocation vector \\(\\alpha_{t-1}\\) (decided at \\(t-1\\)). The vector of date-\\(t\\) prices is \\(p_t\\). At any date \\(t+j\\), \\(j=0,1,...\\), the agent will face the budget constraint: \\[ q_{t+j}C_{t+j}+\\alpha&#39;_{t+j}p_{t+j} = R_{t+j} + \\alpha&#39;_{t+j-1}p_{t+j}, \\] where \\(C_{t+j}\\)is her consumption on date \\(t+j\\). The representative agent maximizes her expected utility time-separable preferences \\(\\mathbb{E}_t \\sum^\\infty_{j=0} \\delta^j U(C_{t+j})\\) subject to the budget constraints at \\(t+j\\), \\(j= 0,1,\\dots\\), where \\(U\\) is the utility function and \\(\\delta\\)is time discount factor or subjective discount factor. Note that the latter is different from SDF. Replacing \\(C_{t+j}\\) with \\([R_{t+j}-(\\alpha&#39;_{t+j}-\\alpha&#39;_{t+j-1})p_{t+j}]/q_{t+j}\\), the objective function becomes: \\[ \\mathbb{E}_t \\sum^\\infty_{j=0} \\delta^j U [R_{t+j}/q_{t+j}-(\\alpha&#39;_{t+j}-\\alpha&#39;_{t+j-1})p_{t+j}/q_{t+j}]. \\] The vector of allocation \\(\\alpha_t\\) appears in the first two terms: \\[ U[R_t/q_t-(\\alpha&#39;_t-\\alpha&#39;_{t-1})p_t/q_t] + \\delta \\mathbb{E}_t U[R_{t+1}/q_{t+1}-(\\alpha&#39;_{t+1}-\\alpha&#39;_t)p_{t+1}/q_{t+1}]. \\] As a result, the first order condition associated with vector \\(\\alpha_t\\) reads \\[ \\frac{p_t}{q_t} \\frac{d U(C_t)}{dC} = \\delta \\mathbb{E}_t \\left[ \\begin{array}{l} \\frac{p_{t+1}}{q_{t+1}} \\frac{dU(C_{t+1})}{dC} \\end{array} \\right], \\] or \\[\\begin{equation} p_t = \\mathbb{E}_t(p_{t+1} \\mathcal{M}_{t,t+1}),\\tag{2.1} \\end{equation}\\] where \\(\\mathcal{M}_{t,t+1}\\) is a strictly positive scalar called stochastic discount factor (SDF) between dates \\(t\\) and \\(t+1\\). We have: \\[ \\mathcal{M}_{t,t+1} \\equiv \\delta \\frac{q_t}{q_{t+1}} \\frac{ \\frac{dU(C_{t+1})}{dC}} { \\frac{dU(C_t)}{dC}}. \\] Example 2.1 (Power utility function) A standard case is the one of the power utility \\(U(C) = \\frac{C^{1-\\gamma}-1}{1-\\gamma}\\), where \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. We have \\(U&#39;(C) = C^{-\\gamma} &gt; 0\\) and \\(U&#39;&#39;(C) = - \\gamma C^{-\\gamma-1} &lt; 0\\). As a result, the stochastic discount factor is \\[\\begin{eqnarray} &amp;&amp; \\mathcal{M}_{t,t+1} \\nonumber \\\\ &amp;=&amp; \\frac{q_t}{q_{t+1}} \\delta \\left( \\frac{C_{t+1}}{C_t} \\right)^{-\\gamma} \\nonumber\\\\ &amp;=&amp; \\exp(\\log \\delta + \\log q_t + \\gamma \\log C_t - \\log q_{t+1} - \\gamma \\log C_{t+1}).\\tag{2.2} \\end{eqnarray}\\] The vector of prices \\(p_t\\) satisfies: \\[ p_t = \\delta q_t C^\\gamma_t \\mathbb{E}_t \\left( \\begin{array}{l} \\frac{C^{-\\gamma}_{t+1}}{q_{t+1}} p_{t+1} \\end{array} \\right), \\] or (Euler equation): \\[ \\mathbb{E}_t\\left[ \\begin{array}{l} \\delta\\left( \\begin{array}{l} \\frac{C_{t+1}}{C_t} \\end{array} \\right)^{-\\gamma} \\frac{q_t}{q_{t+1}} \\frac{p_{t+1}}{p_t} - 1 \\end{array} \\right] = 0. \\] According to Eq. (2.1), for any asset \\(j\\): \\[\\begin{equation} p_{j,t} = \\mathbb{E}_t(\\mathcal{M}_{t,t+1} p_{j,t+1}).\\tag{2.3} \\end{equation}\\] Using that \\(p_{j,t+1} = \\mathbb{E}_t(\\mathcal{M}_{t+1,t+2} p_{j,t+2})\\), we get: \\[\\begin{eqnarray*} p_{j,t} &amp;=&amp; \\mathbb{E}_t[\\mathbb{E}_{t+1}(p_{j,t+2}\\mathcal{M}_{t+1,t+2})\\mathcal{M}_{t,t+1}] \\\\ &amp;=&amp; \\mathbb{E}_t(\\mathcal{M}_{t,t+1} \\mathcal{M}_{t+1,t+2}p_{j,t+2}). \\end{eqnarray*}\\] This can be generalized as follows: \\[ p_{j,t} = \\mathbb{E}_t[\\mathcal{M}_{t,t+1} \\dots \\mathcal{M}_{t+h-1,t+h}p_{j,t+h}], \\; \\forall h. \\] 2.2 Recursive utilities XXXX 2.3 SDF: Absence of Arbitrage Approach Consider a period of interest \\({\\mathcal T} = \\{0,1,2,...,T^*\\}\\). As in Chapter 1, vector \\(w_t\\) constitutes the new information in the economy at \\(t\\). The historical, or physical, dynamics of \\(w_t\\), \\(f(\\underline{w_t})\\), is defined by \\(f(w_{t+1}|\\underline{w_t})\\). The physical probability is denoted by \\(\\mathbb{P}\\). \\(L_{2t}, t \\in {\\mathcal T}\\), is the (Hilbert) space of square integrate functions \\(g(\\underline{w_t})\\), and we have \\(L_{2t} \\subset L_{2s}, t&lt; s\\). 2.3.1 Existence and unicity of the SDF Hypothesis 2.1 (Price existence and uniqueness) For any \\(\\underline{w_t}\\), there exists a unique \\(p_t[g(\\underline{w_s})]\\), function of \\(\\underline{w_t}\\), price at \\(t\\) of a payoff \\(g(\\underline{w_s})\\) delivered at \\(s, \\forall t \\le s\\). Hypothesis 2.2 (Linearity and continuity) For all \\(t &lt; s\\), \\(\\underline{w_t}\\), \\(g_1\\), \\(g_2\\), we have \\(p_t[\\lambda_1 g_1(\\underline{w_s}) + \\lambda_2g_2(\\underline{w_s})] = \\lambda_1p_t[g_1(\\underline{w_s})]+\\lambda_2 p_t[g_2(\\underline{w_s})]\\), If \\(g_n(\\underline{w_s}) \\overset{L_{2s}}{\\underset{n\\rightarrow\\infty}{\\longrightarrow}} 0\\), then \\(p_t[g_n(\\underline{w_s})] \\underset{n\\rightarrow\\infty}{\\longrightarrow} 0\\). Hypothesis 2.3 (Absence of Arbitrage Opportunity (AAO)) At any \\(t\\), it is impossible to constitute a portfolio of future payoffs, possibly modified at subsequent dates, such that: the price of the portfolio at \\(t\\) is zero, payoffs at subsequent dates are \\(\\ge 0\\), there is at least one subsequent date \\(s\\) such that the net payoff at \\(s\\) is strictly positive with a non zero conditional probability at \\(t\\). Theorem 2.1 (Riesz representation theorem) Under Assumptions 2.1 and 2.2, for all \\(\\underline{w_t}\\), and \\(s &gt; t\\), there exists \\(\\mathcal{M}_{t,s}(\\underline{w_s}) \\in L_{2s}\\), unique such that, \\(\\forall g(\\underline{w_s}) \\in L_{2s}\\), \\[ p_t[g(\\underline{w_s})] = \\mathbb{E}[\\mathcal{M}_{t,s}(\\underline{w_s})g(\\underline{w_s})|\\underline{w_t}]. \\] In particular the price at \\(t\\) of a zero coupon bond maturing at \\(s\\) is \\(\\mathbb{E}(\\mathcal{M}_{t,s}|\\underline{w_t})\\). Proposition 2.1 (Positivity of M) If Assumption 2.3 is satisfied, then for all \\(t\\) and \\(s\\), \\(\\mathbb{P}(\\mathcal{M}_{t,s}&gt;0|\\underline{w_t})=1\\). Proof. \\(\\Leftarrow\\) is obvious. If \\(\\Rightarrow\\) was not true, the payoff \\(\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}\\), at \\(s\\), would be such that: \\(\\mathbb{P}[\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}=1|\\underline{w_t}] &gt; 0\\) and \\(p_t[\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}] = \\mathbb{E}_t[\\mathcal{M}_{t,s}\\textbf{1}_{\\{\\mathcal{M}_{t,s} \\le 0\\}}] \\le 0\\). Proposition 2.2 (Time consistency) For all \\(t &lt; r &lt; s\\), we have \\(\\mathcal{M}_{t,s} = \\mathcal{M}_{t,r} \\mathcal{M}_{r,s}\\), which implies: \\(\\mathcal{M}_{t,s} = \\mathcal{M}_{t,t+1} \\mathcal{M}_{t+1,t+2}\\dots\\mathcal{M}_{s-1,s}\\) \\(\\mathcal{M}_{0,t} = \\Pi^{t-1}_{j=0} \\mathcal{M}_{j,j+1}\\) (\\(\\mathcal{M}_{0,t}\\) is called pricing kernel). Proof. Using Lemma 2.1 we have: \\[\\begin{eqnarray*} p_t(g_s) &amp;=&amp; \\mathbb{E}(\\mathcal{M}_{t,s}g_s|\\underline{w_t}) = \\mathbb{E}(\\mathcal{M}_{t,r} p_r(g_s)|\\underline{w_t}) \\\\ &amp;=&amp; \\mathbb{E}[\\mathcal{M}_{t,r}\\mathbb{E}(\\mathcal{M}_{r,s} g_s|\\underline{w_r})|\\underline{w_t}] = \\mathbb{E}(\\mathcal{M}_{t,r} \\mathcal{M}_{r,s} g_s|\\underline{w_t}), \\forall g, \\forall \\underline{w}_{t} \\end{eqnarray*}\\] and, therefore, \\(\\mathcal{M}_{t,s} = \\mathcal{M}_{t,r}\\mathcal{M}_{r,s}\\). Lemma 2.1 For any payoff \\(g_s\\) at \\(s, p_t(g_s) = p_t[p_r(g_s)]\\). Proof. If this was not true, we could construct a sequence of portfolios with a strictly positive payoff at \\(s\\) with zero payoff at any other future date and with price zero at \\(t\\), contradicting Assumption 2.3. Indeed, assuming, for instance, \\(p_t(g_s) &gt; p_t[p_r(g_s)]\\), the payoff at \\(s\\) is defined by the following strategy: (i) at \\(t\\): buy \\(p_r(g_s)\\), (short) sell \\(g_s\\), buy \\(\\frac{p_t(g_s)-p_t[p_r(g_s)]}{\\mathbb{E}(\\mathcal{M}_{t,s}|\\underline{w_t})}\\) zero-coupon bonds maturing at \\(s\\), at global price zero, (ii) at \\(r\\): buy \\(g_s\\) and sell \\(p_r(g_s)\\), generating a zero net payoff, (iii) at \\(s\\), the net payoff is: \\(g_s-g_s+\\frac{p_t(g_s)-p_t[p_r(g_s)]}{\\mathbb{E}(\\mathcal{M}_{t,s}|\\underline{w_t})} &gt; 0\\). Consider an asset whose payoff, on date \\(s\\), is \\(g(\\underline{w_s})\\). We have, \\(\\forall t &lt; s\\): \\[\\begin{equation} \\boxed{p_t[g(\\underline{w_s})] = \\mathbb{E}_t[\\mathcal{M}_{t,t+1}...\\mathcal{M}_{s-1,s}g(\\underline{w_s})].}\\tag{2.4} \\end{equation}\\] In particular, since \\(L_{2,t+1}\\) contains 1, the price at \\(t\\) of a zero-coupon with residual maturity one is given by: \\[ B(t,1) := \\mathbb{E}_t [\\mathcal{M}_{t,t+1}]. \\] Denoting by \\(r_t\\) the continously-compounded interest rate, defined through \\(B(t,1)=\\exp(-r_{t})\\), we get \\[ r_{t}=-\\log \\mathbb{E}_t [\\mathcal{M}_{t,t+1}]. \\] Definition 2.1 (Bank account) The bank account process \\(R_t\\) is defined by \\(R_{t} \\equiv \\exp(r_0+...+r_{t-1}) = \\frac{1}{\\mathbb{E}_0[ \\mathcal{M}_{0,1}]\\times ... \\times \\mathbb{E}_{t-1} [\\mathcal{M}_{t-1,t}]}\\). \\(R_t\\) is the price of an investment initiated on date 0, when it was worth one dollar, and invested on each date at the risk-free rate (for one period). For any price process \\(p_t\\), we have \\(p_t = \\mathbb{E}_t(\\mathcal{M}_{t,s} p_s)\\) (with \\(s&gt;t\\)), or \\(\\mathcal{M}_{0,t} p_t = \\mathbb{E}_t(\\mathcal{M}_{0,s}p_s)\\). That is, \\(\\mathcal{M}_{0,t} p_t\\) is a martingale. In particular \\(\\mathcal{M}_{0,t} R_t\\) is a martingale. 2.3.2 Exponential affine SDF A specific (tractable) case is that of exponential affine SDF. Assume that \\[ \\mathcal{M}_{t,t+1}(\\underline{w_{t+1}}) = \\exp[\\alpha_t(\\underline{w_t})&#39;w_{t+1}+\\beta_t(\\underline{w_t})] \\] where \\(\\alpha_t\\) defines the prices of risk or sensitivity vector. Using \\(\\mathbb{E}_t[\\mathcal{M}_{t,t+1}]=\\exp(-r_{t})=\\exp[\\psi_t(\\alpha_t)+\\beta_t]\\), we get: \\[\\begin{equation} \\mathcal{M}_{t,t+1} = \\exp[-r_{t}+\\alpha&#39;_tw_{t+1}-\\psi_t(\\alpha_t)].\\tag{2.5} \\end{equation}\\] Example 2.2 (CCAPM/Power utility case) In the CCAPM-power-utility case (see Example 2.1), we have (Eq. (2.2)): \\[ \\mathcal{M}_{t,t+1} = \\exp(\\log \\delta + \\log q_t + \\gamma \\log C_t - \\log q_{t+1} - \\gamma \\log C_{t+1}), \\] where \\(q_t\\) is the price of the consumption good, \\(C_t\\) is the quantity consumed at \\(t\\) and \\(\\delta\\) is the intertemporal discount rate. Hence, in that case, \\(\\mathcal{M}_{t,t+1}\\) is exponential affine in \\(w_{t+1} = (\\log q_{t+1}, \\log C_{t+1})&#39;\\) (and its first lag). 2.4 The risk-neutral (R.N.) dynamics The historical Dynamics is characterized by \\(f(\\underline{w_{T^*}})\\), or by the sequence of conditional p.d.f. \\(f_{t+1}(w_{t+1}|\\underline{w_t})\\), or \\(f_{t+1}(w_{t+1})\\), with respect to (w.r.t.) some measure \\(\\mu\\). We define the conditional risk-neutral p.d.f. w.r.t. the conditional historical probability. For that, we employ the Radon-Nikodym derivative \\(d^{\\mathbb{Q}}_{t+1}(w_{t+1}|\\underline{w_t})\\):5 \\[ d^{\\mathbb{Q}}_{t+1}(w_{t+1}|\\underline{w_t}) = \\frac{\\mathcal{M}_{t,t+1}(\\underline{w_{t+1}})}{\\mathbb{E}[\\mathcal{M}_{t,t+1}(\\underline{w_{t+1}})|\\underline{w_t}]}, \\] or \\[ d^{\\mathbb{Q}}_{t+1}(w_{t+1})= \\frac{\\mathcal{M}_{t,t+1}}{\\mathbb{E}_t(\\mathcal{M}_{t,t+1})}=\\exp(r_{t}) \\mathcal{M}_{t,t+1}. \\] In this context, the risk neutral conditional p.d.f. is: \\[\\begin{eqnarray} f^{\\mathbb{Q}}_{t+1}(w_{t+1}) &amp;=&amp; f_{t+1}(w_{t+1})d^{\\mathbb{Q}}_{t+1}(w_{t+1}) \\nonumber \\\\ &amp;=&amp;f_{t+1} (w_{t+1}) \\mathcal{M}_{t,t+1} (\\underline{w_{t+1}}) \\exp [r_{t} (\\underline{w_t})].\\tag{2.6} \\end{eqnarray}\\] The p.d.f. of \\(\\mathbb{Q}\\) w.r.t. the historical dynamics \\(\\mathbb{P}\\) is: \\[ \\xi_{T^*} = \\frac{d\\mathbb{Q}}{d\\mathbb{P}} = \\Pi^{T^{*}-1}_{t=0} d^{\\mathbb{Q}}_{t+1}(w_{t+1}) = \\Pi^{T^{*}-1}_{t=0} \\exp(r_{t}) \\mathcal{M}_{t,t+1}, \\] and the p.d.f. of the R.N. distribution of \\(\\underline{w_t}\\), w.r.t. the corresponding historical distribution is: \\[ \\xi_t= \\Pi^{t-1}_{\\tau=1} d^{\\mathbb{Q}}_{\\tau+1}(w_{\\tau+1})=\\mathbb{E}_t\\left(\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right) = \\mathbb{E}_t\\xi_{T^*}. \\] Therefore, \\(\\xi_t\\) is a \\(\\mathbb{P}\\)-martingale.6 Consider the date-\\(t\\) price of a payoff \\(g(\\underline{w_s})\\) at time \\(s&gt;t\\). An equivalent form of the pricing formula (2.4) is: \\[\\begin{eqnarray*} p_t[g(\\underline{w_s})] &amp;=&amp; \\mathbb{E}_t[\\mathcal{M}_{t,t+1}...\\mathcal{M}_{s-1,s}g(\\underline{w_s})] \\\\ &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t[\\exp(-r_{t}-...-r_{s-1})g(\\underline{w_s})], \\end{eqnarray*}\\] or, with simpler notations: \\[ p_t = \\mathbb{E}^{\\mathbb{Q}}_t[\\exp(-r_{t}-...-r_{s-1})p_s] = \\mathbb{E}^{\\mathbb{Q}}_t\\left(\\frac{R_t}{R_s} p_s\\right), \\] where \\(R_t\\) is the bank account. We also have \\(p_t/R_t = \\mathbb{E}^{\\mathbb{Q}}_t\\left( p_s/R_s\\right)\\), that is, \\(p_t/R_t\\) is a \\(\\mathbb{Q}\\)-martingale. In particular \\(p_t = \\exp(-r_{t})\\mathbb{E}^{\\mathbb{Q}}_t(p_{t+1})\\), or, using the arithmetic return of any payoff \\((p_{t+1}-p_t)/p_t\\), and the arithmetic return of the riskless asset \\(r_{A,t+1}=\\exp(r_{t})-1\\), we get: \\[ \\mathbb{E}^{\\mathbb{Q}}_t\\left(\\frac{p_{t+1}-p_t}{p_t}\\right)=r_{A,t}. \\] Moreover the excess arithmetic return process \\((p_{t+1}-p_t)/p_t-r_{A,t}\\) is a \\(\\mathbb{Q}\\)-martingale difference and, therefore, \\(\\mathbb{Q}\\)-serially uncorrelated. Let us consider the case of an exponential affine SDF \\(\\mathcal{M}_{t,t+1}=\\exp(\\alpha&#39;_t w_{t+1}+\\beta_t)\\): \\[ d^{\\mathbb{Q}}_{t+1}(w_{t+1}) = \\frac{\\mathcal{M}_{t,t+1}}{\\mathbb{E}_t(\\mathcal{M}_{t,t+1})} = \\frac{\\exp(\\alpha&#39;_t w_{t+1}+\\beta_t)}{\\exp[\\psi_t(\\alpha_t)+\\beta_t]} = \\exp[\\alpha&#39;_t w_{t+1}-\\psi_t(\\alpha_t)] \\] We then have that \\(d^{\\mathbb{Q}}_{t+1}(w_{t+1})\\) is also exponential affine. Moreover: \\[ f^{\\mathbb{Q}}_{t+1} (w_{t+1}) = \\frac{f_{t+1} (w_{t+1}) \\exp (\\alpha&#39;_t w_{t+1})}{\\varphi_t (\\alpha_t)}. \\] The previous equation shows that \\(f^{\\mathbb{Q}}_{t+1}\\) is the Esscher transform of \\(f_{t+1}\\) evaluated at \\(\\alpha_t\\). Let us know consider the Laplace transform of the conditional R.N. probability, \\(\\varphi^{\\mathbb{Q}}_t(u|\\underline{w_t})\\), also denoted by \\(\\varphi^{\\mathbb{Q}}_t(u)\\). We have: \\[\\begin{eqnarray*} \\varphi^{\\mathbb{Q}}_t(u) &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}}_t \\exp(u&#39; w_{t+1}) \\\\ &amp;=&amp; \\mathbb{E}_t \\exp[(u+\\alpha_t)&#39;w_{t+1}-\\psi_t(\\alpha_t)] \\\\ &amp;=&amp; \\exp[\\psi_t(u+\\alpha_t)-\\psi_t(\\alpha_t)] = \\frac{\\varphi_t(u+\\alpha_t)}{\\varphi_t(\\alpha_t)}. \\end{eqnarray*}\\] Hence: \\[\\begin{equation} \\boxed{\\psi^{\\mathbb{Q}}_t(u) = \\psi_t(u+\\alpha_t)-\\psi_t(\\alpha_t).}\\tag{2.7} \\end{equation}\\] We check that, if \\(\\alpha_t=0\\), \\(\\psi^{\\mathbb{Q}}_t=\\psi_t\\) (since \\(\\psi_t(0)=0)\\). Moreover, putting \\(u=-\\alpha_t\\) in the expression of \\(\\psi^{\\mathbb{Q}}_t(u)\\) we get \\(\\psi^{\\mathbb{Q}}_t(-\\alpha_t)=-\\psi_t(\\alpha_t)\\), and, replacing \\(u\\) by \\(u-\\alpha_t\\), we get: \\[ \\boxed{\\psi_t(u) = \\psi^{\\mathbb{Q}}_t(u-\\alpha_t)-\\psi^{\\mathbb{Q}}_t(-\\alpha_t).} \\] Also: \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} d_{t+1}(w_{t+1}) &amp;=&amp; \\exp[-\\alpha&#39;_t(w_{t+1})-\\psi^{\\mathbb{Q}}_t(-\\alpha_t)] \\\\ d^{\\mathbb{Q}}_{t+1}(w_{t+1}) &amp;=&amp; \\exp[\\alpha&#39;_t(w_{t+1})+\\psi^{\\mathbb{Q}}_t(-\\alpha_t)]. \\end{array} \\right. \\end{equation*}\\] 2.5 Typology of econometric asset-pricing models Definition 2.2 (Econometric Asset Pricing Model (EAPM)) An Econometric Asset Pricing Model (EAPM) is defined by the following functions: \\(r_{t}(\\underline{w_t})\\), \\(f(w_{t+1}|\\underline{w_t}))\\) [or \\(\\psi_t(u)\\)], \\(\\mathcal{M}_{t,t+1}(\\underline{w_{t+1}})\\), \\(f^{\\mathbb{Q}}(w_{t+1}|\\underline{w_t})\\) [or \\(\\psi^{\\mathbb{Q}}_t(u)\\)]. The previous functions have to to be specified and parameterized. They are linked by: \\[ f^{\\mathbb{Q}}(w_{t+1}|\\underline{w_t}) = f(w_{t+1}|\\underline{w_t}) \\mathcal{M}_{t,t+1}(\\underline{w_{t+1}}) \\exp[r_{t}(\\underline{w_t}))]. \\] In the following, we present three ways of specifying an EAPM: the direct modelling, the R.N.-constrained direct modelling (or mixed modelling), the back modelling. We focus on the case where \\(\\mathcal{M}_{t,t+1}\\) is exponential affine, as in Eq. (2.5): \\[ \\mathcal{M}_{t,t+1} (\\underline{w_{t+1}}) = \\exp\\left\\{ -r_{t} (\\underline{w_t}) + \\alpha&#39;_t(\\underline{w_t})w_{t+1} - \\psi_t [\\alpha_t (w_t)]\\right\\}. \\] Once the short-term rate function \\(r_{t}(\\underline{w_t})\\) is specified, we have to specify \\(\\psi_t\\), \\(\\alpha_t\\), and \\(\\psi^{\\mathbb{Q}}_t\\), that are linked by Eq. (2.7). In all approaches, we have to specify the status of the short rate. The short rate \\(r_{t}\\) is a function of \\(\\underline{w_t}\\), this function may be known or unknown by the econometrician. It is known in two cases: (a) \\(r_{t}\\) is exogenous (\\(r_{t}(\\underline{w_t})\\) does not depend on \\(\\underline{w_t}\\)) or (b) \\(r_{t}\\) is a component of \\(w_t\\). By contrast, if the function \\(r_{t} (\\underline{w_t})\\) is unknown, it has to be specified parametrically: \\[ \\left\\{ r_{t} (\\underline{w_t}, \\tilde{\\theta}), \\tilde{\\theta}\\in \\tilde{\\Theta} \\right\\}, \\] where \\(r_{t}(\\bullet,\\bullet)\\) is a known function. Let us now detail the three steps on which each of the three ways of defining an EAPM is based. 2.5.1 The direct modelling Step 1 – Specification of the historical dynamics. We choose a parametric family for the conditional historical Log-Laplace transform \\(\\psi_t(u|\\underline{w_t})\\): \\(\\left\\{ \\psi_t (u|\\underline{w_t} ; \\theta_1), \\theta_1 \\in \\Theta_1 \\right\\}\\). Step 2 – Specification of the SDF. Considering the affine specification of as Eq. (2.5), that is: \\[ \\mathcal{M}_{t,t+1} (\\underline{w_{t+1}}) = \\exp\\left\\{ -r_{t}(\\underline{w_t}, \\tilde{\\theta}) + \\alpha&#39;_t(\\underline{w_t})w_{t+1} - \\psi_t [\\alpha_t (w_t)|\\underline{w_t} ; \\theta_1]\\right\\}, \\] we need to specifiy functions \\(r_{t}(\\underline{w_t}, \\tilde{\\theta})\\) and \\(\\alpha_t(\\underline{w_t})\\). Assume that \\(\\alpha_t(\\underline{w_t})\\) belongs to a parametric family: \\(\\left\\{ \\alpha_t (\\underline{w_t} ; \\theta_2),\\theta_2 \\in \\Theta_2 \\right\\}\\). We then have: \\[\\begin{eqnarray*} \\mathcal{M}_{t,t+1}(\\underline{w_{t+1}}, \\theta) &amp;=&amp; \\exp \\left\\{ - r_{t} (\\underline{w_t}, \\tilde{\\theta}) + \\alpha&#39;_t (\\underline{w_t},\\theta_2) w_{t+1} - \\psi_{t} \\left[ \\alpha_t (\\underline{w_t}, \\theta_2) | \\underline{w_t} ; \\theta_1 \\right] \\right\\}, \\end{eqnarray*}\\] where \\(\\theta = (\\tilde{\\theta}&#39;, \\theta&#39;_1,\\theta&#39;_2)&#39; \\in \\tilde{\\Theta}\\times \\Theta_1 \\times \\Theta_2 = \\Theta\\). Step 3 – Internal consistency conditions (ICC). For any payoff \\(g(\\underline{w_s})\\) settled at \\(s&gt;t\\), with price \\(p(\\underline{w_t})\\) at \\(t\\) which is a known function of \\(\\underline{w_t}\\), we must have: \\[\\begin{equation*} p(\\underline{w_t}) = \\mathbb{E} \\left\\{\\mathcal{M}_{t,t+1} (\\theta) \\dots \\mathcal{M}_{s-1,s} (\\theta) g(\\underline{w_s}) | \\underline{w_t}, \\theta_1 \\right\\} \\forall \\; \\underline{w_t}, \\theta.\\tag{2.8} \\end{equation*}\\] These ICC pricing conditions may imply strong constraints on \\(\\theta\\). For instance, when components of \\(w_t\\) are returns of some assets: if \\(w_{1,t} = \\log(p_{1,t}/p_{1,t-1})\\), then we must have \\(\\mathbb{E}_t [\\mathcal{M}_{t,t+1} \\exp (e&#39;_1 w_{t+1})]= 1\\) (Euler equation). Or, in the case of interest rates with various maturities: if \\(w_{1,t} = -1/h\\log B(t,h)\\), then we must have \\(e&#39;_1 w_{t} = - 1/h \\log \\mathbb{E}_t (\\mathcal{M}_{t,t+1}\\times \\dots \\times \\mathcal{M}_{t+h-1,t+h})\\). The previous three steps imply the specification of the R.N. dynamics (according to Eq. (2.7)): \\[\\begin{equation*} \\psi^{\\mathbb{Q}} (u | \\underline{w_t}, \\theta_1, \\theta_2) = \\psi_t \\left[ u + \\alpha_t (\\underline{w_t}, \\theta_2) | \\underline{w_t}, \\theta_1 \\right] - \\psi_t \\left[ \\alpha_t (\\underline{w_t}, \\theta_2) | \\underline{w_t}, \\theta_1 \\right]. \\end{equation*}\\] 2.5.2 The R.N.-constrained direct modelling (or mixed modelling) Step 1 – Specification of the physical dynamics. We select a family \\(\\{ \\psi_t (u | \\underline{w_t},\\theta_1), \\theta_1 \\in \\Theta_1 \\}\\). Step 2 – Specification of the risk-neutral dynamics. We select a family \\(\\{\\psi^{\\mathbb{Q}}_t (u | \\underline{w_t}, \\theta^*),\\theta^* \\in \\Theta^* \\}\\) and, possibly, \\(\\{r_{t}(\\underline{w_t},\\tilde{\\theta}),\\tilde{\\theta}\\in\\tilde{\\Theta}\\}\\). Step 3 – Internal Consistency Conditions (ICC). Once the parameterization \\((\\tilde{\\theta}, \\theta_1, \\theta^*) \\in \\tilde{\\Theta} \\times \\Theta^*_1\\) is defined, ICCs may be imposed. For instance, if \\(w_{1,t} = \\log(p_{1,t}/p_{1,t-1})\\), then we must have \\(\\exp(-r_t)\\mathbb{E}^{\\mathbb{Q}}_t \\exp (e_{1}&#39; w_{t+1}) = 1\\). Or if \\(w_{1,t} = B(t,h)\\), then \\(e_{1}&#39; w_{t} = \\mathbb{E}_t^{\\mathbb{Q}} \\exp(-r_t - \\dots - r_{t+h-1})\\). The SDF is a by-product. If we want an exponential affine SDF, for any pair \\((\\psi^{\\mathbb{Q}}_t, \\psi_t)\\) belonging to these families, there must exist a unique function \\(\\alpha_t (\\underline{w_t})\\) denoted by \\(\\alpha_t (w_t ; \\theta_1, \\theta^*)\\), and satisfying: \\[\\begin{equation*} \\psi^{\\mathbb{Q}}_t (u | \\underline{w_t}) = \\psi_t \\left[ u + \\alpha_t (w_t) | \\underline{w_t} \\right] - \\psi_t \\left[ \\alpha_t (\\underline{w_t}) | \\underline{w_t} \\right]. \\end{equation*}\\] 2.5.3 Back modelling (based on three steps) Step 1 – Specification of the R.N. dynamics, and possibly of \\(r_{t}(\\underline{w_t})\\)]: \\(\\psi^{\\mathbb{Q}}_t (u | \\underline{w_t}; \\theta^*_1)\\). Step 2 – Internal consistency conditions (ICC), if relevant, are taken into account: \\[\\begin{equation*} \\begin{array}{lll} &amp;&amp; p(\\underline{w_t}) = \\mathbb{E}^{\\mathbb{Q}}_t \\left[ \\exp (-r_{t} (\\underline{w_t},\\tilde{\\theta}) - \\dots - r_{s-1} (\\underline{w_s}, \\tilde{\\theta}))g(\\underline{w_s}) | \\underline{w_t} , \\theta^*_1\\right] ,\\\\ &amp;&amp; \\forall \\underline{w_t} , \\tilde{\\theta} , \\theta^*_1. \\end{array} \\end{equation*}\\] Step 3 – Choice of the specification of the prices of risk. One chooses function \\(\\alpha_t(\\underline{w_t})\\) without any constraint; this amounts to defining the family \\(\\{ \\alpha_t (\\underline{w_t}, \\theta^*_2), \\theta^*_2\\in \\Theta^*_2 \\}\\). The historical dynamics is obtained as a by-product. Indeed: \\[\\begin{equation*} \\psi_t(u | \\underline{w_t} ; \\theta^*_1, \\theta^*_2) = \\psi_t^{\\mathbb{Q}}\\left[ u -\\alpha_t (\\underline{w_t}, \\theta^*_2)|\\underline{w_t} ; \\theta^*_1 \\right] -\\psi^{\\mathbb{Q}}_t \\left[- \\alpha_t (\\underline{w_t}, \\theta^*_2) | \\underline{w_t},\\theta^*_1 \\right]. \\end{equation*}\\] Of course, the conditional historical p.d.f. with respect to the conditional risk-neutral (R.N.) p.d.f. is: \\(d_{t+1}(w_{t+1}) = \\frac{1}{d^{\\mathbb{Q}}_{t+1}(w_{t+1})}\\) or \\(d_{t+1}(w_{t+1}) = \\frac{\\exp(-r_{t})}{\\mathcal{M}_{t,t+1}}\\).↩︎ Indeed: \\[ \\mathbb{E}_t \\left( \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right) = \\Pi^{t-1}_{\\tau = 1} d^{\\mathbb{Q}}_{\\tau + 1} (w_{\\tau+1}) \\mathbb{E}_t \\left( d^{\\mathbb{Q}}_{t+1} (w_{t+1}) \\ldots d^{\\mathbb{Q}}_{T^*} (w_{T^*})\\right). \\]↩︎ "],["estimation-of-affine-asset-pricing-models.html", "Chapter 3 Estimation of affine asset-pricing models 3.1 State-space model 3.2 Kalman-filter-based approach 3.3 About non-constant conditional matrix \\(\\Sigma\\) 3.4 Non-linear models 3.5 The inversion technique 3.6 A typical small-sample issue", " Chapter 3 Estimation of affine asset-pricing models 3.1 State-space model By nature, dynamic asset-pricing models are state-space models: The dynamics of all variables, gathered in vector \\(y_t\\) (yields, equity returns, macroeconomic variables, survey-based variables) are accounted for by state variables (\\(w_t\\)). The equations defining the relationship between the other variables and the state variables are called measurement equations (Eq. (3.1)). The equations defining the dynamics of the state variables are called transition equations (Eq. (3.2)). In the case where \\(w_t\\) is an affine process (see Definition 1.1), the transition equations admit a VAR(1) representation (Proposition 1.1). In that case, the state-space model is said to be linear, as formalized by the following definition. This defintion introduces, in particular, the notion of measurement errors. Definition 3.1 (Linear State-Space Model) A linear state-space model writes as follows: \\[\\begin{eqnarray} \\underset{(m \\times 1)}{y_t} &amp;=&amp; A + Bw_t + \\eta_t \\quad \\mbox{with } \\eta_t \\sim i.i.d. \\mathcal{N}(0,\\Omega) \\tag{3.1} \\\\ \\underset{(n \\times 1)}{w_t} &amp; =&amp; \\mu + \\Phi w_{t-1} + \\Sigma^{\\frac{1}{2}}(w_{t-1}) \\varepsilon_t,\\tag{3.2} \\end{eqnarray}\\] where \\(\\varepsilon_t\\) is a martingale difference sequence satisfying \\(\\mathbb{V}ar_t(\\varepsilon_{t+1}) = Id\\). The components of \\(\\eta_t\\) are measurement errors. Note: Eq. (3.2) derives from Proposition 1.1 (Eq. (1.12)). In practice, one can distinguish two situations: (a) all state variables (components of \\(w_t\\)) are observed and (b) some of these variables are latent. What do we mean by model estimation in case (b)? There are different situations: (b.i) We know the model parameters but we want to recover the latent factors—for instance to compute model-implied prices. (b.ii) We know neither the model parameters nor the latent variables, we want to estimate both of them. (b.iii) We know neither the model parameters nor the latent variables, we are just interested in the model parameters. In case (a): One can resort to standard estimation techniques (GMM, Maximum Likelihood) to estimate model parameters. Take for instance the maximum-likelihood case, we have: \\[\\begin{eqnarray*} f(y_t,w_t|\\underline{y_{t-1}},\\underline{w_{t-1}}) &amp;=&amp; f(y_t|\\underline{y_{t-1}},\\underline{w_{t}}) \\times \\underbrace{f(w_t|\\underline{y_{t-1}},\\underline{w_{t-1}})}_{= f(w_t|\\underline{w_{t-1}})}\\\\ &amp;=&amp; \\mathbf{n}(y_t;A + Bw_t,\\Omega) f(w_t|\\underline{w_{t-1}}), \\end{eqnarray*}\\] where \\(\\mathbf{n}(x;\\mu,\\Omega)\\) denotes the evaluation, at vector \\(x\\), of the p.d.f. of the multivariate normal distribution \\(\\mathcal{N}(\\mu,\\Omega)\\). That is, if \\(x\\) is a \\(m\\)-dimensional vector: \\[\\begin{equation} \\mathbf{n}(x;\\mu,\\Omega) = \\frac{1}{\\sqrt{(2 \\pi)^{m}|\\Omega|}} \\exp\\left(-\\frac{1}{2}\\{x - \\mu\\}&#39;\\Omega^{-1}\\{x-\\mu\\}\\right).\\tag{3.3} \\end{equation}\\] Once this conditional p.d.f. is known, the total likelihood is given by (conditional on \\(y_0\\) and \\(w_0\\)): \\[ \\prod_{t=1}^T f(y_t,w_t|\\underline{y_{t-1}},\\underline{w_{t-1}}). \\] Of course, \\(f(w_t|\\underline{w_{t-1}})\\) depends on the process chosen for \\(w_t\\). If it is complicated to compute, one can employ Pseudo Maximum Likelihood (C. Gourieroux, Monfort, and Trognon 1984). The p.d.f. may involve, for instance, an infinite sum, which is the case in the ARG case of Example 1.8. When \\(w_t\\) is affine, the Pseudo Maximum Likelihood approach consists in replacing \\(f(w_t|\\underline{w_{t-1}})\\) by: \\[ \\mathbf{n}(w_t;\\mu + \\Phi w_{t-1},\\Sigma(w_t)), \\] where \\(\\mu\\), \\(\\Phi\\) and \\(\\Sigma(w_t)\\) are introduced in Eqs. (1.13) and (1.14) in Proposition 1.1. In case (b.iii), one can estimate the model by Generalized Method of Moments (GMM), fitting sample moments computed using observed variables (prices, yields, returns). In the context of affine processes, conditional and unconditional moments of the state vector \\(w_t\\) are available in closed from, as shown by Eqs. (1.15), (1.16) and (1.17). If the model-implied moments are not available in closed-form, one may have to to resort to the Simulated Method of Moments (SMM) (see, e.g., Gouriéroux and Monfort (1997) or Darrell Duffie and Singleton (1993)). In cases (b.i) and (b.ii), one has to implement filtering methods, on which we focus on in the following. 3.2 Kalman-filter-based approach 3.2.1 The Gaussian linear state-space case Let us start with a particular case of state-space model (Def. 3.1) where \\(\\varepsilon_t\\) is Gaussian and where \\(\\Sigma^{\\frac{1}{2}}\\) does not depend on \\(w_t\\), i.e.with a homoskedastic linear Gaussian state-space model. Let’s denote by \\(\\theta\\) the vector of parameters that defines the model. For a given \\(\\theta\\) and a sequence of observations \\(\\{y_1,\\dots,y_T\\}\\), the Kalman filter computes the distribution of \\(w_t\\) given \\(\\{y_1,\\dots,y_t\\}\\) (see Def. 3.2). This distribution is Gaussian, and obtained by a recursive algorithm. A byproduct of this algorithm is the likelihood function associated with \\(\\theta\\) and \\(\\{y_1,\\dots,y_T\\}\\). This opens the door to the estimation of \\(\\theta\\) by MLE, maximizing this function. In this sense, Kalman-filter techniques can address Objective (b.ii). Let us first introduce the notion of filtered and smoothed estimates of a latent variable (or vector of variables) \\(w_t\\): Definition 3.2 (Filtered versus smoothed estimates) The filtering and smoothing problems consist in computing the following conditional moments: \\[\\begin{equation*} \\begin{array}{lccllllll} \\mbox{Filtering:} &amp; w_{t|t} &amp; = &amp; \\mathbb{E}(w_t|\\underline{y_t}) &amp; \\mbox{and} &amp; P_{t|t} &amp;=&amp; \\mathbb{V}ar(w_t|\\underline{y_t})\\\\ \\mbox{Smoothing:} &amp; w_{t|T} &amp; = &amp; \\mathbb{E}(w_t|\\underline{y_T}) &amp; \\mbox{and} &amp; P_{t|T} &amp;=&amp; \\mathbb{V}ar(w_t|\\underline{y_T}). \\end{array} \\end{equation*}\\] The following proposition outlines the Kalman algorithm (see, e.g. Nelson and Kim (1999)). Proposition 3.1 (Kalman filter and smoother) If \\(\\varepsilon_t \\sim \\mathcal{N}(0,I)\\) in the state-space defined in Def. 3.1, then we have (filtering): \\[ w_t|y_1,\\dots,y_t \\sim \\mathcal{N}(w_{t|t}|P_{t|t}), \\] where \\(w_{t|t}\\) and \\(P_{t|t}\\) result from the following recursive equations: \\[ \\boxed{ \\begin{array}{ccl} w_{t|t} &amp;=&amp; w_{t|t-1} + K_t \\lambda_t\\\\ P_{t|t} &amp;=&amp; (I - K_t B)P_{t|t-1} \\\\ \\\\ \\mbox{where (updating step)} \\\\ \\lambda_t &amp;=&amp; y_t - A - Bw_{t|t-1} \\quad \\mbox{(forecast error)}\\\\ S_{t|t-1} &amp;=&amp; \\mathbb{V}ar(y_t|\\underline{y_{t-1}}) = B P_{t|t-1} B&#39; + \\Omega\\\\ K_t &amp;=&amp; P_{t|t-1}B&#39;S_{t|t-1}^{-1} \\quad \\mbox{(Kalman gain)} \\\\ \\\\ \\mbox{and where (forecasting step)} \\\\ w_{t|t-1} &amp;=&amp; \\mu + \\Phi w_{t-1|t-1} \\\\ P_{t|t-1} &amp;=&amp; \\Sigma + \\Phi P_{t-1|t-1} \\Phi&#39; \\quad (\\Sigma = \\Sigma^{\\frac{1}{2}}{\\Sigma^{\\frac{1}{2}}}&#39;). \\end{array} } \\] The log likelihood is (recursively) computed as follows: \\[\\begin{eqnarray} \\log \\mathcal{L}(\\theta;\\underline{y_T}) &amp;=&amp; \\frac{mT}{2}\\log\\left(2\\pi\\right) \\tag{3.4}\\\\ &amp; &amp; -\\frac{1}{2}\\sum_{t=1}^{T}\\left(\\log\\left|S_{t | t-1}(\\theta)\\right|+\\lambda&#39;_{t}(\\theta)S_{t\\mid t-1}^{-1}(\\theta)\\lambda{}_{t}(\\theta)\\right). \\nonumber \\end{eqnarray}\\] Moreover, we have (smoothing): \\[ w_t|y_1,\\dots,y_T \\sim \\mathcal{N}(w_{t|T}|P_{t|T}), \\] where \\(w_{t|T}\\) and \\(P_{t|T}\\) result from the following recursive equations: \\[ \\boxed{ \\begin{array}{ccl} w_{t|T} &amp; = &amp; w_{t|t}+F_{t}(w_{t+1|T}-w_{t+1|t})\\\\ P_{t|T} &amp; = &amp; P_{t|t}+F_{t}(P_{t+1|T}-P_{t+1|t})F&#39;_{t}\\\\ \\\\ \\mbox{where} \\\\ F_{t} &amp;=&amp; P_{t|t}\\Phi&#39;_{t+1}P_{t+1|t}^{-1}. \\end{array} } \\] The following figure illustrates the updating step of the Kalman algorithm: Figure 3.1: Updating in the Kalman filter. The recursive equations of the Kalman filter need to be initialized. That is, one needs to provide initial values for \\(w_{0|0}\\), \\(P_{0|0}\\). Different possibilities have been proposed. One can for instance: Include the elements of(\\(w_{0|0}\\), \\(P_{0|0}\\)) among the parameters to estimate; Set \\(w_{0|0}\\) and \\(P_{0|0}\\) to their unconditional values (using, e.g., Eq. (1.17)); Set \\(w_{0|0}\\) to a prior value and take either an arbitrary large value for \\(P_{0\\mid0}\\) if the prior value is uncertain (which depicts a situation of diffuse prior) or a small value for \\(P_{0\\mid0}\\) if we are confident in this prior value \\(w_{0|0}\\). Example 3.1 (Kalman filtering and smoothing) To illustrate, consider the following model: \\[\\begin{eqnarray} \\left[\\begin{array}{c} y_{1,t}\\\\ y_{2,t} \\end{array}\\right] &amp; = &amp; \\left[\\begin{array}{cc} \\alpha_{1} &amp; 0\\\\ 0 &amp; \\alpha_{2} \\end{array}\\right] \\left[\\begin{array}{c} y_{1,t-1}\\\\ y_{2,t-2} \\end{array}\\right]+\\left[\\begin{array}{c} \\gamma_{1}\\\\ \\gamma_{2} \\end{array}\\right]w_{t}+ D\\eta_t\\label{eq_measur}\\\\ w_{t} &amp; = &amp; \\phi w_{t-1}+\\varepsilon_{t},\\label{eq_trans} \\end{eqnarray}\\] where \\(\\eta_t \\sim \\mathcal{N}(0,Id)\\). In the following lines, we specify one version of the previous model. We simulate trajectories of \\(y_{1,t}\\), \\(y_{2,t}\\) and \\(w_t\\) over 100 periods (Figure 3.2) and we further call Kalman_filter and Kalman_filter (from the TSModels package) to compute the filtered and smoothed estimates of \\(w_t\\) (Figure 3.3). library(TSModels) # Kalman filter procedure in there. # Set model specifications: alpha1 &lt;- .5;alpha2 &lt;- .95;Alpha &lt;- diag(c(alpha1,alpha2)) d_11 &lt;- 1;d_12 &lt;- .5;d_21 &lt;- .5;d_22 &lt;- 2 D &lt;- matrix(c(d_11,d_21,d_12,d_22),2,2) gamma1 &lt;- 1;gamma2 &lt;- 2;Gamma &lt;- matrix(c(gamma1,gamma2),2,1) phi &lt;- .8 # Simulate model: T &lt;- 100 Y &lt;- NULL;X &lt;- NULL Alpha.Y_1 &lt;- NULL y &lt;- c(0,0);x &lt;- 0 for(i in 1:T){ Alpha.Y_1 &lt;- rbind(Alpha.Y_1,c(Alpha %*% y)) y &lt;- Alpha %*% y + Gamma * x + D %*% rnorm(2) x &lt;- phi * x + rnorm(1) Y &lt;- rbind(Y,t(y));X &lt;- rbind(X,x)} # Define matrices needed in the Kalman_filter procedures: nu_t &lt;- matrix(0,T,1) H &lt;- phi;G &lt;- Gamma mu_t &lt;- Alpha.Y_1 N &lt;- 1;M &lt;- D Sigma_0 &lt;- 1/(1-phi^2) # unconditional variance of w rho_0 &lt;- 0 # unconditional mean of w filter.res &lt;- Kalman_filter(Y,nu_t,H,N,mu_t,G,M,Sigma_0,rho_0,indic_pos=0) smoother.res &lt;- Kalman_smoother(Y,nu_t,H,N,mu_t,G,M,Sigma_0,rho_0,indic_pos=0) Figure 3.2: Simulated trajectories of \\(y_{1,t}\\), \\(y_{2,t}\\), and \\(w_t\\). Figure 3.3: Filtered and smoothed estimates of \\(w_t\\). 3.2.2 Missing observations In many application, one does not observe all the entries of \\(y_t\\) at every date. This arises for instance in situations where (i) measurement variables feature different frequencies, (ii) we have unreliable data for some period (that we prefer not to include among observations), (iii) some of the measurement variables are observed over a shorter time span. These situations are easily addressed by Kalman filtering/smoothing (e.g., Chow and Lin (1971) or Harvey and Pierse (1984)). To accommodate missing observations in some of the \\(y_t\\)’s, one simply has to change the size of this vector (and of \\(\\lambda_t\\), \\(S_{t|t-1}\\), \\(A\\) and \\(B\\)) for the relevant dates. Of course, the accuracy of \\(w_{t|t}\\) will tend to be lower during periods where one or several (or all) the entries of \\(y_t\\) are unobserved. (This will be apparent in the resulting covariance matrix of \\(w_{t|t}\\), namely \\(P_{t|t}\\).) The log-likelihood computation (Eq. (3.4)) is still valid in this case; one simply has to adjust the number of observed variables at each iteration; that is, \\(m\\) then depends on time. Example 3.2 (Kalman filtering and smoothing) This example extends Example 3.2. We take the simulated path of the obeerved variables \\(y_{1,t}\\) and \\(y_{2,t}\\) and remove observations of \\(y_{1,t}\\) (respectively of \\(y_{2,t}\\)) between periods \\(t=30\\) and \\(t=50\\) (resp. between periods \\(t=40\\) and \\(t=70\\)), and then use the Kalman filter and smother to recover the states \\(w_t\\) in this situation with missing observations. Y.modif &lt;- Y Y.modif[30:50,1] &lt;- NaN Y.modif[40:70,2] &lt;- NaN # Call of Kalman filter and smoother: filter.res &lt;- Kalman_filter(Y.modif,nu_t,H,N,mu_t,G,M,Sigma_0,rho_0,indic_pos=0) smoother.res &lt;- Kalman_smoother(Y.modif,nu_t,H,N,mu_t,G,M,Sigma_0,rho_0,indic_pos=0) Figure 3.4: Situation of missing observations. Figure 3.5: Filtered and smoothed estimates of \\(w_t\\) in a situation of missing observations. The lower plot shows the standard errors associated with filtered and smoothed estimates. As expected, undertainty is larger for those dates where observations are missing. 3.3 About non-constant conditional matrix \\(\\Sigma\\) Proposition 3.1 is valid when \\(\\varepsilon_t\\) is Gaussian and when \\(\\Sigma^{\\frac{1}{2}}\\) does not depend on \\(w_t\\). However, in the general case (but when \\(w_t\\) is an affine process), we have that \\(\\Sigma(w_{t-1}) \\equiv \\mathbb{V}ar(w_{t+1}|\\underline{w_t})\\) is affine in \\(w_t\\) (see Prop. 1.1). In order to deal with this, the Kalman filter algorithm can be modified. Specifically, in the prediction step (see Prop. 3.1), \\(P_{t|t-1}\\) can be approximated by: \\[ P_{t|t-1} = \\Sigma\\color{red}{(w_{t-1|t-1})} + \\Phi P_{t-1|t-1} \\Phi&#39;, \\] i.e. we replace \\(\\Sigma(w_{t-1})\\) by \\(\\Sigma(w_{t-1|t-1})\\). Though the approach is then not necessarily optimal, it shows good empirical properties (Jong (2000) or Duan and Simonato (1999)). In order to test for the validity of the approach in a specific context, one can resort to Monte-Carlo simulations (Alain Monfort et al. 2017). 3.4 Non-linear models As soon as \\(w_t\\) follows an affine process, it admits the VAR dynamics presented in Prop. 1.1, i.e., it features a linear transition equation. However, measurement equations may be non-linear (affine) functions of \\(w_t\\). This is in particular the case if observed variables include Swaps rates (see Remark SWAPS XXXX), CDS rates (see Subsection ??, in particular Eq. (4.35)) or prices of tranche products (see Example 4.5). In a context of non-linear measurement equations, one can for instance resort to the Extended Kalman Filter (linearizing the measurement equations) or, to higher-order Taylor. Alain Monfort, Renne, and Roussellet (2015) develop a Quadratic Kalman Filter (QKF), where measurement equations are quadratic functions of the state vector. 3.5 The inversion technique The inversion technique has been introduced by Chen and Scott (1993). It is used, e.g., by Ang and Piazzesi (2003) and Liu, Longstaff, and Mandell (2006). Contrary to Kalman-type approaches, this approach is not recursive. it can therefore be faster, especially for long sample. This approach works under the assumption that some of the observed variables are perfectly priced (or modelled).(Recall that \\(y_t\\) and \\(w_t\\) are respectively of dimension \\(m\\) and \\(n\\), see Eqs. (3.1) and (3.2) in Def. 3.1.) Formally: Hypothesis 3.1 (Perfectly-modelled variables) \\(n\\) components of the \\(m\\)-dimensional vector \\(y_t\\) (with \\(n \\le m\\)) are perfectly modelled. That is, there is no measurement errors in associated measurement equations. Without loss of generality, these perfectly-modelled variables are the first \\(n\\) components of \\(y_t\\), that is: \\[ y_t = \\left(\\begin{array}{c} \\underbrace{y_{1,t}}_{(n \\times 1)} \\\\ \\underbrace{y_{2,t}}_{(m-n)\\times1} \\end{array}\\right). \\] Under Assumption 3.1, the measurement equation (Eq. (3.1)) becomes: \\[ \\left[ \\begin{array}{c} y_{1,t}\\\\ y_{2,t} \\end{array} \\right] = \\left[ \\begin{array}{c} A_{1}\\\\ A_{2} \\end{array} \\right]+ \\left[ \\begin{array}{c} B_{1}\\\\ B_{2} \\end{array} \\right]w_t + \\left[ \\begin{array}{c} 0\\\\ \\eta_{2,t} \\end{array} \\right], \\] where \\(\\eta_{2,t} \\sim \\mathcal{N}(0,\\Omega_2)\\) (say). This notably implies that \\[\\begin{equation} w_t = B_{1}^{-1}(y_{1,t} - A_1).\\tag{3.5} \\end{equation}\\] Under this assumption, and if the conditional distribution of \\(w_t\\) is available in closed form, then Proposition 3.2 shows that the (exact) likelihood of the model can then be computed. This proposition shows in particular that the conditional p.d.f. \\(f_{Y_t|\\underline{Y_{t-1}}}(y_t;\\underline{y_{t-1}})\\) involves three terms: The first term (in blue in (3.6)) stems from the conditional distribution \\(w_t|\\underline{w_{t-1}}\\). The second term (in red in (3.6)) is associated with the measurement errors pertaining to \\(y_{2,t}\\), that are the components of \\(\\eta_{2,t}\\). The third term (in brown in (3.6)) is the determinant of the Jacobian matrix associated with the linear transformation between \\(w_t\\) and \\(y_{1,t}\\) (Eq. (3.5)), that is \\(|B_1|\\). Once one knows how to compute \\(f_{Y_t|\\underline{Y_{t-1}}}(y_t;\\underline{y_{t-1}})\\), the total likelihood is easily obtained since: \\[ f_{Y_1,\\dots,Y_T}(y_1,y_2,\\dots,y_T) = f_{Y_1}(y_1) \\prod_{t=2}^T f_{Y_t|\\underline{Y_{t-1}}}(y_t;\\underline{y_{t-1}}). \\] Proposition 3.2 (Log-likelihood in the inversion context) In the context of a linear state-space model as defined in Def. 3.1, under Assumption 3.1, and if \\(w_t\\) is a Markovian process, we have: \\[\\begin{eqnarray} f_{Y_t|\\underline{Y_{t-1}}}(y_t;\\underline{y_{t-1}}) &amp;=&amp; \\color{blue}{f_{w_t|w_{t-1}}(w(y_{1,t});w(y_{t-1}))} \\times \\nonumber\\\\ &amp;&amp; \\color{red}{\\mathbf{n}(y_{2,t}; A_2 + B_2w(y_{1,t}),\\Omega_2)} \\times \\color{brown}{|B_1|^{-1}}.\\tag{3.6} \\end{eqnarray}\\] where \\(w(y_{1,t}) = B_{1}^{-1}(y_{1,t} - A_1)\\) and where \\(\\mathbf{n}\\) denotes the multivariate normal p.d.f. (Eq. (3.3)). Proof. Since \\(w_t\\) is Markov, so is \\(y_{1,t}\\) and since \\(y_{2,t} = A_2 + B_2 w(y_{1,t}) + \\eta_{2,t}\\), with \\(w(y_{1,t}) = B_{1}^{-1}(y_{1,t} - A_1)\\), we have: \\[\\begin{eqnarray*} f(y_t|y_{t-1}) &amp;=&amp; f_1(y_{1,t}|y_{1,t-1}) f_2(y_{2,t}|y_{1,t}) \\\\ &amp;=&amp; |B_1|^{-1} f_w(w(y_{1,t})|w(y_{1,t-1})) \\mathbf{n}(y_{2,t}; A_2 + B_2w(y_{1,t}),\\Omega_2), \\end{eqnarray*}\\] where \\(f_1(y_{1,t}|y_{1,t-1})= |B_1|^{-1} f_w(w(y_{1,t})|w(y_{1,t-1}))\\) comes from the fact that, if \\(U\\) and \\(V\\) are two random variables such that \\(V = g(U)\\), where \\(g\\) is a bijective and differentiable function, then \\(f_V(v)=\\left|\\frac{\\partial g^{-1}(v)}{\\partial v&#39;}\\right| f_U(g^{-1}(v))\\), and \\(f_2(y_{2,t}|y_{1,t}) = \\mathbf{n}(y_{2,t}; A_2 + B_2w(y_{1,t}),\\Omega_2)\\) comes from the fact that \\(y_{2,t}|y_{1,t} \\sim \\mathcal{N}(A_2 + B_2 w(y_{1,t}),\\Omega_2)\\). 3.5.1 Dealing with unobserved regimes Kalman and inversion techniques are not suited to the case where some of the components of \\(w_t\\) are valued in a discrete set. This is typically the case if \\(w_t\\) is of the form: \\[ w_t = \\left(\\begin{array}{c} z_t \\\\ x_t \\end{array}\\right), \\] where \\(z_t\\) is valued in \\(\\{e_1,\\dots,e_J\\}\\), \\(e_j\\) being the \\(j^{th}\\) column of \\(Id_J\\), which is the case in the prsence of regime switching features (see Subsection 1.6). Assume that \\(z_t\\) is an exogenous and homogenous Markov chain whose dynamics is defined by the \\(\\pi(e_i,e_j)\\)’s that are such that: \\[\\begin{equation} \\pi(e_i, e_j) = \\mathbb{P}(z_{t+1}=e_j | z_t=e_i).\\tag{3.7} \\end{equation}\\] Denote by \\(\\Pi\\) the matrix of transition probabilities, i.e., the \\((i,j)\\) component of \\(\\Pi\\) is \\(\\pi(e_i, e_j)\\). Assume further that we have: \\[\\begin{equation} x_t = m(z_t,x_{t-1}) + \\varepsilon_t,\\tag{3.8} \\end{equation}\\] where \\(\\mathbb{E}(\\varepsilon_t|\\underline{z_t},\\underline{x_{t-1}})=0\\). We denote the conditional distribution of \\(\\varepsilon_t\\) w.r.t. \\((z_t,x_{t-1})\\) by \\(f_{\\varepsilon}(.;z_t,x_{t-1})\\). Hypothesis 3.2 (Measurement equations) The measurement equation is of the form: \\[\\begin{equation} y_t = A z_t + B x_t + \\eta_t, \\quad \\mbox{with } \\eta_t \\sim i.i.d. \\mathcal{N}(0,\\Omega).\\tag{3.9} \\end{equation}\\] Example 3.3 (Regime-Switching Gaussian VAR) Building on Example 1.13, we know that if \\[\\begin{equation} x_t = \\mu z_t + \\Phi x_{t-1} + \\varepsilon_t,\\tag{3.10} \\end{equation}\\] where \\(\\varepsilon_t|\\underline{x_t},z_t \\sim \\mathcal{N}(0,\\Sigma(z_t))\\) and if \\(z_t\\) is an exogenous independent Markov chain, then \\(w_t = (x_t&#39;,z_t&#39;)&#39;\\) is affine. Using the notations of Eq. (3.8), we have: \\[ m(z_t,x_{t-1}) = \\mu z_t + \\Phi x_{t-1}. \\] If \\(r_t\\) and the s.d.f. are respectively affine and exponential in \\(w_t\\), then, in particular, yields are also affine in \\(w_t\\), i.e. of the form \\(R(t,h)= A_h&#39;z_t + B_{h}&#39;x_t\\) (see Eq. (??)). Therefore, if the components of \\(y_t\\) are yields of different maturities, the measurement equations are consistent with Assumption 3.2. How to estimate such a model when the regimes \\(z_t\\) are unobservable? We distinguish two distinct situations: Case 1. The \\(x_t\\) factors are observable. The probabilities of being in the different regimes on each date can be estimated by employing the Kitagawa-Hamilton filter, with (using the notations of Proposition ??): \\(F_t = (y_t&#39;,x_t&#39;)&#39;\\) and \\[\\begin{eqnarray*} f(F_t|z_t=e_j,\\underline{F_{t-1}}) &amp;=&amp; f(y_t|x_t,z_t=e_j,\\underline{F_{t-1}})f(x_t|z_t=e_j,\\underline{F_{t-1}}) \\\\ &amp;=&amp; \\mathbf{n}(y_t;A z_t + B x_t,\\Omega) \\times \\\\ &amp;&amp;f_{\\varepsilon}(x_t - m(z_t,x_{t-1});z_t,x_{t-1}), \\end{eqnarray*}\\] where \\(\\mathbf{n}(u;\\mu,\\Omega)\\) denotes the evaluation, at vector \\(u\\) of the p.d.f. of the multivariate normal distribution \\(\\mathcal{N}(\\mu,\\Omega)\\) (see eq. (3.3)). A by-product of the Kitagawa-Hamilton filter is the likelihood function associated with the dataset. As a result, the model parameterization can be estimated by maximum llikelihood approach. Case 2. The \\(x_t\\) factors are not observable. There are two sub-cases: (2.i) the components of \\(y_t\\) are not perfectly modelled (i.e. \\(\\Omega \\ne 0\\), where \\(\\Omega\\) defined in Eq. (3.9). One has then to resort to filters dealing with two types of uncertainty: hidden discrete values (\\(z_t\\)) and continuously distributed latent variables (\\(x_t\\)). C.-J. Kim (1994)’s filter can be employed when the state-space model is of the form (3.9)-(3.10) (see, e.g., Alain Monfort and Renne (2014)), detailed in Example 4.2). (2.ii) \\(n_x\\) components of \\(y_t\\) are perfectly modelled (where \\(n_x\\) is the dimension of \\(x_t\\)). One can then resort to an inversion technique, complemented with the Kitagawa-Hamilton filter, to estimate the model (see Alain Monfort and Renne (2013) and Renne (2017) for applications). Proposition 3.3 (Kitagawa-Hamilton filter) Consider a \\(q\\)-dimensional vector of variables \\(F_t\\) and an exogenous homogenous Markov chain \\(z_t\\). We make use of the following notations: \\(\\eta_t\\) is a \\(J\\)-dimensional vector whose \\(j^{th}\\) component is the p.d.f. of \\(F_t\\) conditional on \\((z_t = e_j,\\underline{F_{t-1}})\\), i.e. \\(f(F_t|z_t=e_j,\\underline{F_{t-1}})\\) \\(\\xi_{t}\\) is a \\(J\\)-dimensional vector whose \\(j^{th}\\) component is \\(\\mathbb{P}(z_t = e_j|\\underline{F_t})\\). The sequence \\(\\xi_{t}\\) can then be computed recursively as follows: \\[\\begin{equation} \\xi_t = \\frac{(\\Pi&#39; \\xi_{t-1}) \\odot \\eta_t}{\\mathbf{1}&#39;(\\Pi&#39; \\xi_{t-1} \\odot \\eta_t)},\\tag{3.11} \\end{equation}\\] where \\(\\odot\\) denotes the element-by-element (Hadamard) product and where \\(\\mathbf{1}\\) denotes a \\(J\\)-dimensional vector of ones. Moreover, the previous formulas also show how to compute the likelihood of the model since: \\[\\begin{equation} f(F_t|\\underline{F_{t-1}})=\\mathbf{1}&#39;(\\Pi&#39; \\xi_{t-1} \\odot \\eta_t).\\tag{3.12} \\end{equation}\\] 3.5.2 Mixed use of Kitagawa-Hamilton and inversion techniques Without loss of generality, assume that the \\(n_x\\) first components of \\(y_t\\) are observed without error, i.e. Assumption 3.2 becomes \\[ \\left[ \\begin{array}{c} y_{1,t}\\\\ y_{2,t} \\end{array} \\right] = \\left[ \\begin{array}{c} A_{1}z_t\\\\ A_{2}z_t \\end{array} \\right]+ \\left[ \\begin{array}{c} B_{1}\\\\ B_{2} \\end{array} \\right]x_t + \\left[ \\begin{array}{c} 0\\\\ \\eta_{2,t} \\end{array} \\right], \\] where \\(\\varepsilon_2 \\sim \\mathcal{N}(0,\\Omega_2)\\). Since \\(y_{1,t} = A_1 z_t + B_1 x_t\\), we then have: \\[\\begin{equation} x_t \\equiv x(y_{t},z_t) = B_1^{-1}(y_{1,t} - A_1 z_t).\\tag{3.13} \\end{equation}\\] In order to employ the Kitagawa-Hamilton filter (Proposition 3.3), one need to define the extended Markov chain: \\[ \\mathcal{Z}_t = z_{t-1} \\otimes z_t, \\] whose matrix of transition probabilities is detailed in Proposition 3.4. Proposition 3.4 (Kitagawa-Hamilton and inversion techniques) The matrix of transition probabilities of \\(\\mathcal{Z}_t\\) is of the form \\(\\mathbf{1}_{n \\times 1} \\otimes \\widetilde{\\Pi}\\), with \\[ \\widetilde{\\Pi} = \\left[ \\begin{array}{ccccc} \\pi_{1,\\bullet} &amp; 0_{1 \\times n} \\dots &amp; &amp; &amp; 0_{1 \\times n} \\\\ 0_{1 \\times n} &amp; \\pi_{2,\\bullet} &amp; 0_{1 \\times n} &amp; \\dots &amp; 0_{1 \\times n} \\\\ &amp;&amp; \\ddots \\\\ &amp; &amp; 0_{1 \\times n} &amp; \\pi_{n-1,\\bullet} &amp; 0_{1 \\times n} \\\\ 0_{1 \\times n} &amp;\\dots &amp;&amp; 0_{1 \\times n} &amp; \\pi_{n,\\bullet} \\end{array} \\right], \\] where \\(\\pi_{i,\\bullet}\\) denotes the \\(i^{th}\\) row of \\(\\Pi\\) (\\(\\Pi\\) is defined on Slide XXX). The last term appearing in Eq. (3.14) can be computed as follows: \\[\\begin{eqnarray*} &amp;&amp;f\\left(\\left[\\begin{array}{c}x(y_t,z(\\mathcal{Z}_t))\\\\y_{2,t}\\end{array}\\right]|\\mathcal{Z}_t,\\underline{y_{t-1}}\\right) \\\\ &amp;=&amp; f\\left(y_{2,t}|x_t=x(y_t,z(\\mathcal{Z}_t)),\\mathcal{Z}_t,\\underline{y_{t-1}}\\right) \\times\\\\ &amp;&amp; f\\left(x(y_t,z(\\mathcal{Z}_t))|\\mathcal{Z}_t,\\underline{y_{t-1}}\\right) \\\\ &amp;=&amp; \\mathbf{n}(y_{2,t};A_2z_t + B_2x_t,\\Omega_2) \\times\\\\ &amp;&amp; f_\\varepsilon\\left(\\varepsilon_t|z_t = z(\\mathcal{Z}_t),x_{t-1}=x(y_{t-1},z_{-1}(y_{t-1},\\mathcal{Z}_t))\\right), \\end{eqnarray*}\\] where \\(\\mathbf{n}\\) is the p.d.f. of a multivariate normal distri, (Prop. 3.2, where \\(\\varepsilon_t = x_t - m[z(\\mathcal{Z}_t),x(y_{t-1},z_{-1}(y_{t-1},\\mathcal{Z}_t))]\\) (\\(m\\) defined in Eq. (3.8)). Note that we have: \\[ \\left\\{ \\begin{array}{cclll} z_t &amp;\\equiv&amp; z(\\mathcal{Z}_t) &amp;=&amp; (\\mathbf{1}&#39; \\otimes Id_{n}) \\mathcal{Z}_t \\\\ z_{t-1} &amp;\\equiv&amp; z_{-1}(\\mathcal{Z}_t) &amp;=&amp; (Id_{n} \\otimes \\mathbf{1}&#39;) \\mathcal{Z}_t. \\end{array} \\right. \\] The Kitagawa-Hamilton filter (Proposition 3.3) can then be employed, with \\(F_t = y_t\\) and: \\[\\begin{eqnarray} f(y_t|\\mathcal{Z}_t,\\underline{y_{t-1}}) &amp;=&amp;|B_1^{-1}| \\times \\nonumber \\\\ &amp;&amp; f\\left(\\left[\\begin{array}{c}x(y_t,z(\\mathcal{Z}_t))\\\\ y_{2,t}\\end{array}\\right]|\\mathcal{Z}_t,\\underline{y_{t-1}}\\right),\\tag{3.14} \\end{eqnarray}\\] where the computation of the last term is detailed in Proposition 3.4. Example 3.4 (Interest-rate model with monetary policy-related regimes) Renne (2017) porposes a model where regimes have monetary-policy interpretations. The model is estimated on daily data. The short-term rate is the euro-area overnight interbank rate (EONIA). It is modeled as follows: \\[\\begin{equation} \\boxed{r_{t}=\\underset{\\mbox{Target}}{\\underbrace{\\bar{r}_{t}}}+\\underset{\\mbox{EONIA spread}}{\\underbrace{x_{t}}}.} \\end{equation}\\] The target rate \\(\\bar{r}_{t}\\) has a step-like path \\(\\bar{r}_{t}=\\Delta&#39;z_{r,t}\\), where \\(\\Delta=[\\begin{array}{ccccc} 0 &amp; 0.25 &amp; 0.50 &amp; \\ldots &amp; \\bar{r}_{max}\\end{array}]&#39;\\) and \\(z_{r,t}=[\\begin{array}{ccccccc} 0 &amp; \\ldots &amp; 0 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0\\end{array}]&#39;\\) EONIA spread (\\(x_t\\)) persistent and mean-reverting fluctuations (AR process). \\(z_t = z_{r,t}\\otimes z_{m,t}\\) where \\(z_{m,t}\\) is the monetary-policy regime: Easing (\\(z_{m,t}=[\\begin{array}{ccc} 1 &amp; 0 &amp; 0\\end{array}]\\)), Status Quo (\\(z_{m,t}=[\\begin{array}{ccc} 0 &amp; 1 &amp; 0\\end{array}]\\)), Tightening (\\(z_{m,t}=[\\begin{array}{ccc} 0 &amp; 0 &amp; 1\\end{array}]\\)). One can simulate this model by using this web-interface. In terms of observability, \\(z_{r,t}\\) is observed (since the policy rate is osberved), but not \\(z_{m,t}\\). Hence, a filtering procedure is needed. Renne (2017) adapts the approach presented in Subsection 3.5.2 to the case where \\(z_{t}\\) is partially observed. In this model, the results of Example 3.3 imply that we have: \\[ R(t,h) = A_h&#39; z_t + B_h x_t. \\] Denote by \\(\\mathcal{A}_h\\) the \\((3 \\times n_r)\\) matrix such that \\(A_h = vec(\\mathcal{A}_h)\\). We then have \\(A_h&#39; z_t = (\\mathcal{A}_h z_{r,t})&#39; z_{m,t}\\) and, therefore: \\[ R(t,h) = A_{t,h}&#39; z_t + B_h x_t, \\quad \\mbox{where $A_{t,h} = \\mathcal{A}_h z_{r,t}$.} \\] The model is estimated by a combination of Kitagawa-Hamilton and inversion techniques, assuming that a linear combination of yields is modeled without errors (which gives \\(x_t = x(y_t,z_{m,t},z_{r,t})\\)). Figure 3.6: Source: Renne (2017). 3.6 A typical small-sample issue Interest rates are particularly persistent variables. Since affine models eventually lead to linear relationships between state variables and interest rates (see Eq. (??)), some state variables are also necessarily highly persistent. Accordingly, in small sample, maximum-likelihood estimates of the model parameters are likely to suffer from a downward bias (see, e.g., Michael D. Bauer and Wu (2012) or Jardet, Monfort, and Pegoraro (2013)). This relates to a well-known econometric problem illustrated by Figure 3.7. Figure 3.7: 1000 random walk samples of size \\(T\\) (\\(T=50\\) for the left plot and \\(T=400\\) for the right plot) have been simulated. For each sample, we run the OLS regression \\(y_t = \\phi y_{t-1} + \\varepsilon_t\\) to estimate \\(\\phi\\) (whose true value is 1). The plots show the distributions (kernel-based estimation) of the estimated \\(\\phi\\). The vertical red bar indicate the means of the distributions; the vertical blue line shows the usual bias approximation (\\(1-5.3/T\\)). This small-sample downward bias has dramatic consequences in terms of term premium estimates. For the sake of illustration, consider the following process for the short-term interest rate under the physical measure (monthly frequency): \\[ i_{t+1} = \\bar{i} + \\phi (i_{t}-\\bar{i}) + \\sigma \\varepsilon_{t+1}, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,1). \\] and the following under the risk-neutral measure: \\[ i_{t+1} = \\bar{i}^* + \\phi^* (i_{t}-\\bar{i}^*) + \\sigma \\varepsilon_{t+1}, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,1). \\] with \\(\\bar{i} = 3\\%/12\\), \\(\\bar{i}^* = 5\\%/12\\), \\(\\phi = 0.98\\), \\(\\phi^*=0.99\\), \\(\\sigma = 0.2\\%\\). Assume the estimate of \\(\\phi\\) is downward biased (\\(\\hat\\phi=0.9\\)). The influence of that bias on the 10-year term premium is illustrated by Figure 3.8. Figure 3.8: This figure illustrates the influence of the downward bias on on estimated term premiums, see the text for more details. D. H. Kim and Orphanides (2005) have proposed a simple approach to deal with this problem. Their approach consists in adding measurement equations to impose that the model-implied forecasts are—up to some measurement errors—equal to survey-based ones. In their empirical exercise, they use the Blue Chip Financial Forecasts. Alternative (publicly available) surveys are: the Philly Fed Survey of Professional Forecasters and the ECB SPF. D. H. Kim and Orphanides (2005) exploit the fact that, in the context of affine models, model-implied forecasts of yields are affine in the state vector (see see Eq. (??)); this implies that their mesurement equations are affine in the state vector, which facilitates the estimation of the latent factors. As shown by Figure 3.9, their model is able to satisfyingly fit both survey-based forecasts and market yields. Figure 3.9: Source: Kim and Orphanides (2005). References "],["credit-and-liquidity-risks.html", "Chapter 4 Credit and liquidity risks 4.1 Notations 4.2 Pricing under standard assumptions 4.3 Pricing illiquid bonds 4.4 Relaxing the classical framework assumptions 4.5 Top-down approach", " Chapter 4 Credit and liquidity risks 4.1 Notations The information of the investor at date \\(t\\) is still denoted by \\(\\underline{w_t} = (w_t&#39;,w_{t-1}&#39;,\\dots, w_1 &#39;) &#39;\\), but \\(w_t\\) now comprises a new subsector, namely \\(d_t\\), that keeps track of the “default” status of some creditors. Formally, \\(w_t = (y_t&#39;,d_t&#39;)&#39;\\), where \\(y_t\\) is a \\(n_y\\)-dimensional vector of common factors, and \\(d_t = (d_{1, t} , \\dots, d_{E, t}) &#39;\\) a \\(E\\)-dimensional vector of binary variables representing the possible default of entities \\(e \\in \\{1, \\dots,E\\}\\). Vector \\(w_t\\) is \\(K\\)-dimensional, that is \\(K = n_y + E\\). By convention: \\(d_{e, t} = 1\\) if entity \\(e\\) is in default status at time \\(t\\), \\(d_{e, t} = 0\\) if entity \\(e\\) is not in default status at time \\(t\\). We assume that the process \\(\\{w_t\\}\\) is Markov. Formally, its historical (\\(\\mathbb{P}\\)) dynamics is defined by the conditional densities: \\[ f (y_t | w_{t-1}) \\mbox{ and }p ( d_t | y_t, w_{t-1} ). \\] In what follows, we assume that the default state is absorbing: Hypothesis 4.1 (absorbing defaults) The default event is an absorbing state. Formally: \\(p_e ( 0 | y_t, w_{t-1}, d_{e,t-1}=1 ) = 0\\). The survival probability is defined as follows: \\[\\begin{equation} p_e ( 0 | y_t, w_{t-1}, d_{e,t-1}=0 ) = \\exp[-\\underbrace{\\lambda_e ( y_t, w_{t-1})}_{\\color{blue}{\\mbox{default intensity}}}].\\tag{4.1} \\end{equation}\\] It is easily checked that \\(\\lambda_e ( y_t, w_{t-1})\\) is close to the conditional probability of default when it is small. 4.2 Pricing under standard assumptions To start with, we will consider the pricing of defaultable bonds under assumptions that are standard in the literature, a framework that we call the classical credit-risk framework. These assumptions—namely 4.2 and 4.3—lead to simple formulas. We will investigate further what happens when these assumptions are relaxed (Subsection 4.4). Hypothesis 4.2 (Non-systemic entity) \\(\\{d_t\\}\\) does not Granger-cause \\(\\{y_t\\}\\), in the sense that: \\[ f(y_t|w_{t-1}) = f(y_t|y_{t-1}). \\] In other words, under Hypothesis 4.2, \\(y_t\\) is exogenous to \\(d_t\\). Hypothesis 4.3 (Unpriced credit events) The default events (or credit events) are not priced, in the sense that: \\[\\begin{equation*} \\mathcal{M}_{t-1, t}( w_t, w_{t-1}) = \\mathcal{M}_{t-1, t}( y_t, y_{t-1}) . \\end{equation*}\\] In other words, under Hypothesis 4.3, defaults do not affect the SDF (conditionally on \\(\\underline{y_t}\\)). Hypothesis 4.3 also implies that the short-term risk-free rate \\(r_t\\) (\\(=-\\log\\mathbb{E}_t(\\mathcal{M}_{t,t+1})\\)) depends on \\(\\underline{y_t}\\) (but not on \\(d_t\\)). Figure 4.1 provides a graphical representation of the causality scheme in the classical credit-risk framework. Figure 4.1: Graphical representation of the causality scheme in the classical credit-risk framework. Proposition 4.1 (Risk-neutral exogeneity of factors) Under Hypotheses 4.2 and 4.3, we have: \\[ f^{\\mathbb{Q}}(y_t|w_{t-1}) = f^{\\mathbb{Q}}(y_t|y_{t-1}), \\] that is, \\(\\{d_t\\}\\) does not Granger-cause \\(\\{y_t\\}\\) under the risk-neutral measure. Proof. We have: \\[\\begin{eqnarray} f^{\\mathbb{Q}}(y_t|w_{t-1}) &amp;=&amp; f(y_t|w_{t-1})\\mathcal{M}_{t-1,t}\\exp(r_{t-1})\\\\ &amp;=&amp; f(y_t|y_{t-1})\\mathcal{M}_{t-1,t}\\exp(r_{t-1}), \\end{eqnarray}\\] which gives the results, using that \\(\\mathcal{M}_{t-1,t}\\) and \\(\\exp(r_{t-1})\\) depend on \\(y_{t-1}\\) only (and not on \\(d_t\\)). To simplify, let us focus on a single entity (\\(d_t \\equiv d_{1,t}\\)), alive on date \\(t\\) (\\(d_t=0\\)). Consider the date-\\(t\\) price of a bond issued by the defaultable entity. The residual maturity of this bond is denoted by \\(h\\), and we consider a zero recovery rate (in case of default, the pyoff is zero). The price of this bond writes: \\[ B(t,h) = \\mathbb{E}_t^{\\mathbb{Q}}[\\exp(-r_t-\\dots-r_{t+h-1})(1-d_{t+h})]. \\] The following proposition is at the core of the classical credit-risk framework (Darrell Duffie and Singleton 1999): Proposition 4.2 (Defaultabe zero-coupon bond pricing in the classical credit-risk framework) Under Hypotheses 4.2 and 4.3, and with a default intensity defined—as in (4.1)—through: \\[\\begin{equation} \\exp[-\\lambda_t^{\\mathbb{P}}]:=\\mathbb{P} ( d_t=0 | \\underline{y_{t}}, d_{t-1}=0 ), \\end{equation}\\] we have: \\[\\begin{equation} \\lambda_t^{\\mathbb{Q}} = \\lambda_t^{\\mathbb{P}}\\;(=\\lambda_t),\\tag{4.2} \\end{equation}\\] and \\[\\begin{equation} \\boxed{B(t,h) = \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp\\left(-r_t-\\dots-r_{t+h-1}- \\lambda_{t+1} - \\dots - \\lambda_{t+h}\\right)\\right].}\\tag{4.3} \\end{equation}\\] which rewrites, using \\(\\tilde{r}_t = r_t + \\lambda_{t+1}\\): \\[\\begin{equation} B(t,h) = \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp\\left(-\\tilde{r}_t-\\dots-\\tilde{r}_{t+h-1}\\right)\\right],\\tag{4.4} \\end{equation}\\] Proof. We have (Eq. (2.6)): \\[\\begin{equation} f^{\\mathbb{Q}}(d_t,y_t|d_{t-1},y_{t-1}) = \\exp(r_{t-1})\\mathcal{M}_{t-1,t} f^{\\mathbb{P}}(d_t,y_t|d_{t-1},y_{t-1}).\\tag{4.5} \\end{equation}\\] Using Hypothesis 4.3, we obtain, by integrating both sides w.r.t. \\(d_t\\): \\[\\begin{eqnarray} f^{\\mathbb{Q}}(y_t|d_{t-1},y_{t-1}) &amp;=&amp; \\exp(r_{t-1})\\mathcal{M}_{t-1,t} f^{\\mathbb{P}}(y_t|d_{t-1},y_{t-1})\\nonumber\\\\ &amp;=&amp; \\exp(r_{t-1})\\mathcal{M}_{t-1,t} \\underbrace{f^{\\mathbb{P}}(y_t|y_{t-1})}_{\\mbox{by hypothesis}}. \\tag{4.6} \\end{eqnarray}\\] By Bayes, we have: \\[ f^{\\mathbb{Q}}(d_t|d_{t-1},y_t,y_{t-1}) = \\frac{f^{\\mathbb{Q}}(d_t,y_t|d_{t-1},y_{t-1})}{f^{\\mathbb{Q}}(y_t|d_{t-1},y_{t-1})}. \\] Using Eq. (4.5) (numerator) and Eq. (4.6) (denominator), we get: \\[\\begin{eqnarray*} &amp;&amp; f^{\\mathbb{Q}}(d_t|d_{t-1},y_t,y_{t-1}) = f^{\\mathbb{P}}(d_t|d_{t-1},y_t,y_{t-1}), \\end{eqnarray*}\\] which gives (4.2). Conditioning w.r.t. \\(\\underline{y_{t+h}}\\), the bond price is given by: \\[\\begin{eqnarray*} &amp;&amp; \\mathbb{E}_t^{\\mathbb{Q}}[\\mathbb{E}_t^{\\mathbb{Q}}\\{\\exp(-r_t-\\dots-r_{t+h-1})(1-d_{t+h})|\\underline{y_{t+h}}\\}]\\\\ &amp;=&amp; \\mathbb{E}_t^{\\mathbb{Q}}[\\exp(-r_t-\\dots-r_{t+h-1})\\mathbb{E}_t^{\\mathbb{Q}}\\{(1-d_{t+h})|\\underline{y_{t+h}}\\}]\\\\ &amp;=&amp; \\mathbb{E}_t^{\\mathbb{Q}}[\\exp(-r_t-\\dots-r_{t+h-1})\\mathbb{Q}\\{d_{t+h}=0|\\underline{y_{t+h}}\\}]\\\\ &amp;=&amp; \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp(-r_t-\\dots-r_{t+h-1})\\prod_{j=1}^h\\mathbb{Q}\\{d_{t+j}=0|d_{t+j-1}=0,\\underline{y_{t+h}}\\}\\right], \\end{eqnarray*}\\] Because Granger and Sims non-causalities are equivalent, and since \\(d_t\\) does not Granger-cause \\(y_t\\) under the risk-neutral measure (see Proposition 4.1), we have: \\[ \\mathbb{Q}\\{d_{t+j}=0|d_{t+j-1}=0,\\underline{y_{t+{\\color{red}h}}}\\} = \\underbrace{\\mathbb{Q}\\{d_{t+j}=0|d_{t+j-1}=0,\\underline{y_{t+{\\color{red}j}}}\\}}_{=\\exp(-\\lambda_{t+j}^{\\mathbb{Q}})}. \\] As a result: \\[\\begin{equation} B(t,h) = \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp\\left(-r_t-\\dots-r_{t+h-1}- \\lambda_{t+1}^{\\mathbb{Q}} - \\dots - \\lambda_{t+h}^{\\mathbb{Q}}\\right)\\right].\\tag{4.7} \\end{equation}\\] Using (4.2) in (4.7), we obtain (4.3). Although \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\) intensities are the same functions of \\(y_t\\) and \\(y_{t-1}\\), their historical and risk-neutral dynamics are in general different since \\(y_t\\)’s \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\) dynamics are different. Eq. (4.4) is reminiscent of Eq. (1.10), where the risk-free short-term rate \\(r_t\\) is replaced by the credit-adjusted short-term rate \\(\\tilde{r}_t\\). Importantly, if both \\(r_t\\) and \\(\\lambda_t\\) are affine in \\(y_t\\), where the latter is an affine process, then bond prices are easily computed by means of recursive formulas (as was the case in the context of Eq. (1.10), see Example 1.18. In this case, \\(B(t,h)\\) is exponential affine in \\(y_t\\), that is, it is of the form: \\[\\begin{equation} B(t,h) = \\exp({A^d_h}&#39;y_t + B^d_h)\\quad (say).\\tag{4.8} \\end{equation}\\] The previous framework can be adapted in order to accommodate non-zero recovery rates. In this framework, the default intensity will be replaced by a pseudo, or recovery-adjusted default intensity denoted by \\(\\tilde\\lambda_t\\). To go further, we need to specifiy the payoff taking place upon default. There exist different modelling conventions for that; one is the so-called Recovery of Market Value (RMV) (Darrell Duffie and Singleton 1999). Loosely speaking, under the RMV, the recovery payoff is a fraction \\(\\zeta \\in [0,1]\\), called Recovery Rate (RR) of the price that would have prevailed, absent the default. Proposition 4.3 makes this definition more precise and gives the bond price stemming from it. Proposition 4.3 (Bond pricing in the RMV classical framework) Under Hypotheses 4.2 and 4.3, and if, in case of default on date \\(t+i\\), the recovery payment is \\(\\zeta \\widetilde{B}(t+i,h-i)\\), with \\(\\zeta \\in [0,1]\\), then \\[\\begin{equation} B(t,h) = (1-d_t)\\widetilde{B}(t,h),\\tag{4.9} \\end{equation}\\] where \\(\\widetilde{B}(t,h)\\) is a pseudo-price given by: \\[\\begin{equation} \\widetilde{B}(t,h) = \\mathbb{E}_t^{\\mathbb{Q}} [\\exp(-r_{t}-\\tilde\\lambda_{t+1}-\\dots -r_{t+h-1}-\\tilde\\lambda_{t+h}],\\tag{4.10} \\end{equation}\\] where the pseudo-intensity \\(\\tilde\\lambda_{t}\\) is defined by: \\[\\begin{eqnarray} \\exp(-\\tilde\\lambda_{t+1}) &amp;=&amp; \\mathbb{E} [\\{1 - d_{t+1} (1-\\zeta)\\}|\\underline{y_{t+1}},d_t=0] \\nonumber\\\\ &amp;=&amp; 1 - (1-\\zeta) (1-\\exp(-\\lambda_{t+1})).\\tag{4.11} \\end{eqnarray}\\] Proof. Relation (4.9) is true for \\(B(t+h, 0)\\) since \\(\\widetilde{B}(t+h, 0) = 1\\) and \\(B(t+h, 0) = 1 - d_{t+h}\\). Assuming that \\(B(t+i+1, h-i-1) = (1 - d_{t+i+1} ) \\widetilde{B}(t+i+1, h-i-1)\\) (which is valid for \\(i=h-1\\)), we get: \\[\\begin{eqnarray*} B(t+i, h-i) &amp;=&amp; \\left(1 - d_{t+i} \\right) \\mathbb{E}_{t+i}^{\\mathbb{Q}} \\left\\{ \\exp(- r_{t+i}) \\left[ \\left(1 - d_{t+i+1} \\right) \\widetilde{B}(t+i+1, h-i-1) \\right. \\right. \\\\ &amp;&amp; \\left. \\left. + d_{t+i+1}\\zeta\\widetilde{B}(t+i+1, h-i-1) \\right] \\right\\}, \\end{eqnarray*}\\] since, in case of no default at date \\(t+i\\), the value of the bond at date \\(t+i+1\\) is either \\(\\zeta B(t+i+1, h-i-1) = \\zeta \\widetilde{B}(t+i+1, h-i-1)\\) if default happens, and \\(B(t+i+1, h-i-1) = \\widetilde{B}(t+i+1, h-i-1)\\) otherwise. Hypotheses 4.2 and 4.3 imply that \\(\\widetilde{B}(t+i+1, h-i-1)\\) does not depend on \\(d_{t+i+1}\\) and taking first the conditional expectation given \\(w_{t+i}\\) and \\(y_{t+i+1}\\) we obtain: \\[\\begin{equation} \\begin{array}{lll} B(t+i, h-i) &amp;=&amp; \\left(1 - d_{t+i} \\right) \\mathbb{E}_{t+i}^{\\mathbb{Q}} \\left\\{ \\exp(- r_{t+i}) \\widetilde{B}(t+i+1, h-i-1) \\times \\right. \\\\ &amp;&amp; \\quad\\left. \\mathbb{E}^{\\mathbb{Q}} \\left[ \\left(1 - d_{t+i+1} \\right) + \\zeta d_{t+i+1} | y_{t+i+1}, w_{t+i} \\right] \\right\\} . \\end{array}\\tag{4.12} \\end{equation}\\] Using the definition of \\(\\widetilde{\\lambda}_{t}\\) (Eq. (4.11)) we get: \\[\\begin{equation*} \\begin{array}{lll} B(t+i, h-i) &amp;=&amp; \\left(1 - d_{t+i} \\right) \\mathbb{E}_{t+i}^{\\mathbb{Q}} \\left\\{ \\exp \\left( - r_{t+i} - \\widetilde{\\lambda}_{t+i+1} \\right) \\widetilde{B}(t+i+1, h-i-1) \\right\\}\\\\ &amp;=&amp; \\left(1 - d_{t+i} \\right) \\widetilde{B}(t+i, h-i) . \\end{array} \\end{equation*}\\] Therefore it is also true for \\(i=0\\). Again, if \\(r_t\\) and \\(\\tilde\\lambda_t\\) are affine combinations of an affine process \\(y_t\\), then (exponential affine) bond prices are easily computed by means of recursive formulas. This is formalized below. Hypothesis 4.4 (Affine process) The process \\(\\{ y_t \\}\\) is affine under the \\(\\mathbb{Q}\\) measure: \\[\\begin{equation} \\begin{array}{lll} \\varphi^{\\mathbb{Q}}_{y, t-1} (u_y) = \\mathbb{E}^{\\mathbb{Q}} \\left[\\exp( u_y &#39; y_t ) | \\underline{y_{t-1}} \\right] = \\exp \\left[ a^{\\mathbb{Q}}_{y} (u_{y}) &#39; y_{t-1} + b^{\\mathbb{Q}}_{y} (u_{y}) \\right], \\end{array}\\tag{4.13} \\end{equation}\\] and \\(r_t\\) and \\(\\widetilde{\\lambda}_{e, t}\\) are affine in \\((y_t, y_{t-1})\\), that is (say): \\[\\begin{eqnarray} r_{t} &amp;=&amp; \\omega_0 + \\omega_{1} &#39; y_{t} \\\\ \\widetilde{\\lambda}_{e, t} &amp;=&amp; \\kappa_{0} + \\kappa_{1}&#39; y_t. \\end{eqnarray}\\] Proposition 4.4 (Standard affine bond pricing, under RMV convention) Under the Assumptions of Proposition 4.3 and under Hypothesis 4.4, we have, for \\(h \\ge 1\\): \\[\\begin{equation} B(t, h) = \\left(1 - d_{t} \\right)\\exp(- h[\\omega_0 + \\kappa_{0}] - \\omega_{1} &#39; y_{t} + A_h&#39;y_t + B_h),\\tag{4.14} \\end{equation}\\] where \\(A_h\\) and \\(B_h\\) are given by the following recursive equations: \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} A_{h} &amp;=&amp; a(u_{h} + A_{h-1}), \\\\ B_{h} &amp;=&amp; b(u_{h} + A_{h-1}) + B_{h-1}, \\\\ A_{0} &amp;=&amp; 0,\\quad B_{0} = 0, \\end{array} \\right.\\tag{4.15} \\end{equation}\\] with \\(u_1 = -\\kappa_{1}\\) and \\(u_i = -(\\kappa_{1} + \\omega_{1})\\) for \\(i&gt;1\\). Proof. The results directly follows from Proposition 4.3. What precedes has been estiablished under the assumption that the state vector \\(w_t\\) comprises a single defaultable entity (\\(E=1\\)). What about when different entities are considered (\\(E = 2\\))? We can demonstrate that similar pricing formulas are obtained under the assumption of no-contagion (Hypothesis 4.5): Hypothesis 4.5 (no contagion) There is no instantaneous or lagged contagion between entities, i.e.: \\[\\begin{equation*} p ( d_t | y_t, w_{t-1} ) = p_1 ( d_{1, t} | y_t, y_{t-1}, d_{1, t-1}) \\times p_2 ( d_{2, t} | y_t, y_{t-1}, d_{2, t-1}) . \\end{equation*}\\] If Hypothesis 4.5 is not satisfied, for instance, if we have contagion effect from \\(e = 1\\) towards \\(e = 2\\): \\[\\begin{eqnarray*} p(d_t|y_t, w_{t-1} ) &amp;=&amp; p_1 ( d_{1, t} | y_t, y_{t-1}, d_{1, t-1}) \\times \\\\ &amp;&amp; p_2 ( d_{2, t} | y_t, y_{t-1}, \\color{blue}{d_{1, t}}, \\color{blue}{d_{1, t-1}}, d_{2, t-1}), \\end{eqnarray*}\\] then: Proposition 4.3 is still valid for both entities; In an affine framework, the computation of \\(\\widetilde{B}_1 (t, h)\\) (and \\(B_1 (t, h)\\)) is straightforward; but formulas for \\(\\widetilde{B}_2 (t, h)\\) and \\(B_2 (t, h)\\) are not explicit anymore even if \\(\\widetilde{\\lambda}_{2, t}\\) is affine in \\((y_t, y_{t-1}, d_{1, t}, d_{1, t-1})\\). Proposition 4.5 (Absence of contagion) Under Hypotheses 4.1, 4.2, 4.3 and 4.5, the risk-neutral (\\(\\mathbb{Q}\\)) dynamics is such that: \\[ f^{\\mathbb{Q}} ( y_t | w_{t-1}) = f^{\\mathbb{Q}} ( y_t | y_{t-1}) \\propto \\mathcal{M}_{t-1, t}( y_t, y_{t-1}) f (y_t | y_{t-1}) \\] (exogeneity of \\(y_t\\) preserved under \\(\\mathbb{Q}\\)) and \\[ p_t^{\\mathbb{Q}} ( d_{t} | y_t, w_{t-1} ) = p_1 ( d_{1, t} | y_t, y_{t-1}, d_{1, t-1} ) \\times p_2 ( d_{2, t} | y_t, y_{t-1}, d_{2, t-1}), \\] (absence of contagion preserved under \\(\\mathbb{Q}\\)). Denoting by \\(\\lambda^{\\mathbb{Q}}_e \\left( y_t, y_{t-1} \\right) \\equiv - \\log \\left[ p_e^{\\mathbb{Q}} ( 0 | y_t, y_{t-1}, 0 ) \\right]\\) the risk-neutral default intensity of entity \\(e\\), it comes that: \\[ \\lambda^{\\mathbb{Q}}_e \\left( y_t, y_{t-1} \\right) = \\lambda_e \\left( y_t, y_{t-1} \\right) \\] (default intensities are the same under \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\)). Proof. The \\(\\mathbb{Q}\\) conditional density of \\(w_t\\) given \\(w_{t-1}\\), namely \\(f^{\\mathbb{Q}} (y_t | w_{t-1}) p^{\\mathbb{Q}} ( d_t | y_t, w_{t-1} )\\), is proportional to \\(\\mathcal{M}_{t-1, t}( w_t, w_{t-1}) f (y_t | w_{t-1}) p ( d_t | y_t, w_{t-1} )\\), and thus proportional to \\(\\mathcal{M}_{t-1, t}( y_t, y_{t-1}) f (y_t | y_{t-1}) p_1 ( d_{1, t} | y_t, y_{t-1}, d_{1, t-1}) p_2 ( d_{2, t} | y_t, y_{t-1}, d_{2, t-1})\\). The result follows immediately. 4.3 Pricing illiquid bonds This subsection shows that a similar approach can be used to price illiquid assets. The idea consists in replacing the Loss Given Default (\\(LGD = 1-RR^{(e)}\\)) by a Loss Given Liquidity Shock (\\(1-\\theta_\\ell^{(e)}\\)). A structural interpretation of this approach is provided by Ericsson and Renault (2006) (see Example 4.1). Let us introduce a novel binary variable: \\(\\ell_t\\), that represents a liquidity shocks. When a bondholder is hit by the liquidity shock, she needs to liquidate her portfolio (reflecting cash constraints or needs to rebalance portfolio). But in such distressed conditions, she will sell her bonds at a discount (and will therefore face losses, akin to credit losses). Formally: \\(\\ell_{t}=1\\) when the bondholder is hit, and \\(\\ell_{t}=0\\) otherwise. The liquidity-shock intensity \\(\\lambda_\\ell(y_t)\\)—that defines the probability of occurence of liquidity shocks—is defined through: \\[ \\underbrace{1- \\exp[-\\lambda_\\ell(y_t)]}_{\\approx \\lambda_\\ell(y_t)\\mbox{ if small}} = \\mathbb{P}(\\ell_t = 1|\\underline{w_{t-1}},y_t). \\] The binary variable \\(\\ell_t\\) is assumed not to Granger-cause \\(y_t\\) (as in Hypothesis 4.2) and not to be priced (as in Hypothesis 4.3). As in Proposition 4.2 (and more precisely Eq. (4.2)), this implies that \\[ \\lambda_\\ell^{\\mathbb{Q}}(y_t) = \\lambda_\\ell(y_t), \\] where \\[ 1- \\exp[-\\lambda^{\\mathbb{Q}}_\\ell(y_t)] = \\mathbb{Q}(\\ell_t = 1|\\underline{w_{t-1}},y_t). \\] Upon the arrival of the liquidity shock (\\(\\ell_{t}=1\\)), the investor has to exit by selling her bond holdings. She is then limited in the number of traders she can call and, as a result, get only a fraction (\\(\\theta_\\ell^{(e)}\\), say) of the price she would get in normal-liquidity times, i.e., when \\(\\ell_{t}=0\\) (see the structural interpretation in Example 4.1). In that context, following the approach used in Proposition 4.3, one can define a pseudo-illiquidity intensity as follows (mimicking Eq. (4.11)): \\[\\begin{eqnarray} \\widetilde{\\lambda}_{e,\\ell, t} \\left( y_t, y_{t-1} \\right) &amp;=&amp; - \\log \\left\\{ \\exp \\Big[ - \\lambda_{e,\\ell} \\left( y_t\\right) \\Big] \\right. + \\tag{4.16}\\\\ &amp;&amp; \\left. \\Big( 1 - \\exp \\Big[ - \\lambda_{e,\\ell} \\left( y_t \\right) \\Big] \\Big) \\theta_\\ell^{(e)} \\right\\} \\nonumber \\end{eqnarray}\\] Still consiering credit risk (on top of liquidity risk), we then have the following payoff on date \\(t\\) (assuming no default on date \\(t-1\\)): \\[\\begin{equation} \\widetilde{B}_e (t, h) = \\mathbb{E}^{\\mathbb{Q}}_t \\left\\{ \\exp \\left[ - \\sum_{i=1}^{h} \\left( r_{t+i-1} + \\widetilde{\\lambda}_{e,c, t+i} + \\widetilde{\\lambda}_{e,\\ell, t+i} \\right) \\right] \\right\\}.\\tag{4.17} \\end{equation}\\] Example 4.1 (Structural interpretation of the illiquidity intensity) This structural interpretation of the illiquidity intensity is due to Ericsson and Renault (2006). When hit by a liquidity shock, a bondholder has to liquidate her bond in a small amount of time (between \\(t\\) and \\(t+\\varepsilon\\), with \\(\\varepsilon \\ll 1\\)). She collects offers on the market; each offer is a random fraction \\(\\omega_{i}\\) (\\(i\\in\\{1,\\ldots,\\kappa\\}\\)) of \\(\\widetilde{B}_e (t, h)\\) (see Eq. (4.17)), where the \\(\\omega_{i}\\)’s are uniformly distributed in \\([0,1]\\). At \\(t+\\varepsilon\\), the bond is sold to the trader that has offered the highest price. Formally, when \\(\\ell_{t}=1\\), the selling price is: \\[ \\left(\\max_{i\\in\\{1,\\ldots,\\kappa\\}}\\omega_{i}\\right)\\tilde{P}_{t,h}^{(e)}. \\] Conditional on \\(\\kappa\\), the expectation of \\(\\max_{i}(\\omega_{i})\\) is \\(\\kappa/(\\kappa+1)\\). (Because the \\(\\omega_{i}\\) are i.i.d., the c.d.f. of \\(\\max(\\omega_{1},\\ldots,\\omega_{\\kappa})\\) is \\(x\\mapsto F(x)^{\\kappa}\\), where \\(F\\) is the c.d.f. of \\(\\omega_{i}\\).) Hence, \\(\\theta_\\ell^{(e)}\\)—the unconditional expectation of \\(\\max_{i}(\\omega_{i})\\)—is given by: \\[\\begin{eqnarray*} \\sum_{k=1}^{\\infty}\\frac{e^{-\\varepsilon\\gamma^{(e)}}k}{k+1}\\frac{\\left(\\varepsilon\\gamma^{(e)}\\right)^{k}}{k!} &amp; = &amp; \\left[\\left(1-e^{-\\varepsilon\\gamma^{(e)}}\\right)\\frac{\\varepsilon\\gamma^{(e)}-1}{\\varepsilon\\gamma^{(e)}}+e^{-\\varepsilon\\gamma^{(e)}}\\right]\\\\ &amp; = &amp; g(\\varepsilon\\gamma^{(e)})\\qquad\\mbox{(say).} \\end{eqnarray*}\\] Function \\(g\\) is increasing and valued in \\([0,1]\\). In addition \\(\\lim_{x\\rightarrow\\infty}g(x)\\rightarrow1\\). Example 4.2 (Euro area sovereign credit spreads) A. Monfort and Renne (2011) and Alain Monfort and Renne (2014) propose credit/liquidity decompositions of euro-area sovereign spreads. They emply different estimation approaches: C.-J. Kim (1994)’s filter for MR2014, and Kitagawa-Hamilton filter for A. Monfort and Renne (2011) (see Subsection 3.5.1). In both papers, the \\(y_t\\) part of the state vector \\(w_t\\) is of the form \\(y_t = (x_t&#39;,z_t&#39;)&#39;\\); it follows a Regime-Switching Gaussian VAR (see Example 3.3). The time-homogenous Markov chain \\(z_t\\) tracks the crisis status (crisis versus normal times). The total intensity is of the form: \\[ \\widetilde{\\lambda}_{e,t} = \\underbrace{\\alpha_{e,c}&#39;z_t + \\beta_{e,c}&#39;y_t}_{\\mbox{credit intensity $\\widetilde{\\lambda}_{e,c,t}$}} + \\underbrace{\\alpha_{e,\\ell}&#39;z_t + \\beta_{e,\\ell}&#39;y_t.}_{\\mbox{liquidity intensity $\\widetilde{\\lambda}_{e,\\ell,t}$}} \\] In both papers, the identification of credit versus liquidity components is based on: \\[ \\mbox{KfW-Bund spreads} = \\mbox{liquidity spreads}. \\] KfW (Kreditanstalt für Wiederaufbau) is a German agency whose bonds are guaranteed by the Federal Republic of Germany. Therefore, they benefit from the same credit quality than their sovereign counterparts—the Bunds—but are less liquid; accordingly, the KfW-Bund spread should beessentially liquidity-driven. In Alain Monfort and Renne (2014), we have \\[ \\widetilde{\\lambda}_{e,t} = \\underbrace{x_{e,t}}_{=\\widetilde{\\lambda}_{e,c,t}} + \\underbrace{(1-\\theta^{(e)}_\\ell)x_{E+1,t}}_{=\\widetilde{\\lambda}_{e,\\ell,t}}, \\] hence \\(x_t\\) of dimension \\(E+1 = 12\\) (11 countries are involved). There are 9 Markovian regimes: \\(z_t = z_{c,t} \\otimes z_{\\ell,t}\\), where \\(z_{i,t}\\) (\\(i \\in \\{c,\\ell\\}\\)) is 3-dimensional: \\(z_{i,t}=[1,0,0]&#39;\\): low stress, \\(z_{i,t}=[0,1,0]&#39;\\): medium stress, \\(z_{i,t}=[0,0,1]&#39;\\): high stress. \\(z_{c,t}\\) and \\(z_{\\ell,t}\\) are not independent: for instance, the probability to transit from the low-credit-stress regime to the medium/high-credit-stress regimes depends on the liquidity-stress regime. For each intensity \\(x_{i,t}\\) (simulation below XXX): \\[ x_{i,t} = \\mu_i&#39;z_t + \\phi x_{i,t-1} + \\sigma_i \\varepsilon_{i,t}, \\] with \\(\\varepsilon_{t} \\sim i.i.d. \\mathcal{N}(0,\\Sigma)\\), where \\(\\Sigma\\) is diagonal, and \\(\\mu_{i,1} &lt; \\mu_{i,2} &lt; \\mu_{i,3}\\). The Markov chain \\(z_t\\) is the only source of correlation across the \\(x_{i,t}\\)’s. Figure 4.2 shows a simulation of such a process, with: \\[ \\mu=\\left[\\begin{array}{c} 0.01 \\\\ 0.03 \\\\ 0.10\\end{array}\\right],\\quad \\phi=.8 ,\\quad P=\\left[\\begin{array}{ccc} 0.98 &amp; 0.02 &amp; 0\\\\ 0.05 &amp; 0.90 &amp; 0.05\\\\ 0 &amp; 0.20 &amp; 0.80 \\end{array}\\right],\\quad \\sigma = 0.002. \\] The SDF exponential affine in \\((z_t&#39;,x_t&#39;)&#39;\\). We have the same type of dynamics under \\(\\mathbb{Q}\\) (with risk-asjusted parameters, in particular risk-adjusted transition probabilities). Figure 4.2: Simulation of an process following \\(x_{t} = \\mu&#39;z_t + \\phi x_{t-1} + \\sigma \\varepsilon_{t}\\), where \\(z_t\\) follows a three-state time-homogenous Markov process. See the text for the exact parameterization. The light-grey shaded area corresponds to the second regime (\\(z_t = [0,1,0]&#39;\\)); the dark-grey shaded area corresponds to the third regime (\\(z_t = [0,0,1]&#39;\\)) Example 4.3 (IBOR-OIS spreads) Dubecq et al. (2016) decompose the EURIBOR versus Overnight Index Swap (OIS) spreads into a credit and a liquidity component. (See Subsection ?? for the definition of a swap contract.) From the point of view of bank \\(j\\), the EURIBOR rate is the rate of an unsecured loan to one bank (\\(e\\), say) of the panel. Assume that the panel of the \\(N\\) constituting the EURIBOR are homogenous. We have: \\[\\begin{equation} R^{IBOR}_{t,h} = - \\frac{1}{h} \\log \\mathbb{E}^{\\mathbb{Q}}_t \\left\\{ \\exp \\left[ - \\sum_{i=1}^{h} \\left( r_{t+i-1} + \\widetilde{\\lambda}_{e,c, t+i} + \\widetilde{\\lambda}_{j,\\ell, t+i} \\right) \\right] \\right\\}.\\tag{4.18} \\end{equation}\\] Note that the liquidity intensity refers to the lending bank (\\(j\\)) and the default intensity relates to the borrowing bank (\\(e\\)). By constrast, the Overnight-Index Swap (OIS) rate satisfies:7 \\[\\begin{equation} R^{OIS}_{t,h} = - \\frac{1}{h} \\log \\mathbb{E}^{\\mathbb{Q}}_t \\left\\{ \\exp \\left( - \\sum_{i=1}^{h} r_{t+i-1} \\right) \\right\\}.\\tag{4.19} \\end{equation}\\] Assuming that \\(r_t\\), the short-term risk-free rate (i.e., the reference rate of the OIS, EONIA here) is independent from the intensities and under the homogeneity assumption, we obtain:8 \\[ R^{IBOR}_{t,h} - R^{OIS}_{t,h} = - \\frac{1}{h} \\log \\mathbb{E}^{\\mathbb{Q}}_t \\left\\{ \\exp \\left[ - \\sum_{i=1}^{h} \\left( \\widetilde{\\lambda}_{c, t+i} + \\widetilde{\\lambda}_{\\ell, t+i} \\right) \\right] \\right\\}. \\] Dubecq et al. (2016) employ a quadratic specification for \\(\\widetilde{\\lambda}_{c, t}\\) and \\(\\widetilde{\\lambda}_{\\ell, t}\\): \\[ \\widetilde{\\lambda}_{c, t} = x_{c,t}^2 \\quad \\mbox{and} \\quad \\widetilde{\\lambda}_{\\ell, t} = x_{\\ell,t}^2, \\] where \\(X_t = (x_{c,t},x_{\\ell,t})&#39;\\) follows a Gaussian VAR(1) process. (This is the quandratic Gaussian framework presented in Example 1.7.) The state vector \\(X_t\\) is latent. The estimation involves the Quadratic Kalman Filter (Alain Monfort, Renne, and Roussellet 2015). The measurement equations include equations stating that observed proxies of credit and liquidity risks relate to quadratic functions of \\(x_{c,t}\\) and \\(x_{\\ell,t}\\), respectively. Figure 4.3: Source: Dubecq et al. (2016). 4.4 Relaxing the classical framework assumptions Formula (4.14) and its tractability is a key feature of numerous credit-risk term-structure models, e.g., D. Duffie and Singleton (2003), Pan and Singleton (2008), and Longstaff et al. (2011) among many others. This formula is valid under Assumptions 4.2 of non-systemic entities, also called no-jump condition, 4.3 of unpriced credit risk events, and 4.5 absence of contagion. These hypotheses define what we have called the classical credit-risk models. We lose the high degree of tractability of Formula (4.14) when some of the previous assumptions are relaxed: If \\(f (y_t | w_{t-1}) = f (y_t | y_{t-1} , d_{1, t-1})\\) (entity \\(e = 1\\) is systemic) then: The pseudo-price \\(\\widetilde{B}_e (t, h)\\) now depends on \\(y_t\\) and \\(d_{1, t}\\); Eq. (4.3) is valid for entity \\(e = 2\\) only; The computation of \\(\\widetilde{B}_2 (t, h)\\) is not straightforward: \\(\\{ y_t \\}\\) is not autonomous and the autonomous process \\(\\{ y_t, d_{1,t} \\}\\) is not affine. If there is a contagion effect from \\(e = 1\\) towards \\(e = 2\\), i.e., if \\[\\begin{eqnarray*} p ( d_t | y_t, w_{t-1} ) &amp;=&amp; p_1 ( d_{1, t} | y_t, y_{t-1}, d_{1, t-1}) \\\\ &amp;&amp; p_2 ( d_{2, t} | y_t, y_{t-1}, d_{1, t}, d_{1, t-1}, d_{2, t-1}), \\end{eqnarray*}\\] then: Eq. (4.3) is still valid for both entities; In an affine framework, the computation of \\(\\widetilde{B}_1 (t, h)\\) (and \\(B_1 (t, h)\\)) is straightforward; but formulas for \\(\\widetilde{B}_2 (t, h)\\) and \\(B_2 (t, h)\\) are not explicit anymore even if \\(\\widetilde{\\lambda}_{2, t}\\) is affine in \\((y_t, y_{t-1}, d_{1, t}, d_{1, t-1})\\). If the default event of the first entity only (\\(d_{1,t}\\)) is a source of risk that is priced: \\(\\mathcal{M}_{t-1, t}( w_t, w_{t-1}) = \\mathcal{M}_{t-1, t}( y_t, y_{t-1}, d_{1, t}, d_{1, t-1})\\), then \\(\\lambda^{\\mathbb{Q}}_{1, t} \\neq \\lambda_{1, t}\\) and \\(\\lambda^{\\mathbb{Q}}_{2, t} = \\lambda_{2, t}\\); the exogeneity of \\(\\{ y_t \\}\\) is no longer preserved under \\(\\mathbb{Q}\\); Eq. (4.3) is no longer valid for entity \\(e = 1\\). It remains valid for \\(e=2\\) but the computation of \\(\\widetilde{B}_{2} (t, h)\\) is not straightforward: even if \\(y_t\\) is \\(\\mathbb{P}\\)-autonomous, this is not true under \\(\\mathbb{Q}\\). 4.4.1 General affine credit-risk framework Exploiting Vector Auto-Regressive Gamma (VARG) processes, Alain Monfort et al. (2021) propose a general affine credit-risk pricing model jointly allowing for: systemic entities (breaking down the no-jump condition; see Collin-Dufresne, Goldstein, and Hugonnier (2004)); contagion effects between entities (economic/financial linkages; see Ait-Sahalia, Laeven, and Pelizzon (2014)); pricing of credit events (credit spread puzzle; see C. Gourieroux, Monfort, and Renne (2014)); and stochastic recovery rates (RR) (Altman et al. 2005). In this general framework, the state vector \\(w_t\\) is of the form \\([y_t&#39;,\\delta_t&#39;]&#39;\\), where \\(\\delta_t\\) is a \\(E\\)-dimensional vector of credit-event variables (see Hypothesis 4.6). Figure 4.4: Schematic comparison of the classical and general credit-risk frameworks. Hypothesis 4.6 (Credit events) The default date \\(\\tau^{(e)}\\) (say) of any entity \\(e\\) is defined as: \\[\\begin{equation} \\tau^{(e)} = \\inf \\left\\{ t &gt; 0 : \\delta^{(e)}_t &gt; 0 \\right\\},\\tag{4.20} \\end{equation}\\] where \\(\\delta^{(e)}_t\\) is a non-negative variable called credit-event variable. The default indicator function can be equivalently written as \\(d^{(e)}_t = \\textbf{1}_{ \\{ \\tau^{(e)} \\leq t \\} }\\) or \\(d^{(e)}_t = 1 - \\textbf{1}_{ \\{ \\underline{\\delta^{(e)}_t} &#39; {\\bf 1} = 0 \\} }\\), with \\(\\underline{\\delta^{(e)}_t} = (\\delta^{(e)}_t, \\ldots, \\delta^{(e)}_1)\\) and where \\({\\bf 1} = (1, \\ldots, 1)&#39;\\) with conformable dimension. Hypothesis 4.7 (State-vector dynamics) The stochastic process \\(\\{w_t\\}\\) is affine under the historical probability measure \\(\\mathbb{P}\\). The historical Laplace transform of \\(w_t\\), conditional \\(\\underline{w}_{t-1}\\), is denoted by: \\[\\begin{eqnarray} \\varphi^{\\mathbb{P}}_{w, t-1} (u_w) &amp;=&amp; \\mathbb{E} \\left[\\exp( u_w &#39; w_t ) | \\underline{w_{t-1}} \\right] \\nonumber \\\\ &amp;=&amp; \\exp \\left[ a_{w} (u_{w}) &#39; w_{t-1} + b_{w} (u_{w}) \\right],\\tag{4.21} \\end{eqnarray}\\] with \\(u_w = (u_y &#39;, u_{\\delta} &#39;)&#39;\\). Example 4.4 (Vector Autoregressive Gamma process) In Alain Monfort et al. (2021), \\(w_t\\) follows a VARG (positive affine) process, which satisfies Hypthesis 4.7 (Example 1.8 presents the univariate version of this process.): \\[\\begin{equation} \\begin{array}{lll} y_{t} | \\underline{w_{t-1}} &amp;\\overset{\\mathbb{P}}{\\sim}&amp; \\otimes_{j=1}^{N_y} \\gamma_{\\nu_{j}^{(y)}} \\left( \\alpha_{j}^{(y)} + \\overbrace{\\beta_{j, y}^{(y)} y_{t-1}}^{\\mbox{factors}} + \\underbrace{ \\overbrace{\\color{blue}{ \\beta^{(y)}_{j, \\delta} \\delta_{t-1}}}^{\\begin{array}{c}\\mbox{credit-event}\\\\\\mbox{variables}\\end{array}}}_{\\begin{array}{c}\\color{red}{\\mbox{kills no-jump}}\\\\\\color{red}{\\mbox{condit.}}\\end{array}} , \\mu_j^{(y)} \\right) \\\\ \\delta_t | y_t, \\underline{w_{t-1}} &amp;\\overset{\\mathbb{P}}{\\sim}&amp; \\otimes_{e = 1}^{E} \\gamma_{0} \\left( \\alpha_e^{(\\delta)} + \\color{blue}{\\beta^{(\\delta)}_{e, y} y_{t}} + \\underbrace{\\color{blue}{\\beta^{(\\delta)}_{e, \\delta} \\delta_{t-1}}}_{\\color{red}{\\mbox{contagion}}}, \\mu_e^{(\\delta)} \\right) , \\end{array}\\tag{4.22} \\end{equation}\\] where \\(\\delta_t = (\\delta^{(1)}_t, \\ldots, \\delta^{(E)}_t)&#39;\\) and \\(\\gamma_{\\nu} \\left(\\lambda, \\mu \\right)\\): non-central Gamma distribution. Figure 4.5: A potential causality scheme in Monfort et al. (2021). Hypothesis 4.8 (Stochastic discount factor) The one-period positive SDF \\(\\mathcal{M}_{t-1, t}\\) is given by (this is Eq. (2.5), with \\(\\alpha_t \\equiv \\alpha_w\\)): \\[\\begin{equation} \\mathcal{M}_{t,t+1} = exp[-r_{t}+\\alpha&#39;_w w_{t+1}-\\psi_t(\\alpha_w)], \\end{equation}\\] and the risk-free short rate (between \\(t\\) and \\(t+1\\)) is given by the following affine function of the factors: \\[\\begin{equation}(\\#eq:short_rate) \\begin{array}{lll} r_{t}(\\underline{w_{t}}) = \\xi_0 + \\xi_1 &#39; w_{t}. \\end{array} \\end{equation}\\] \\(\\alpha_w = (\\alpha_y &#39; , \\alpha_x &#39; , \\alpha_{\\delta} &#39;) &#39;\\) is the vector of prices of risk. Under Hypotheses 4.7 and 4.8, it comes that: \\[\\begin{eqnarray} \\varphi^{\\mathbb{Q}}_{w, t-1} (u_w) &amp;=&amp; \\mathbb{E}^{\\mathbb{Q}} \\left[\\exp( u_w &#39; w_t ) | \\underline{w_{t-1}} \\right] \\nonumber \\\\ &amp;=&amp; \\exp \\left[ a^{\\mathbb{Q}}_{w} (u_{w}) &#39; w_{t-1} + b^{\\mathbb{Q}}_{w} (u_{w}) \\right],\\tag{4.23} \\end{eqnarray}\\] where, using Eq. (2.7): \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} a^{\\mathbb{Q}}_{w} (u_{w}) &amp;=&amp; a_{w} (u_{w} + \\alpha_w) - a_{w} (\\alpha_w) \\\\ b^{\\mathbb{Q}}_{w} (u_{w}) &amp;=&amp; b_{w} (u_{w} + \\alpha_w) - b_{w} (\\alpha_w). \\end{array} \\right.\\tag{4.24} \\end{equation}\\] Hypothesis 4.9 (General recovery payment) The Recovery Paymentof a defaultable ZCB, in the case of default at date \\(t+i = \\tau^{(e)}\\), is given by: \\[\\begin{equation} \\begin{array}{lll} RR^{(e)}_{t+i} \\times \\mathcal{V}^{(e)}_{t+i, h-i} \\end{array}\\tag{4.25} \\end{equation}\\] where the Recovery Rate \\(RR^{(e)}_{t+i}\\) is \\[\\begin{equation} RR^{(e)}_{t+i} = \\exp \\left( - a_{e} - a_{w, e} &#39; w_{t+i} \\right),\\tag{4.26} \\end{equation}\\] and where \\(\\mathcal{V}^{(e)}_{t+i, h-i}\\) denotes the Recovery Value (Exposure-at-Default) at \\(t+i\\)}. The general framework allows for flexible specifications of the recovery payment. It can be: the fraction of the pre-default value of the claim (Recovery of Market Value, or RMV) [see Prop. 4.7]; the fraction of par (Recovery of Face Value, or RFV) [see Proposition 4.6]; the fraction of a no-default version of the same claim (Recovery of Treasury, or RT). In that case, the recovery payment is a fraction of the (risk-free) present value of the principal. Proposition 4.6 (General pricing under the RFV convention) The price, at date \\(t &lt; \\tau^{(e)}\\), of a ZCB issued by entity \\(e\\) and maturing in \\(h\\) periods is given by: \\[\\begin{equation} \\begin{array}{lll} \\boxed{B_{e} (t, h) = \\sum_{i = 1}^{h} \\left( \\Lambda^{\\mathbb{Q}}_{(1, t, i)} - \\Lambda^{\\mathbb{Q}}_{(2, t, i)} \\right) + \\Lambda^{\\mathbb{Q}}_{(3, t, h)} ,} \\end{array}\\tag{4.27} \\end{equation}\\] where: \\[\\begin{equation} \\begin{array}{lll} \\Lambda_{(1,t,i)}^{\\mathbb{Q}} &amp;:=&amp; \\underset{u \\rightarrow - \\infty}{\\lim} \\Psi^{\\mathbb{Q}}_{(t, i)} (a_e , u \\widetilde{e}_\\delta - \\xi_1, - a_{w,e}) \\\\ \\Lambda_{(2,t,i)}^{\\mathbb{Q}} &amp;:=&amp; \\underset{u \\rightarrow - \\infty}{\\lim} \\Psi^{\\mathbb{Q}}_{(t, i)} (a_e , u \\widetilde{e}_\\delta - \\xi_1,u \\widetilde{e}_\\delta - a_{w,e} ) \\\\ \\Lambda_{(3,t,i)}^{\\mathbb{Q}} &amp;:=&amp; \\underset{u \\rightarrow - \\infty}{\\lim} \\Psi^{\\mathbb{Q}}_{(t, i)} (0 , u \\widetilde{e}_\\delta - \\xi_1,u \\widetilde{e}_\\delta) \\end{array}\\tag{4.28} \\end{equation}\\] with \\(u \\in \\mathbb{R}\\) and where [denoting \\(\\varphi^{\\mathbb{Q}}_{w, t, i} \\left(u_2, \\ldots, u_2 , u_1 \\right) = \\varphi^{\\mathbb{Q}}_{w, t, i} \\left( u_2 , u_1 \\right)\\)]: \\[\\begin{equation} \\begin{array}{lll} \\Psi^{\\mathbb{Q}}_{(t, i)} (\\kappa , u_1, u_2) &amp;:=&amp; \\exp \\left[ -i \\xi_0 + \\kappa + u_2 &#39; w_t \\right] \\varphi_{w,t,i}^{\\mathbb{Q}}(u_2, u_1) \\end{array}\\tag{4.29} \\end{equation}\\] Proof. See Alain Monfort et al. (2021). Proposition 4.7 (General pricing under the RMV convention) If the recovery value at date \\(t+i\\) (defined in 4.9) is of the form: \\[\\begin{equation} \\mathcal{V}^{(e)}_{t+i, h-i} = \\mathbb{E}^{\\mathbb{Q}} \\left\\{ \\exp \\left[ - \\sum_{j = i}^{h-1} \\left(r_{t+j} + \\delta^{(e)}_{t+j+1} \\right) \\right] \\Big| \\underline{w_{t+i}} \\right\\},\\tag{4.30} \\end{equation}\\] with \\(RR_t^{(e)} = \\exp(-\\delta_t^{(e)})\\), and under Hypothesis 4.6 to 4.8, the price \\(B_{e} (t, h)\\) at date \\(t &lt; \\tau^{(e)}\\) is given by: \\[\\begin{equation} B_{e} (t, h) = \\mathcal{V}^{(e)}_{t, h},\\tag{4.31} \\end{equation}\\] where \\[\\begin{equation} \\begin{array}{lll} \\mathcal{V}^{(e)}_{t, h} = \\exp \\left[ \\left( \\mathcal{A}_h - \\xi_1 \\right) &#39; w_t + \\left( \\mathcal{B}_h - h \\xi_0 \\right) \\right]. \\end{array}\\tag{4.32} \\end{equation}\\] where \\(\\mathcal{A}_h\\) and \\(\\mathcal{B}_h\\) are obtained recursively by employing Eqs. (1.19) of Proposition 1.5, replacing functions \\(a\\) and \\(b\\) with \\(a^{\\mathbb{Q}}\\) and \\(b^{\\mathbb{Q}}\\) (see Eq. (4.24)) with \\(u_{1} = - \\widetilde{e}_{\\delta}\\) and, for \\(i&gt;1\\), \\(u_i = - (\\widetilde{e}_{\\delta} + \\xi_1)\\) where \\(\\widetilde{e}_{\\delta} = (0&#39;,e_{\\delta}&#39;)&#39;\\) is a \\(N\\)-dimensional vector, and where \\(e_{\\delta}\\) is the \\(e^{th}\\) column of the \\((N_{\\delta}, N_{\\delta})\\)-dimensional identity matrix. Proof. See Appendix A.4 of Alain Monfort et al. (2021). Eq. (4.31) is a key result. It reads: \\[\\begin{equation} \\boxed{B_{e} (t, h) = \\mathbb{E}^{\\mathbb{Q}} \\left\\{ \\exp \\left[ - \\left( \\sum_{i = 1}^{h}{\\color{red}r_{t+i-1} + \\delta^{(e)}_{t+i}} \\right) \\right] \\Big| \\underline{w_{t}} \\right\\}.}\\tag{4.33} \\end{equation}\\] It is reminiscent of Eq. (4.3), with \\(\\lambda\\) (default intensity) replaced with \\(\\delta\\) (credit-event variable). However, contrary to the classical framework, credit events are priced sources of risk, the no-jump condition is relaxed, contagion is allowed, and the recovery rate is stochastic. This formula can therefore be seen as a generalization of the setting of Darrell Duffie and Singleton (1999). 4.4.2 CDS pricing in the general framework This subsection shows how CDS can be priced in the context of the general credit-risk model. We consider two types of CDS. The first is the standard one; the second is a CDS whose payoffs are expressed in a foreign currency (which is typical to sovereign CDSs). Figures 4.6 and 4.7 show the payoffs associated with these two types of CDSs. Figure 4.6: CDS payoffs. Figure 4.7: CDS payoffs when the payoffs are expressed in a foreign currency. Since the domestic-currency case is a special case of the multi-currency one, we focus on the latter case in the following. Pricing a CDS amounts to determining \\(\\mathcal{S}_{t,t+h}^{(e)}\\), that is the CDS spread, for \\(t &lt; \\tau^{(e)}\\). The spread \\(\\mathcal{S}_{t,t+h}^{(e)}\\) is such that the date-\\(t\\) value of the fixed leg’s payoffs is equal to the date-\\(t\\) value of the floating leg’s payoffs. The payoffs of the assets issued by the reference entity are assumed to be expressed in the domestic currency. The (logarithmic) exchange rate is denoted by \\(s_t = \\ln ( FX_t )\\). By convention, an increase in \\(s_t\\) corresponds to a depreciation of the domestic currency. (In other words, \\(FX_t\\) is the value of one unit of foreign currency expressed in the domestic currency.) We consider a CDS whose notional is equal to one unit of the foreign currency (i.e. to \\(\\exp (s_t)\\) units of the domestic currency). Let us determine the value of the fixed leg. If entity \\(e\\) has not defaulted at date \\(t+i\\) (\\(\\le t+h\\)), the cash flow on this date, expressed in the domestic currency, is: \\[\\begin{equation*} \\begin{array}{lll} \\mathcal{S}^{(e) f}_{t,t+h} \\exp(s_{t+i}). \\end{array} \\end{equation*}\\] Hence, the present values of the fixed-leg payments, expressed in the domestic currency are: \\[\\begin{equation*} \\begin{array}{lll} \\mathcal{S}^{(e) f}_{t, t+h} \\sum_{i=1}^{h} \\mathbb{E}^{\\mathbb{Q}} \\left[\\exp \\left( s_{t+i} - \\sum_{j = 1}^{i} r_{t + j - 1} \\right) \\textbf{1}_{\\{ \\delta^{(e) &#39;}_{t : t+i} {\\bf 1} = 0 \\} } \\Big| \\underline{w_t} \\right], \\end{array} \\end{equation*}\\] Let us turn to the floating leg. Under the RFV convention, the protection seller will make a payment of \\((1 - RR^{(e)}_{t+i}) \\exp(s_{t+i})\\) (this is the loss-given-default. LGD) at date \\(t+i\\) in case of default over the time interval \\(] t + i - 1, t+i ]\\). The present values of the floating leg, expressed in the domestic currency, is: \\[\\begin{equation*} \\begin{array}{lll} &amp;&amp; \\sum_{i=1}^{h} \\mathbb{E}^{\\mathbb{Q}} \\left[ \\exp \\left( s_{t+i} - \\sum_{j = 1}^{i} r_{t + j - 1} \\right) (1 - RR^{(e)}_{t+i})\\right. \\\\ \\\\ &amp;&amp; \\hspace{7em} \\left. \\left( \\textbf{1}_{\\{ \\delta^{(e) &#39;}_{t : t+i-1} {\\bf 1} = 0 \\} } - \\textbf{1}_{\\{ \\delta^{(e) &#39;}_{t : t+i} {\\bf 1} = 0 \\} } \\right) \\Big| \\underline{w_t} \\right],\\tag{4.34} \\end{array} \\end{equation*}\\] We assume that \\(RR^{(e)}_{t} = \\exp \\left( - a_{e} - a_{w, e} &#39; w_t \\right)\\) and that \\(s_t = \\chi + u_s &#39; w_t\\). Proposition 4.8 (Price of a multi-currency CDS) In the context described above, we have: \\[\\begin{equation} \\boxed{\\mathcal{S}^{(e) f}_{t, t+h} = \\frac{\\sum_{i=1}^{h} \\Lambda^{\\mathbb{Q}}_{(t, i)} }{\\sum_{i=1}^{h} \\lim_{u \\to - \\infty} \\Psi^{\\mathbb{Q}}_{(t, i)} \\left( \\chi, u \\tilde{e}_{\\delta} - \\xi_1 , u \\tilde{e}_{\\delta} + u_s \\right)},}\\tag{4.35} \\end{equation}\\] where: \\[\\begin{eqnarray*} \\Lambda^{\\mathbb{Q}}_{(t, i)} &amp; = &amp; \\underset{u \\to - \\infty}{\\lim} \\Big[ \\Psi^{\\mathbb{Q}}_{(t, i)} \\left( \\chi , u \\widetilde{e}_{\\delta} - \\xi_1 , u_s \\right) - \\Psi^{\\mathbb{Q}}_{(t, i)} \\left( \\chi , u \\widetilde{e}_{\\delta} - \\xi_1 , u \\widetilde{e}_{\\delta} + u_s \\right) \\\\ &amp;&amp; \\hspace{7em} - \\Psi^{\\mathbb{Q}}_{(t, i)} \\left(\\chi - a_e, u \\widetilde{e}_{\\delta} - \\xi_1 , u_s - a_w \\right) \\\\ &amp;&amp; \\hspace{7em} + \\Psi^{\\mathbb{Q}}_{(t, i)} \\left(\\chi - a_e, u \\widetilde{e}_{\\delta} - \\xi_1 , u \\widetilde{e}_{\\delta} + u_s - a_w \\right) \\Big] , \\end{eqnarray*}\\] and where \\(\\Psi^{\\mathbb{Q}}_{(t, i)} \\left(\\kappa, u_2 , u_1 \\right)\\) is given in Eq. (4.29). Proof. See Online Appendix A.4 of Alain Monfort et al. (2021). The proof notably makes use of Lemma ?? (Subsection 1.3). To price a standard CDS (with payments in domestic currency), one simply has to set \\(\\chi=0\\) and \\(u_s=0\\), which gives \\(s_t=0\\) (since \\(s_t = \\chi + u_s &#39; w_t\\)). Alain Monfort et al. (2021) exploit these formulas to model quanto CDSs in the euro area. Quanto CDSs are spread differentials between the two types of CDS. They exploit this framework to estimate “depreciations upon default”. Indeed, in their model, \\(s_t\\) (the log EURUSD exchange rate) is affected by the credit event variables \\(\\delta_t\\). 4.5 Top-down approach Relaxing the assumptions underlying the classical framework can also be done in the context of top-down approaches. Top-down models focus on default counting (or loss) processes (see, e.g., Azizpour, Giesecke, and Kim (2011) and Giesecke, Goldberg, and Ding (2011)), contrary to the (bottom-up) approaches presented above. The latter consider default processes of individual firms as the model primitives (e.g., Lando (1998), Darrell Duffie and Singleton (1999), Darrell Duffie and Garleanu (2001)). The top-down approach has been shown to satisfactorily capture the existence of default clustering (e.g., Brigo, Pallavicini, and Torresetti (2007), Errais, Giesecke, and Goldberg (2010)). Building on C. Gourieroux, Monfort, and Renne (2014), Christian Gourieroux et al. (2021) propose an affine top-down model consistent with: the presence of systemic entities, contagion, and the pricing of default events. Example 4.5 (Disastrous Defaults) Model proposed by Christian Gourieroux et al. (2021). \\(n^s_{t}\\): Number of systemic defaults on date \\(t\\). \\(N^s_{t}\\): number of systemic entities in default at date \\(t\\) (\\(N^s_{t}=n^s_{t} + N^s_{t-1}\\)). \\(\\underline{k_{t}}=\\{k_t,k_{t-1},\\dots\\}\\). Conditional distribution of the number of systemic defaults: \\[\\begin{eqnarray} n^s_{t+1}| \\underline{x_{t+1}}, \\underline{y_{t+1}}, \\underline{N^s_{t}} &amp;\\sim&amp; \\mathcal{P}oisson(\\beta y_{t+1}+c n^s_{t}),\\tag{4.36} \\end{eqnarray}\\] where \\(y_t\\) is a nonnegative factor fluctuating around a low-frequency component \\(x_t\\). \\((y_t,x_t)&#39;\\) follows a VARG dynamics (Eq. (??)). This dynamics admits the following VAR representation (using Prop. 1.1): \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} y_t - x_t &amp;=&amp; \\rho_y (y_{t-1} - x_{t-1}) + \\sigma_{y,t}\\varepsilon_{y,t}\\\\ x_t - \\mu_x &amp;=&amp; \\rho_x (x_{t-1} - \\mu_x) + \\sigma_{x,t}\\varepsilon_{x,t}, \\end{array} \\right.\\tag{4.37} \\end{equation}\\] with \\(0&lt;\\rho_y&lt;\\rho_x&lt;1\\). If \\(c&gt;0\\): Defaults on date \\(t\\) increases the conditional probability of having additional defaults on the next date \\(\\Rightarrow\\) Systemic defaults are infectious (Davis and Lo 2001}), or contagious. \\(\\Delta c_t = \\log(C_t/C_{t-1})\\): Log growth rate of per capita consumption: \\[\\begin{equation} \\Delta c_t = \\mu_{c,0} + \\mu_{c,x} x_t + \\mu_{c,y} y_t + {\\mu_{c,z}} z_{t}.\\tag{4.38} \\end{equation}\\] where \\(z_t\\) depends on systemic defaults: \\[\\begin{equation} z_t| \\underline{x_{t}}, \\underline{y_{t}}, \\underline{N^s_{t}} \\sim \\gamma_0(\\xi_{z}n^s_{t-1},\\mu_z).\\tag{4.39} \\end{equation}\\] \\(\\gamma_0\\) is a distribution featuring a point mass at zero [see Eq. ((1.4) in Example 1.8]. \\(\\Rightarrow\\) The conditional probability that \\(z_t=0\\) is \\(\\exp(-\\xi_{z}n^s_{t-1})\\), \\(z_{t}=0\\) as long as there has been no systemic defaults in the previous period, which is rather frequent. If \\(\\mu_{c,z}&lt;0\\) and \\(|\\mu_{c,z}|\\) is large or if \\(\\mu_{c,z}&lt;0\\) and \\(|\\mu_{c,z}|\\) not so large but \\(c\\) (contamination) is large, then systemic defaults can give rise to disastrous decreases in \\(C_t\\). Agents feature Epstein-Zin preferences, with a unit elasticity of intertemporal substitution (EIS). An admissible s.d.f. is exponential affine [as in Eq. (2.5)]. \\(\\Rightarrow\\) \\(w_t = (x_t,y_t,z_t,n^{s}_t)&#39;\\) is affine under \\(\\mathbb{P}\\) and \\(\\mathbb{Q}\\). In this context, a wide range of credit derivatives can be priced. In particular those whose payoffs depend on the default status of the constituents of a reference portfolio: Credit Index swaps (CIS) and synthetic CDO. CDS: protection payoff \\(&gt;0\\), when the entity on which the CDS is written defaults. CIS: protection payoff \\(&gt;0\\), when one entity of the underlying portfolio defaults. CDO: protection payoff \\(&gt;0\\), when one entity of the underlying portfolio defaults, given that losses are in a given interval \\([a,b]\\) (e.g. \\([a,b]=[3\\%,6\\%]\\)). See Figure ??. Typical credit indices: iTraxx (Europe) and CDX (U.S.). 125 large firms. Pricing formula based on an intensive use of the truncated Laplace transform (see Eq. (1.18), Darrell Duffie, Pan, and Singleton 2000). Estimation based (in particular) on iTraxx Europe main index; CDOs: maturities of 3, 5 and 7 years and, for each maturity, 5 tranches: 0%-3%, 3%-6%, 6%-9%, 9%-12% and 12%-22%. Fit of tranche prices shown on Figure ??. References "],["references.html", "Chapter 5 References", " Chapter 5 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
